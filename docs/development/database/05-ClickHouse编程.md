# ClickHouseç¼–ç¨‹ - Goè¯­è¨€å®æˆ˜æŒ‡å—

> ä½¿ç”¨ Go è¯­è¨€æ“ä½œ ClickHouse é«˜æ€§èƒ½åˆ—å¼æ•°æ®åº“

---

## ğŸ“‹ ç›®å½•


- [ClickHouseæ¦‚è¿°](#clickhouseæ¦‚è¿°)
  - [ç‰¹ç‚¹](#ç‰¹ç‚¹)
  - [é€‚ç”¨åœºæ™¯](#é€‚ç”¨åœºæ™¯)
  - [OLTP vs OLAP](#oltp-vs-olap)
- [å®‰è£…ä¸é…ç½®](#å®‰è£…ä¸é…ç½®)
  - [å®‰è£…é©±åŠ¨](#å®‰è£…é©±åŠ¨)
  - [æœ¬åœ°å®‰è£…ClickHouse](#æœ¬åœ°å®‰è£…clickhouse)
- [è¿æ¥ç®¡ç†](#è¿æ¥ç®¡ç†)
  - [åŸç”Ÿåè®®è¿æ¥](#åŸç”Ÿåè®®è¿æ¥)
  - [HTTPåè®®è¿æ¥](#httpåè®®è¿æ¥)
- [è¡¨è®¾è®¡ä¸æ“ä½œ](#è¡¨è®¾è®¡ä¸æ“ä½œ)
  - [è¡¨å¼•æ“ç±»å‹](#è¡¨å¼•æ“ç±»å‹)
  - [åˆ›å»ºè¡¨](#åˆ›å»ºè¡¨)
- [æ•°æ®æ’å…¥](#æ•°æ®æ’å…¥)
  - [æ‰¹é‡æ’å…¥](#æ‰¹é‡æ’å…¥)
- [æ•°æ®æŸ¥è¯¢](#æ•°æ®æŸ¥è¯¢)
  - [åŸºæœ¬æŸ¥è¯¢](#åŸºæœ¬æŸ¥è¯¢)
  - [èšåˆæŸ¥è¯¢](#èšåˆæŸ¥è¯¢)
  - [æ¼æ–—åˆ†æ](#æ¼æ–—åˆ†æ)
  - [ç•™å­˜åˆ†æ](#ç•™å­˜åˆ†æ)
- [ç‰©åŒ–è§†å›¾](#ç‰©åŒ–è§†å›¾)
  - [åˆ›å»ºç‰©åŒ–è§†å›¾](#åˆ›å»ºç‰©åŒ–è§†å›¾)
- [åˆ†åŒºä¸åˆ†ç‰‡](#åˆ†åŒºä¸åˆ†ç‰‡)
  - [åˆ†åŒºç®¡ç†](#åˆ†åŒºç®¡ç†)
- [å®æ—¶æ•°æ®åˆ†æ](#å®æ—¶æ•°æ®åˆ†æ)
  - [å®æ—¶çœ‹æ¿](#å®æ—¶çœ‹æ¿)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
  - [æŸ¥è¯¢ä¼˜åŒ–](#æŸ¥è¯¢ä¼˜åŒ–)
  - [ç´¢å¼•ä¼˜åŒ–](#ç´¢å¼•ä¼˜åŒ–)
  - [æ‰¹é‡å†™å…¥ä¼˜åŒ–](#æ‰¹é‡å†™å…¥ä¼˜åŒ–)
- [ç›‘æ§ä¸è¿ç»´](#ç›‘æ§ä¸è¿ç»´)
  - [ç³»ç»Ÿç›‘æ§](#ç³»ç»Ÿç›‘æ§)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
  - [1. è¡¨è®¾è®¡åŸåˆ™](#1-è¡¨è®¾è®¡åŸåˆ™)
  - [2. æŸ¥è¯¢ä¼˜åŒ–](#2-æŸ¥è¯¢ä¼˜åŒ–)
  - [3. å†™å…¥ä¼˜åŒ–](#3-å†™å…¥ä¼˜åŒ–)
  - [4. ç›‘æ§å‘Šè­¦](#4-ç›‘æ§å‘Šè­¦)
- [æ€»ç»“](#æ€»ç»“)

## ClickHouseæ¦‚è¿°

### ç‰¹ç‚¹

ClickHouseæ˜¯ä¸€ä¸ª**OLAP**ï¼ˆåœ¨çº¿åˆ†æå¤„ç†ï¼‰æ•°æ®åº“ï¼š

- **åˆ—å¼å­˜å‚¨**: é€‚åˆåˆ†ææŸ¥è¯¢ï¼Œå‹ç¼©ç‡é«˜
- **é«˜æ€§èƒ½**: å•æœåŠ¡å™¨æ¯ç§’å¤„ç†æ•°äº¿è¡Œ
- **SQLæ”¯æŒ**: å®Œæ•´çš„SQLæ–¹è¨€
- **å®æ—¶æ’å…¥**: æ”¯æŒå®æ—¶æ•°æ®æµ
- **åˆ†å¸ƒå¼**: è‡ªåŠ¨æ•°æ®åˆ†ç‰‡å’Œå‰¯æœ¬
- **ä¸°å¯Œçš„æ•°æ®ç±»å‹**: æ•°ç»„ã€åµŒå¥—ã€JSONç­‰

### é€‚ç”¨åœºæ™¯

```text
âœ… æ—¥å¿—åˆ†æ
âœ… ç”¨æˆ·è¡Œä¸ºåˆ†æ
âœ… å®æ—¶ç›‘æ§
âœ… æ—¶åºæ•°æ®å­˜å‚¨
âœ… å•†ä¸šæ™ºèƒ½(BI)
âœ… æ•°æ®ä»“åº“
```

### OLTP vs OLAP

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç‰¹æ€§       â”‚   OLTP     â”‚    OLAP      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ•°æ®é‡       â”‚ GBçº§       â”‚ TB-PBçº§      â”‚
â”‚ æŸ¥è¯¢ç±»å‹     â”‚ ç‚¹æŸ¥è¯¢     â”‚ èšåˆæŸ¥è¯¢     â”‚
â”‚ å†™å…¥æ¨¡å¼     â”‚ éšæœºå†™     â”‚ æ‰¹é‡å†™       â”‚
â”‚ å…¸å‹æ•°æ®åº“   â”‚ MySQL/PG   â”‚ ClickHouse   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å®‰è£…ä¸é…ç½®

### å®‰è£…é©±åŠ¨

```bash
# ClickHouseå®˜æ–¹Goé©±åŠ¨
go get github.com/ClickHouse/clickhouse-go/v2

# æ ‡å‡†database/sqlæ¥å£
go get github.com/ClickHouse/clickhouse-go/v2/lib/driver
```

### æœ¬åœ°å®‰è£…ClickHouse

```bash
# Dockeræ–¹å¼ï¼ˆæ¨èï¼‰
docker run -d \
  --name clickhouse-server \
  --ulimit nofile=262144:262144 \
  -p 9000:9000 \
  -p 8123:8123 \
  clickhouse/clickhouse-server

# æµ‹è¯•è¿æ¥
curl http://localhost:8123/ping
```

---

## è¿æ¥ç®¡ç†

### åŸç”Ÿåè®®è¿æ¥

```go
package database

import (
    "context"
    "crypto/tls"
    "fmt"
    "time"
    
    "github.com/ClickHouse/clickhouse-go/v2"
    "github.com/ClickHouse/clickhouse-go/v2/lib/driver"
)

// ClickHouse æ•°æ®åº“å®ä¾‹
type ClickHouse struct {
    Conn driver.Conn
}

// NewClickHouse åˆ›å»ºClickHouseè¿æ¥ï¼ˆåŸç”Ÿåè®®ï¼‰
func NewClickHouse(addr string) (*ClickHouse, error) {
    conn, err := clickhouse.Open(&clickhouse.Options{
        Addr: []string{addr}, // æ”¯æŒå¤šä¸ªåœ°å€ç”¨äºè´Ÿè½½å‡è¡¡
        Auth: clickhouse.Auth{
            Database: "default",
            Username: "default",
            Password: "",
        },
        Settings: clickhouse.Settings{
            "max_execution_time": 60, // æœ€å¤§æ‰§è¡Œæ—¶é—´(ç§’)
        },
        DialTimeout:      5 * time.Second,
        MaxOpenConns:     10,
        MaxIdleConns:     5,
        ConnMaxLifetime:  time.Hour,
        ConnOpenStrategy: clickhouse.ConnOpenInOrder,
        
        // TLSé…ç½®ï¼ˆå¯é€‰ï¼‰
        TLS: &tls.Config{
            InsecureSkipVerify: true,
        },
        
        // è°ƒè¯•æ¨¡å¼
        Debug: false,
    })
    
    if err != nil {
        return nil, fmt.Errorf("failed to connect: %w", err)
    }
    
    // æµ‹è¯•è¿æ¥
    ctx := context.Background()
    if err := conn.Ping(ctx); err != nil {
        return nil, fmt.Errorf("ping failed: %w", err)
    }
    
    fmt.Println("âœ… ClickHouseè¿æ¥æˆåŠŸ")
    
    return &ClickHouse{Conn: conn}, nil
}

// Close å…³é—­è¿æ¥
func (c *ClickHouse) Close() error {
    return c.Conn.Close()
}

// Exec æ‰§è¡ŒSQL
func (c *ClickHouse) Exec(ctx context.Context, query string, args ...interface{}) error {
    return c.Conn.Exec(ctx, query, args...)
}

// Query æŸ¥è¯¢
func (c *ClickHouse) Query(ctx context.Context, dest interface{}, query string, args ...interface{}) error {
    rows, err := c.Conn.Query(ctx, query, args...)
    if err != nil {
        return err
    }
    defer rows.Close()
    
    return rows.ScanStruct(dest)
}
```

### HTTPåè®®è¿æ¥

```go
import (
    "database/sql"
    _ "github.com/ClickHouse/clickhouse-go/v2"
)

// NewClickHouseHTTP åˆ›å»ºClickHouseè¿æ¥ï¼ˆHTTPåè®®ï¼‰
func NewClickHouseHTTP(dsn string) (*sql.DB, error) {
    // DSNæ ¼å¼: clickhouse://username:password@host:port/database?param=value
    db, err := sql.Open("clickhouse", dsn)
    if err != nil {
        return nil, err
    }
    
    // è®¾ç½®è¿æ¥æ± 
    db.SetMaxOpenConns(10)
    db.SetMaxIdleConns(5)
    db.SetConnMaxLifetime(time.Hour)
    
    // æµ‹è¯•è¿æ¥
    if err := db.Ping(); err != nil {
        return nil, err
    }
    
    fmt.Println("âœ… ClickHouse HTTPè¿æ¥æˆåŠŸ")
    
    return db, nil
}
```

---

## è¡¨è®¾è®¡ä¸æ“ä½œ

### è¡¨å¼•æ“ç±»å‹

ClickHouseæ”¯æŒå¤šç§è¡¨å¼•æ“ï¼š

- **MergeTree**: æœ€å¸¸ç”¨ï¼Œæ”¯æŒç´¢å¼•å’Œåˆ†åŒº
- **ReplacingMergeTree**: å»é‡
- **SummingMergeTree**: è‡ªåŠ¨æ±‚å’Œèšåˆ
- **AggregatingMergeTree**: èšåˆå‡½æ•°ç»“æœ
- **CollapsingMergeTree**: çŠ¶æ€æŠ˜å 
- **Distributed**: åˆ†å¸ƒå¼è¡¨

### åˆ›å»ºè¡¨

```go
package repository

import (
    "context"
    "fmt"
    
    "github.com/ClickHouse/clickhouse-go/v2/lib/driver"
)

// EventRepository äº‹ä»¶ä»“å‚¨
type EventRepository struct {
    conn driver.Conn
}

// NewEventRepository åˆ›å»ºäº‹ä»¶ä»“å‚¨
func NewEventRepository(conn driver.Conn) *EventRepository {
    return &EventRepository{conn: conn}
}

// CreateTable åˆ›å»ºäº‹ä»¶è¡¨
func (r *EventRepository) CreateTable(ctx context.Context) error {
    query := `
    CREATE TABLE IF NOT EXISTS events (
        event_id String,
        user_id UInt64,
        event_type String,
        event_time DateTime,
        properties String,  -- JSONæ ¼å¼
        country String,
        city String,
        device String,
        os String,
        browser String,
        session_id String,
        created_at DateTime DEFAULT now()
    )
    ENGINE = MergeTree()
    PARTITION BY toYYYYMM(event_time)    -- æŒ‰æœˆåˆ†åŒº
    ORDER BY (event_type, user_id, event_time)  -- æ’åºé”®ï¼ˆä¸»é”®ï¼‰
    TTL event_time + INTERVAL 90 DAY     -- æ•°æ®ä¿ç•™90å¤©
    SETTINGS index_granularity = 8192
    `
    
    err := r.conn.Exec(ctx, query)
    if err != nil {
        return fmt.Errorf("create table failed: %w", err)
    }
    
    fmt.Println("âœ… è¡¨åˆ›å»ºæˆåŠŸ")
    
    return nil
}

// CreateDistributedTable åˆ›å»ºåˆ†å¸ƒå¼è¡¨
func (r *EventRepository) CreateDistributedTable(ctx context.Context, cluster string) error {
    query := fmt.Sprintf(`
    CREATE TABLE IF NOT EXISTS events_distributed AS events
    ENGINE = Distributed(%s, default, events, rand())
    `, cluster)
    
    return r.conn.Exec(ctx, query)
}

// DropTable åˆ é™¤è¡¨
func (r *EventRepository) DropTable(ctx context.Context) error {
    return r.conn.Exec(ctx, "DROP TABLE IF EXISTS events")
}

// OptimizeTable ä¼˜åŒ–è¡¨ï¼ˆæ‰‹åŠ¨è§¦å‘åˆå¹¶ï¼‰
func (r *EventRepository) OptimizeTable(ctx context.Context) error {
    return r.conn.Exec(ctx, "OPTIMIZE TABLE events FINAL")
}
```

---

## æ•°æ®æ’å…¥

### æ‰¹é‡æ’å…¥

```go
package models

import "time"

// Event äº‹ä»¶æ¨¡å‹
type Event struct {
    EventID    string    `ch:"event_id"`
    UserID     uint64    `ch:"user_id"`
    EventType  string    `ch:"event_type"`
    EventTime  time.Time `ch:"event_time"`
    Properties string    `ch:"properties"`
    Country    string    `ch:"country"`
    City       string    `ch:"city"`
    Device     string    `ch:"device"`
    OS         string    `ch:"os"`
    Browser    string    `ch:"browser"`
    SessionID  string    `ch:"session_id"`
    CreatedAt  time.Time `ch:"created_at"`
}
```

```go
// Insert æ’å…¥å•æ¡äº‹ä»¶
func (r *EventRepository) Insert(ctx context.Context, event *Event) error {
    query := `
    INSERT INTO events (
        event_id, user_id, event_type, event_time, properties,
        country, city, device, os, browser, session_id
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    `
    
    err := r.conn.Exec(ctx, query,
        event.EventID, event.UserID, event.EventType, event.EventTime,
        event.Properties, event.Country, event.City, event.Device,
        event.OS, event.Browser, event.SessionID,
    )
    
    return err
}

// BatchInsert æ‰¹é‡æ’å…¥ï¼ˆæ¨èï¼‰
func (r *EventRepository) BatchInsert(ctx context.Context, events []*Event) error {
    // ä½¿ç”¨Batchæ¥å£ï¼Œæ€§èƒ½æ›´å¥½
    batch, err := r.conn.PrepareBatch(ctx, "INSERT INTO events")
    if err != nil {
        return err
    }
    
    for _, event := range events {
        err := batch.Append(
            event.EventID, event.UserID, event.EventType, event.EventTime,
            event.Properties, event.Country, event.City, event.Device,
            event.OS, event.Browser, event.SessionID,
        )
        if err != nil {
            return err
        }
    }
    
    // å‘é€æ‰¹æ¬¡
    if err := batch.Send(); err != nil {
        return fmt.Errorf("batch send failed: %w", err)
    }
    
    fmt.Printf("âœ… æ‰¹é‡æ’å…¥æˆåŠŸï¼Œæ’å…¥äº† %d æ¡è®°å½•\n", len(events))
    
    return nil
}

// AsyncInsert å¼‚æ­¥æ’å…¥ï¼ˆClickHouse 22.3+ï¼‰
func (r *EventRepository) AsyncInsert(ctx context.Context, events []*Event) error {
    // å¼‚æ­¥æ’å…¥ä¼šåœ¨æœåŠ¡å™¨ç«¯æ‰¹é‡å¤„ç†
    query := `
    INSERT INTO events (
        event_id, user_id, event_type, event_time, properties,
        country, city, device, os, browser, session_id
    ) VALUES
    `
    
    // ä½¿ç”¨ SETTINGS async_insert=1
    ctx = clickhouse.Context(ctx, clickhouse.WithSettings(clickhouse.Settings{
        "async_insert": 1,
        "wait_for_async_insert": 0, // ä¸ç­‰å¾…æ’å…¥å®Œæˆ
    }))
    
    batch, err := r.conn.PrepareBatch(ctx, query)
    if err != nil {
        return err
    }
    
    for _, event := range events {
        batch.Append(
            event.EventID, event.UserID, event.EventType, event.EventTime,
            event.Properties, event.Country, event.City, event.Device,
            event.OS, event.Browser, event.SessionID,
        )
    }
    
    return batch.Send()
}
```

---

## æ•°æ®æŸ¥è¯¢

### åŸºæœ¬æŸ¥è¯¢

```go
// FindByUserID æ ¹æ®ç”¨æˆ·IDæŸ¥è¯¢äº‹ä»¶
func (r *EventRepository) FindByUserID(ctx context.Context, userID uint64, limit int) ([]*Event, error) {
    query := `
    SELECT event_id, user_id, event_type, event_time, properties,
           country, city, device, os, browser, session_id, created_at
    FROM events
    WHERE user_id = ?
    ORDER BY event_time DESC
    LIMIT ?
    `
    
    rows, err := r.conn.Query(ctx, query, userID, limit)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var events []*Event
    for rows.Next() {
        var event Event
        if err := rows.Scan(
            &event.EventID, &event.UserID, &event.EventType, &event.EventTime,
            &event.Properties, &event.Country, &event.City, &event.Device,
            &event.OS, &event.Browser, &event.SessionID, &event.CreatedAt,
        ); err != nil {
            return nil, err
        }
        events = append(events, &event)
    }
    
    return events, rows.Err()
}

// CountByEventType æŒ‰äº‹ä»¶ç±»å‹ç»Ÿè®¡
func (r *EventRepository) CountByEventType(ctx context.Context, startTime, endTime time.Time) (map[string]uint64, error) {
    query := `
    SELECT event_type, count() as count
    FROM events
    WHERE event_time >= ? AND event_time < ?
    GROUP BY event_type
    ORDER BY count DESC
    `
    
    rows, err := r.conn.Query(ctx, query, startTime, endTime)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    result := make(map[string]uint64)
    for rows.Next() {
        var eventType string
        var count uint64
        if err := rows.Scan(&eventType, &count); err != nil {
            return nil, err
        }
        result[eventType] = count
    }
    
    return result, rows.Err()
}
```

### èšåˆæŸ¥è¯¢

```go
// GetDailyStatistics è·å–æ¯æ—¥ç»Ÿè®¡
func (r *EventRepository) GetDailyStatistics(ctx context.Context, days int) ([]DailyStats, error) {
    query := `
    SELECT 
        toDate(event_time) as date,
        count() as total_events,
        uniq(user_id) as unique_users,
        uniq(session_id) as sessions,
        countIf(event_type = 'page_view') as page_views,
        countIf(event_type = 'click') as clicks,
        avg(toUInt32(JSONExtractString(properties, 'duration'))) as avg_duration
    FROM events
    WHERE event_time >= now() - INTERVAL ? DAY
    GROUP BY date
    ORDER BY date DESC
    `
    
    rows, err := r.conn.Query(ctx, query, days)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var stats []DailyStats
    for rows.Next() {
        var stat DailyStats
        if err := rows.Scan(
            &stat.Date, &stat.TotalEvents, &stat.UniqueUsers,
            &stat.Sessions, &stat.PageViews, &stat.Clicks, &stat.AvgDuration,
        ); err != nil {
            return nil, err
        }
        stats = append(stats, stat)
    }
    
    return stats, rows.Err()
}

// DailyStats æ¯æ—¥ç»Ÿè®¡
type DailyStats struct {
    Date        time.Time
    TotalEvents uint64
    UniqueUsers uint64
    Sessions    uint64
    PageViews   uint64
    Clicks      uint64
    AvgDuration float64
}

// GetTopCountries è·å–Topå›½å®¶
func (r *EventRepository) GetTopCountries(ctx context.Context, limit int) ([]CountryStats, error) {
    query := `
    SELECT 
        country,
        count() as events,
        uniq(user_id) as users,
        uniq(session_id) as sessions
    FROM events
    WHERE event_time >= now() - INTERVAL 7 DAY
    GROUP BY country
    ORDER BY events DESC
    LIMIT ?
    `
    
    rows, err := r.conn.Query(ctx, query, limit)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var stats []CountryStats
    for rows.Next() {
        var stat CountryStats
        if err := rows.Scan(&stat.Country, &stat.Events, &stat.Users, &stat.Sessions); err != nil {
            return nil, err
        }
        stats = append(stats, stat)
    }
    
    return stats, rows.Err()
}

// CountryStats å›½å®¶ç»Ÿè®¡
type CountryStats struct {
    Country  string
    Events   uint64
    Users    uint64
    Sessions uint64
}
```

### æ¼æ–—åˆ†æ

```go
// FunnelAnalysis æ¼æ–—åˆ†æ
func (r *EventRepository) FunnelAnalysis(ctx context.Context, steps []string, window int) (*FunnelResult, error) {
    // ä½¿ç”¨windowFunnelå‡½æ•°
    query := `
    SELECT 
        windowFunnel(?, 'strict')(event_time, ` + buildFunnelConditions(steps) + `) as level,
        count() as users
    FROM events
    WHERE event_time >= now() - INTERVAL 7 DAY
    GROUP BY level
    ORDER BY level
    `
    
    rows, err := r.conn.Query(ctx, query, window)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    result := &FunnelResult{Steps: steps}
    for rows.Next() {
        var level int
        var users uint64
        if err := rows.Scan(&level, &users); err != nil {
            return nil, err
        }
        result.Levels = append(result.Levels, FunnelLevel{Level: level, Users: users})
    }
    
    return result, rows.Err()
}

// buildFunnelConditions æ„å»ºæ¼æ–—æ¡ä»¶
func buildFunnelConditions(steps []string) string {
    var conditions []string
    for _, step := range steps {
        conditions = append(conditions, fmt.Sprintf("event_type = '%s'", step))
    }
    return strings.Join(conditions, ", ")
}

// FunnelResult æ¼æ–—ç»“æœ
type FunnelResult struct {
    Steps  []string
    Levels []FunnelLevel
}

// FunnelLevel æ¼æ–—å±‚çº§
type FunnelLevel struct {
    Level int
    Users uint64
}
```

### ç•™å­˜åˆ†æ

```go
// RetentionAnalysis ç•™å­˜åˆ†æ
func (r *EventRepository) RetentionAnalysis(ctx context.Context, days int) ([][]float64, error) {
    query := `
    SELECT 
        retention
    FROM
    (
        SELECT 
            user_id,
            groupArray(toDate(event_time)) as dates
        FROM events
        WHERE event_time >= today() - INTERVAL ? DAY
        GROUP BY user_id
    )
    ARRAY JOIN retention(dates, today() - INTERVAL ? DAY, toDate(today()), 1) as retention
    `
    
    rows, err := r.conn.Query(ctx, query, days, days)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var retentionData [][]float64
    // å¤„ç†ç»“æœ
    // ...
    
    return retentionData, nil
}
```

---

## ç‰©åŒ–è§†å›¾

### åˆ›å»ºç‰©åŒ–è§†å›¾

```go
// CreateMaterializedView åˆ›å»ºç‰©åŒ–è§†å›¾
func (r *EventRepository) CreateMaterializedView(ctx context.Context) error {
    // åˆ›å»ºç›®æ ‡è¡¨
    createTableQuery := `
    CREATE TABLE IF NOT EXISTS event_hourly_stats (
        event_date Date,
        event_hour UInt8,
        event_type String,
        country String,
        total_events UInt64,
        unique_users UInt64
    )
    ENGINE = SummingMergeTree()
    PARTITION BY toYYYYMM(event_date)
    ORDER BY (event_date, event_hour, event_type, country)
    `
    
    if err := r.conn.Exec(ctx, createTableQuery); err != nil {
        return err
    }
    
    // åˆ›å»ºç‰©åŒ–è§†å›¾
    createMVQuery := `
    CREATE MATERIALIZED VIEW IF NOT EXISTS event_hourly_stats_mv
    TO event_hourly_stats
    AS SELECT 
        toDate(event_time) as event_date,
        toHour(event_time) as event_hour,
        event_type,
        country,
        count() as total_events,
        uniqState(user_id) as unique_users
    FROM events
    GROUP BY event_date, event_hour, event_type, country
    `
    
    if err := r.conn.Exec(ctx, createMVQuery); err != nil {
        return err
    }
    
    fmt.Println("âœ… ç‰©åŒ–è§†å›¾åˆ›å»ºæˆåŠŸ")
    
    return nil
}

// QueryMaterializedView æŸ¥è¯¢ç‰©åŒ–è§†å›¾
func (r *EventRepository) QueryMaterializedView(ctx context.Context, date time.Time) ([]HourlyStats, error) {
    query := `
    SELECT 
        event_hour,
        event_type,
        country,
        sum(total_events) as events,
        uniqMerge(unique_users) as users
    FROM event_hourly_stats
    WHERE event_date = ?
    GROUP BY event_hour, event_type, country
    ORDER BY event_hour, events DESC
    `
    
    rows, err := r.conn.Query(ctx, query, date)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var stats []HourlyStats
    for rows.Next() {
        var stat HourlyStats
        if err := rows.Scan(&stat.Hour, &stat.EventType, &stat.Country, &stat.Events, &stat.Users); err != nil {
            return nil, err
        }
        stats = append(stats, stat)
    }
    
    return stats, rows.Err()
}

// HourlyStats å°æ—¶ç»Ÿè®¡
type HourlyStats struct {
    Hour      uint8
    EventType string
    Country   string
    Events    uint64
    Users     uint64
}
```

---

## åˆ†åŒºä¸åˆ†ç‰‡

### åˆ†åŒºç®¡ç†

```go
// ListPartitions åˆ—å‡ºæ‰€æœ‰åˆ†åŒº
func (r *EventRepository) ListPartitions(ctx context.Context) ([]PartitionInfo, error) {
    query := `
    SELECT 
        partition,
        rows,
        bytes_on_disk,
        modification_time
    FROM system.parts
    WHERE table = 'events' AND active = 1
    ORDER BY partition
    `
    
    rows, err := r.conn.Query(ctx, query)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var partitions []PartitionInfo
    for rows.Next() {
        var p PartitionInfo
        if err := rows.Scan(&p.Partition, &p.Rows, &p.BytesOnDisk, &p.ModificationTime); err != nil {
            return nil, err
        }
        partitions = append(partitions, p)
    }
    
    return partitions, rows.Err()
}

// PartitionInfo åˆ†åŒºä¿¡æ¯
type PartitionInfo struct {
    Partition        string
    Rows             uint64
    BytesOnDisk      uint64
    ModificationTime time.Time
}

// DropPartition åˆ é™¤åˆ†åŒº
func (r *EventRepository) DropPartition(ctx context.Context, partition string) error {
    query := fmt.Sprintf("ALTER TABLE events DROP PARTITION '%s'", partition)
    return r.conn.Exec(ctx, query)
}

// DetachPartition åˆ†ç¦»åˆ†åŒºï¼ˆä¸åˆ é™¤æ•°æ®ï¼‰
func (r *EventRepository) DetachPartition(ctx context.Context, partition string) error {
    query := fmt.Sprintf("ALTER TABLE events DETACH PARTITION '%s'", partition)
    return r.conn.Exec(ctx, query)
}

// AttachPartition é™„åŠ åˆ†åŒº
func (r *EventRepository) AttachPartition(ctx context.Context, partition string) error {
    query := fmt.Sprintf("ALTER TABLE events ATTACH PARTITION '%s'", partition)
    return r.conn.Exec(ctx, query)
}
```

---

## å®æ—¶æ•°æ®åˆ†æ

### å®æ—¶çœ‹æ¿

```go
package analytics

import (
    "context"
    "time"
    
    "github.com/ClickHouse/clickhouse-go/v2/lib/driver"
)

// Dashboard å®æ—¶çœ‹æ¿
type Dashboard struct {
    conn driver.Conn
}

// NewDashboard åˆ›å»ºçœ‹æ¿
func NewDashboard(conn driver.Conn) *Dashboard {
    return &Dashboard{conn: conn}
}

// GetRealtimeMetrics è·å–å®æ—¶æŒ‡æ ‡
func (d *Dashboard) GetRealtimeMetrics(ctx context.Context) (*RealtimeMetrics, error) {
    query := `
    SELECT 
        -- æœ€è¿‘1åˆ†é’Ÿ
        countIf(event_time >= now() - INTERVAL 1 MINUTE) as events_1m,
        uniqIf(user_id, event_time >= now() - INTERVAL 1 MINUTE) as users_1m,
        
        -- æœ€è¿‘5åˆ†é’Ÿ
        countIf(event_time >= now() - INTERVAL 5 MINUTE) as events_5m,
        uniqIf(user_id, event_time >= now() - INTERVAL 5 MINUTE) as users_5m,
        
        -- æœ€è¿‘1å°æ—¶
        countIf(event_time >= now() - INTERVAL 1 HOUR) as events_1h,
        uniqIf(user_id, event_time >= now() - INTERVAL 1 HOUR) as users_1h,
        
        -- ä»Šæ—¥
        countIf(toDate(event_time) = today()) as events_today,
        uniqIf(user_id, toDate(event_time) = today()) as users_today
    FROM events
    WHERE event_time >= today()
    `
    
    row := d.conn.QueryRow(ctx, query)
    
    var metrics RealtimeMetrics
    if err := row.Scan(
        &metrics.Events1m, &metrics.Users1m,
        &metrics.Events5m, &metrics.Users5m,
        &metrics.Events1h, &metrics.Users1h,
        &metrics.EventsToday, &metrics.UsersToday,
    ); err != nil {
        return nil, err
    }
    
    return &metrics, nil
}

// RealtimeMetrics å®æ—¶æŒ‡æ ‡
type RealtimeMetrics struct {
    Events1m     uint64
    Users1m      uint64
    Events5m     uint64
    Users5m      uint64
    Events1h     uint64
    Users1h      uint64
    EventsToday  uint64
    UsersToday   uint64
}

// GetActiveUsers è·å–æ´»è·ƒç”¨æˆ·
func (d *Dashboard) GetActiveUsers(ctx context.Context, minutes int) ([]ActiveUser, error) {
    query := `
    SELECT 
        user_id,
        count() as event_count,
        max(event_time) as last_event_time
    FROM events
    WHERE event_time >= now() - INTERVAL ? MINUTE
    GROUP BY user_id
    ORDER BY event_count DESC
    LIMIT 100
    `
    
    rows, err := d.conn.Query(ctx, query, minutes)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var users []ActiveUser
    for rows.Next() {
        var user ActiveUser
        if err := rows.Scan(&user.UserID, &user.EventCount, &user.LastEventTime); err != nil {
            return nil, err
        }
        users = append(users, user)
    }
    
    return users, rows.Err()
}

// ActiveUser æ´»è·ƒç”¨æˆ·
type ActiveUser struct {
    UserID        uint64
    EventCount    uint64
    LastEventTime time.Time
}
```

---

## æ€§èƒ½ä¼˜åŒ–

### æŸ¥è¯¢ä¼˜åŒ–

```go
// 1. ä½¿ç”¨PREWHEREæ›¿ä»£WHEREï¼ˆæ›´å¿«çš„æ•°æ®è¿‡æ»¤ï¼‰
query := `
SELECT count()
FROM events
PREWHERE event_type = 'click'  -- å…ˆåœ¨åˆ—å­˜å‚¨å±‚è¿‡æ»¤
WHERE country = 'US'
`

// 2. é¿å…SELECT *ï¼ŒåªæŸ¥è¯¢éœ€è¦çš„åˆ—
query := `
SELECT event_id, event_type, event_time
FROM events
WHERE user_id = ?
`

// 3. ä½¿ç”¨FINALæ…é‡ï¼ˆä¼šé™ä½æ€§èƒ½ï¼‰
query := `
SELECT * 
FROM events 
-- FINAL  -- é¿å…ä½¿ç”¨ï¼Œé™¤éå¿…éœ€
WHERE user_id = ?
`

// 4. åˆç†ä½¿ç”¨LIMIT
query := `
SELECT * 
FROM events
ORDER BY event_time DESC
LIMIT 1000  -- é™åˆ¶è¿”å›è¡Œæ•°
`

// 5. ä½¿ç”¨é‡‡æ ·æŸ¥è¯¢ï¼ˆå¤§æ•°æ®é›†ï¼‰
query := `
SELECT event_type, count()
FROM events
SAMPLE 0.1  -- é‡‡æ ·10%çš„æ•°æ®
WHERE event_time >= today()
GROUP BY event_type
`
```

### ç´¢å¼•ä¼˜åŒ–

```go
// åˆ›å»ºè·³æ•°ç´¢å¼•ï¼ˆSkip Indexï¼‰
func (r *EventRepository) CreateSkipIndex(ctx context.Context) error {
    queries := []string{
        // MinMaxç´¢å¼•ï¼ˆé€‚åˆèŒƒå›´æŸ¥è¯¢ï¼‰
        `ALTER TABLE events ADD INDEX idx_user_id_minmax user_id TYPE minmax GRANULARITY 4`,
        
        // Setç´¢å¼•ï¼ˆé€‚åˆINæŸ¥è¯¢ï¼‰
        `ALTER TABLE events ADD INDEX idx_country_set country TYPE set(100) GRANULARITY 4`,
        
        // Bloom Filterç´¢å¼•ï¼ˆé€‚åˆç­‰å€¼æŸ¥è¯¢ï¼‰
        `ALTER TABLE events ADD INDEX idx_event_id_bloom event_id TYPE bloom_filter GRANULARITY 4`,
    }
    
    for _, query := range queries {
        if err := r.conn.Exec(ctx, query); err != nil {
            return err
        }
    }
    
    fmt.Println("âœ… è·³æ•°ç´¢å¼•åˆ›å»ºæˆåŠŸ")
    
    return nil
}
```

### æ‰¹é‡å†™å…¥ä¼˜åŒ–

```go
// BatchWriter æ‰¹é‡å†™å…¥å™¨
type BatchWriter struct {
    repo      *EventRepository
    batchSize int
    buffer    []*Event
    mu        sync.Mutex
    ticker    *time.Ticker
    done      chan struct{}
}

// NewBatchWriter åˆ›å»ºæ‰¹é‡å†™å…¥å™¨
func NewBatchWriter(repo *EventRepository, batchSize int, flushInterval time.Duration) *BatchWriter {
    bw := &BatchWriter{
        repo:      repo,
        batchSize: batchSize,
        buffer:    make([]*Event, 0, batchSize),
        ticker:    time.NewTicker(flushInterval),
        done:      make(chan struct{}),
    }
    
    // å¯åŠ¨å®šæ—¶åˆ·æ–°
    go bw.autoFlush()
    
    return bw
}

// Write å†™å…¥äº‹ä»¶
func (bw *BatchWriter) Write(event *Event) error {
    bw.mu.Lock()
    defer bw.mu.Unlock()
    
    bw.buffer = append(bw.buffer, event)
    
    // è¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œç«‹å³åˆ·æ–°
    if len(bw.buffer) >= bw.batchSize {
        return bw.flush()
    }
    
    return nil
}

// flush åˆ·æ–°ç¼“å†²åŒº
func (bw *BatchWriter) flush() error {
    if len(bw.buffer) == 0 {
        return nil
    }
    
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    err := bw.repo.BatchInsert(ctx, bw.buffer)
    if err != nil {
        return err
    }
    
    bw.buffer = bw.buffer[:0] // æ¸…ç©ºç¼“å†²åŒº
    
    return nil
}

// autoFlush è‡ªåŠ¨åˆ·æ–°
func (bw *BatchWriter) autoFlush() {
    for {
        select {
        case <-bw.ticker.C:
            bw.mu.Lock()
            bw.flush()
            bw.mu.Unlock()
            
        case <-bw.done:
            return
        }
    }
}

// Close å…³é—­å†™å…¥å™¨
func (bw *BatchWriter) Close() error {
    close(bw.done)
    bw.ticker.Stop()
    
    bw.mu.Lock()
    defer bw.mu.Unlock()
    
    return bw.flush()
}
```

---

## ç›‘æ§ä¸è¿ç»´

### ç³»ç»Ÿç›‘æ§

```go
// GetSystemMetrics è·å–ç³»ç»ŸæŒ‡æ ‡
func (d *Dashboard) GetSystemMetrics(ctx context.Context) (*SystemMetrics, error) {
    query := `
    SELECT 
        uptime() as uptime,
        -- æŸ¥è¯¢ç»Ÿè®¡
        (SELECT count() FROM system.query_log WHERE event_time >= now() - INTERVAL 1 MINUTE) as queries_1m,
        -- æ…¢æŸ¥è¯¢
        (SELECT count() FROM system.query_log WHERE query_duration_ms > 1000 AND event_time >= now() - INTERVAL 1 HOUR) as slow_queries_1h,
        -- å½“å‰è¿æ¥æ•°
        (SELECT count() FROM system.processes) as current_connections,
        -- è¡¨å¤§å°
        (SELECT sum(bytes) FROM system.parts WHERE table = 'events' AND active = 1) as table_size_bytes,
        -- è¡Œæ•°
        (SELECT sum(rows) FROM system.parts WHERE table = 'events' AND active = 1) as table_rows
    `
    
    row := d.conn.QueryRow(ctx, query)
    
    var metrics SystemMetrics
    if err := row.Scan(
        &metrics.Uptime,
        &metrics.Queries1m,
        &metrics.SlowQueries1h,
        &metrics.CurrentConnections,
        &metrics.TableSizeBytes,
        &metrics.TableRows,
    ); err != nil {
        return nil, err
    }
    
    return &metrics, nil
}

// SystemMetrics ç³»ç»ŸæŒ‡æ ‡
type SystemMetrics struct {
    Uptime             uint64
    Queries1m          uint64
    SlowQueries1h      uint64
    CurrentConnections uint64
    TableSizeBytes     uint64
    TableRows          uint64
}
```

---

## æœ€ä½³å®è·µ

### 1. è¡¨è®¾è®¡åŸåˆ™

- âœ… ä½¿ç”¨åˆé€‚çš„è¡¨å¼•æ“ï¼ˆMergeTreeå®¶æ—ï¼‰
- âœ… é€‰æ‹©åˆé€‚çš„ORDER BYï¼ˆå½±å“æŸ¥è¯¢æ€§èƒ½ï¼‰
- âœ… åˆç†è®¾ç½®åˆ†åŒºï¼ˆé€šå¸¸æŒ‰æ—¶é—´åˆ†åŒºï¼‰
- âœ… ä½¿ç”¨TTLè‡ªåŠ¨æ¸…ç†æ—§æ•°æ®
- âœ… é¿å…è¿‡å¤šçš„åˆ—ï¼ˆå½±å“å‹ç¼©ç‡ï¼‰

### 2. æŸ¥è¯¢ä¼˜åŒ–

- âœ… åœ¨ORDER BYçš„ç¬¬ä¸€åˆ—ä¸Šè¿‡æ»¤æ•ˆæœæœ€å¥½
- âœ… ä½¿ç”¨PREWHEREè¿›è¡Œåˆ—è£å‰ª
- âœ… é¿å…SELECT *
- âœ… åˆç†ä½¿ç”¨LIMIT
- âœ… å¯¹å¤§æ•°æ®é›†ä½¿ç”¨SAMPLE

### 3. å†™å…¥ä¼˜åŒ–

- âœ… æ‰¹é‡æ’å…¥ï¼ˆè‡³å°‘1000è¡Œï¼‰
- âœ… ä½¿ç”¨å¼‚æ­¥æ’å…¥
- âœ… é¿å…é¢‘ç¹çš„å°æ‰¹æ¬¡å†™å…¥
- âœ… åˆç†è®¾ç½®mergeå‚æ•°

### 4. ç›‘æ§å‘Šè­¦

```go
// ç›‘æ§æŒ‡æ ‡
- æŸ¥è¯¢å»¶è¿Ÿ (P50, P95, P99)
- æ…¢æŸ¥è¯¢æ•°é‡
- æ’å…¥é€Ÿç‡ (rows/s)
- ç£ç›˜ä½¿ç”¨ç‡
- å‰¯æœ¬å»¶è¿Ÿ
- Mergeé€Ÿåº¦
```

---

## æ€»ç»“

ClickHouse + Go å¼€å‘çš„æ ¸å¿ƒè¦ç‚¹ï¼š

1. **åˆ—å¼å­˜å‚¨ä¼˜åŠ¿**: é€‚åˆOLAPåœºæ™¯ï¼ŒèšåˆæŸ¥è¯¢æ€§èƒ½å“è¶Š
2. **æ‰¹é‡æ“ä½œ**: å°½é‡æ‰¹é‡æ’å…¥å’ŒæŸ¥è¯¢ï¼Œæå‡æ€§èƒ½
3. **åˆç†åˆ†åŒº**: æŒ‰æ—¶é—´åˆ†åŒºï¼Œä¾¿äºæ•°æ®ç®¡ç†å’ŒæŸ¥è¯¢
4. **ç´¢å¼•ç­–ç•¥**: ORDER BYé€‰æ‹©è¦æ…é‡ï¼Œå½±å“æŸ¥è¯¢æ•ˆç‡
5. **ç‰©åŒ–è§†å›¾**: é¢„èšåˆåŠ é€ŸæŸ¥è¯¢
6. **å®æ—¶åˆ†æ**: æ”¯æŒç§’çº§æ•°æ®åˆ†æ
7. **æ°´å¹³æ‰©å±•**: åˆ†å¸ƒå¼è¡¨å®ç°æµ·é‡æ•°æ®å¤„ç†
8. **TTLç®¡ç†**: è‡ªåŠ¨æ¸…ç†è¿‡æœŸæ•°æ®

---

**ç»´æŠ¤è€…**: Documentation Team  
**åˆ›å»ºæ—¥æœŸ**: 2025-10-22  
**æœ€åæ›´æ–°**: 2025-10-22  
**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ
