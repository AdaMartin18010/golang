# Go 1.25.3 云原生部署完整实战

**文档类型**: 云原生部署指南  
**Go版本**: Go 1.25.3  
**难度等级**: ⭐⭐⭐⭐⭐ (专家级)  
**最后更新**: 2025年10月22日

---

## 📖 文档说明

本文档展示Go 1.25.3应用的**云原生部署完整方案**，从容器化到Kubernetes生产部署：

- ✅ Docker容器化最佳实践
- ✅ Kubernetes部署配置
- ✅ Helm Charts管理
- ✅ CI/CD自动化部署
- ✅ 服务网格（Service Mesh）
- ✅ 监控与日志
- ✅ 高可用与自动扩缩容

---

## 目录

- [Go 1.25.3 云原生部署完整实战](#go-1253-云原生部署完整实战)
  - [📖 文档说明](#-文档说明)
  - [目录](#目录)
  - [1. Docker容器化](#1-docker容器化)
    - [1.1 多阶段构建Dockerfile](#11-多阶段构建dockerfile)
    - [1.2 .dockerignore优化](#12-dockerignore优化)
    - [1.3 Docker Compose本地开发](#13-docker-compose本地开发)
  - [2. Kubernetes基础部署](#2-kubernetes基础部署)
    - [2.1 Deployment配置](#21-deployment配置)
    - [2.2 Service配置](#22-service配置)
    - [2.3 Ingress配置](#23-ingress配置)
    - [2.4 ConfigMap与Secret](#24-configmap与secret)
  - [3. Helm Charts管理](#3-helm-charts管理)
    - [3.1 Helm Chart结构](#31-helm-chart结构)
    - [3.2 Chart.yaml](#32-chartyaml)
    - [3.3 values.yaml](#33-valuesyaml)
    - [3.4 部署命令](#34-部署命令)
  - [4. CI/CD自动化](#4-cicd自动化)
    - [4.1 GitHub Actions](#41-github-actions)
    - [4.2 GitLab CI](#42-gitlab-ci)
  - [5. 服务网格Istio](#5-服务网格istio)
    - [5.1 Istio Gateway](#51-istio-gateway)
    - [5.2 VirtualService](#52-virtualservice)
    - [5.3 DestinationRule](#53-destinationrule)
  - [6. 监控与日志](#6-监控与日志)
    - [6.1 Prometheus ServiceMonitor](#61-prometheus-servicemonitor)
    - [6.2 应用监控代码](#62-应用监控代码)
  - [7. 高可用与扩缩容](#7-高可用与扩缩容)
    - [7.1 HorizontalPodAutoscaler](#71-horizontalpodautoscaler)
    - [7.2 PodDisruptionBudget](#72-poddisruptionbudget)
  - [8. 完整案例](#8-完整案例)
    - [8.1 健康检查实现](#81-健康检查实现)
  - [📚 云原生最佳实践](#-云原生最佳实践)
    - [Docker最佳实践](#docker最佳实践)
    - [Kubernetes最佳实践](#kubernetes最佳实践)
    - [监控与可观测性](#监控与可观测性)
  - [🎯 总结](#-总结)

---

## 1. Docker容器化

### 1.1 多阶段构建Dockerfile

**最佳实践Dockerfile**:

```dockerfile
# 第一阶段：构建
FROM golang:1.25.3-alpine AS builder

# 设置工作目录
WORKDIR /build

# 安装必要工具
RUN apk add --no-cache git ca-certificates tzdata

# 复制依赖文件
COPY go.mod go.sum ./
RUN go mod download

# 复制源代码
COPY . .

# 构建应用
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build \
    -ldflags="-w -s" \
    -o app \
    ./cmd/server

# 第二阶段：运行
FROM scratch

# 从builder复制必要文件
COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=builder /build/app /app

# 复制配置文件
COPY --from=builder /build/configs /configs

# 暴露端口
EXPOSE 8080

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD ["/app", "healthcheck"]

# 运行应用
ENTRYPOINT ["/app"]
```

**优化点**:

- ✅ 多阶段构建减小镜像体积（from 800MB → 20MB）
- ✅ 使用scratch基础镜像
- ✅ 静态编译（CGO_ENABLED=0）
- ✅ 移除调试信息（-ldflags="-w -s"）
- ✅ 内置健康检查

---

### 1.2 .dockerignore优化

```text
# .dockerignore
.git
.gitignore
.github
*.md
.vscode
.idea

# 测试文件
*_test.go
testdata/

# 构建产物
bin/
dist/
*.exe
*.dll
*.so
*.dylib

# 临时文件
*.swp
*.swo
*~
.DS_Store

# 依赖
vendor/
```

---

### 1.3 Docker Compose本地开发

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - DB_HOST=postgres
      - REDIS_HOST=redis
      - ENV=development
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./configs:/configs:ro
    networks:
      - app-network
    restart: unless-stopped

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: myapp
      POSTGRES_PASSWORD: secret
      POSTGRES_DB: myapp
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U myapp"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - app-network

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - app-network

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - app-network

volumes:
  postgres-data:
  redis-data:
  prometheus-data:
  grafana-data:

networks:
  app-network:
    driver: bridge
```

---

## 2. Kubernetes基础部署

### 2.1 Deployment配置

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: production
  labels:
    app: myapp
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
        version: v1.0.0
    spec:
      # 服务账号
      serviceAccountName: myapp
      
      # 镜像拉取密钥
      imagePullSecrets:
        - name: registry-secret
      
      # 容器配置
      containers:
      - name: myapp
        image: myregistry.com/myapp:v1.0.0
        imagePullPolicy: Always
        
        # 端口
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        - name: metrics
          containerPort: 9090
          protocol: TCP
        
        # 环境变量
        env:
        - name: ENV
          value: "production"
        - name: DB_HOST
          valueFrom:
            configMapKeyRef:
              name: myapp-config
              key: db.host
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: myapp-secret
              key: db.password
        
        # 资源限制
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        
        # 健康检查
        livenessProbe:
          httpGet:
            path: /health/live
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /health/ready
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        # 启动探针（Go应用启动较快）
        startupProbe:
          httpGet:
            path: /health/startup
            port: http
          initialDelaySeconds: 0
          periodSeconds: 2
          timeoutSeconds: 3
          failureThreshold: 30
        
        # 挂载
        volumeMounts:
        - name: config
          mountPath: /configs
          readOnly: true
        - name: tmp
          mountPath: /tmp
      
      # 卷
      volumes:
      - name: config
        configMap:
          name: myapp-config
      - name: tmp
        emptyDir: {}
      
      # 亲和性
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - myapp
              topologyKey: kubernetes.io/hostname
```

---

### 2.2 Service配置

```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: production
  labels:
    app: myapp
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP
  - name: metrics
    port: 9090
    targetPort: metrics
    protocol: TCP
  selector:
    app: myapp
  sessionAffinity: None

---
# Headless Service for StatefulSet
apiVersion: v1
kind: Service
metadata:
  name: myapp-headless
  namespace: production
spec:
  clusterIP: None
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  selector:
    app: myapp
```

---

### 2.3 Ingress配置

```yaml
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp
  namespace: production
  annotations:
    # NGINX Ingress
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    
    # 限流
    nginx.ingress.kubernetes.io/limit-rps: "100"
    
    # CORS
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://example.com"
    
    # WebSocket
    nginx.ingress.kubernetes.io/websocket-services: "myapp"
    
    # 超时
    nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
    
    # Let's Encrypt
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - api.example.com
    secretName: myapp-tls
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp
            port:
              number: 80
```

---

### 2.4 ConfigMap与Secret

```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myapp-config
  namespace: production
data:
  app.yaml: |
    server:
      port: 8080
      timeout: 30s
    db:
      host: postgres.production.svc.cluster.local
      port: 5432
      database: myapp
      maxConnections: 25
    redis:
      host: redis.production.svc.cluster.local
      port: 6379
    log:
      level: info
      format: json

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myapp-secret
  namespace: production
type: Opaque
stringData:
  db.password: "your-secure-password"
  jwt.secret: "your-jwt-secret"
  api.key: "your-api-key"
```

---

## 3. Helm Charts管理

### 3.1 Helm Chart结构

```text
myapp/
├── Chart.yaml
├── values.yaml
├── values-dev.yaml
├── values-staging.yaml
├── values-prod.yaml
└── templates/
    ├── deployment.yaml
    ├── service.yaml
    ├── ingress.yaml
    ├── configmap.yaml
    ├── secret.yaml
    ├── hpa.yaml
    ├── pdb.yaml
    ├── serviceaccount.yaml
    └── _helpers.tpl
```

### 3.2 Chart.yaml

```yaml
apiVersion: v2
name: myapp
description: A Helm chart for MyApp Go application
type: application
version: 1.0.0
appVersion: "1.0.0"

keywords:
  - go
  - microservice
  - cloud-native

maintainers:
  - name: Your Name
    email: your.email@example.com

dependencies:
  - name: postgresql
    version: "12.x.x"
    repository: "https://charts.bitnami.com/bitnami"
    condition: postgresql.enabled
  
  - name: redis
    version: "17.x.x"
    repository: "https://charts.bitnami.com/bitnami"
    condition: redis.enabled
```

### 3.3 values.yaml

```yaml
# Default values for myapp
replicaCount: 3

image:
  repository: myregistry.com/myapp
  pullPolicy: IfNotPresent
  tag: ""  # Overrides the image tag whose default is the chart appVersion

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"
  prometheus.io/path: "/metrics"

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true

service:
  type: ClusterIP
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
  hosts:
    - host: api.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: myapp-tls
      hosts:
        - api.example.com

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - myapp
          topologyKey: kubernetes.io/hostname

# Application configuration
config:
  env: production
  logLevel: info
  
# PostgreSQL subchart
postgresql:
  enabled: true
  auth:
    username: myapp
    password: changeme
    database: myapp

# Redis subchart
redis:
  enabled: true
  auth:
    enabled: true
    password: changeme
```

### 3.4 部署命令

```bash
# 创建命名空间
kubectl create namespace production

# 添加依赖仓库
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# 安装/升级（开发环境）
helm upgrade --install myapp ./myapp \
  --namespace development \
  --create-namespace \
  --values ./myapp/values-dev.yaml \
  --wait

# 安装/升级（生产环境）
helm upgrade --install myapp ./myapp \
  --namespace production \
  --values ./myapp/values-prod.yaml \
  --set image.tag=v1.0.0 \
  --wait \
  --timeout 10m

# 回滚
helm rollback myapp --namespace production

# 卸载
helm uninstall myapp --namespace production
```

---

## 4. CI/CD自动化

### 4.1 GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Build and Deploy

on:
  push:
    branches: [main]
    tags:
      - 'v*'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-go@v5
        with:
          go-version: '1.25.3'
          cache: true
      
      - name: Run tests
        run: |
          go test -v -race -coverprofile=coverage.out ./...
          go tool cover -html=coverage.out -o coverage.html
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.out

  build:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha
      
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache
          cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v')
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'
      
      - name: Configure kubectl
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig
      
      - name: Deploy with Helm
        run: |
          helm upgrade --install myapp ./charts/myapp \
            --namespace production \
            --set image.tag=${{ github.sha }} \
            --wait \
            --timeout 10m
      
      - name: Verify deployment
        run: |
          kubectl rollout status deployment/myapp -n production
```

---

### 4.2 GitLab CI

```yaml
# .gitlab-ci.yml
stages:
  - test
  - build
  - deploy

variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA

test:
  stage: test
  image: golang:1.25.3
  script:
    - go test -v -race -coverprofile=coverage.out ./...
    - go tool cover -func=coverage.out
  coverage: '/total:\s+\(statements\)\s+(\d+\.\d+)%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

build:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - docker build -t $IMAGE_TAG .
    - docker push $IMAGE_TAG
  only:
    - main
    - tags

deploy:production:
  stage: deploy
  image: alpine/helm:latest
  before_script:
    - kubectl config use-context production
  script:
    - helm upgrade --install myapp ./charts/myapp 
        --namespace production 
        --set image.tag=$CI_COMMIT_SHORT_SHA 
        --wait
  environment:
    name: production
    url: https://api.example.com
  only:
    - tags
```

---

## 5. 服务网格Istio

### 5.1 Istio Gateway

```yaml
# k8s/istio-gateway.yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: myapp-gateway
  namespace: production
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: myapp-tls
    hosts:
    - api.example.com
```

### 5.2 VirtualService

```yaml
# k8s/virtualservice.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp
  namespace: production
spec:
  hosts:
  - api.example.com
  gateways:
  - myapp-gateway
  http:
  - match:
    - uri:
        prefix: "/api/v1"
    route:
    - destination:
        host: myapp
        port:
          number: 80
        subset: v1
      weight: 90
    - destination:
        host: myapp
        port:
          number: 80
        subset: v2
      weight: 10
    retries:
      attempts: 3
      perTryTimeout: 2s
    timeout: 10s
```

### 5.3 DestinationRule

```yaml
# k8s/destinationrule.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp
  namespace: production
spec:
  host: myapp
  trafficPolicy:
    loadBalancer:
      simple: LEAST_REQUEST
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
        maxRequestsPerConnection: 2
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  subsets:
  - name: v1
    labels:
      version: v1.0.0
  - name: v2
    labels:
      version: v2.0.0
```

---

## 6. 监控与日志

### 6.1 Prometheus ServiceMonitor

```yaml
# k8s/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: myapp
  namespace: production
  labels:
    app: myapp
spec:
  selector:
    matchLabels:
      app: myapp
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

### 6.2 应用监控代码

```go
package monitoring

import (
 "github.com/prometheus/client_golang/prometheus"
 "github.com/prometheus/client_golang/prometheus/promauto"
 "github.com/prometheus/client_golang/prometheus/promhttp"
 "net/http"
 "time"
)

var (
 // HTTP请求总数
 httpRequestsTotal = promauto.NewCounterVec(
  prometheus.CounterOpts{
   Name: "http_requests_total",
   Help: "Total number of HTTP requests",
  },
  []string{"method", "path", "status"},
 )
 
 // HTTP请求延迟
 httpRequestDuration = promauto.NewHistogramVec(
  prometheus.HistogramOpts{
   Name:    "http_request_duration_seconds",
   Help:    "HTTP request duration in seconds",
   Buckets: prometheus.DefBuckets,
  },
  []string{"method", "path"},
 )
 
 // 数据库连接池
 dbConnectionsInUse = promauto.NewGauge(
  prometheus.GaugeOpts{
   Name: "db_connections_in_use",
   Help: "Number of database connections in use",
  },
 )
)

// MetricsMiddleware 监控中间件
func MetricsMiddleware(next http.Handler) http.Handler {
 return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
  start := time.Now()
  
  // 响应包装器
  wrapper := &responseWrapper{ResponseWriter: w, statusCode: 200}
  
  // 处理请求
  next.ServeHTTP(wrapper, r)
  
  // 记录指标
  duration := time.Since(start).Seconds()
  
  httpRequestsTotal.WithLabelValues(
   r.Method,
   r.URL.Path,
   http.StatusText(wrapper.statusCode),
  ).Inc()
  
  httpRequestDuration.WithLabelValues(
   r.Method,
   r.URL.Path,
  ).Observe(duration)
 })
}

type responseWrapper struct {
 http.ResponseWriter
 statusCode int
}

func (rw *responseWrapper) WriteHeader(code int) {
 rw.statusCode = code
 rw.ResponseWriter.WriteHeader(code)
}

// SetupMetrics 设置监控端点
func SetupMetrics() http.Handler {
 return promhttp.Handler()
}
```

---

## 7. 高可用与扩缩容

### 7.1 HorizontalPodAutoscaler

```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
```

### 7.2 PodDisruptionBudget

```yaml
# k8s/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp
  namespace: production
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
```

---

## 8. 完整案例

### 8.1 健康检查实现

```go
package health

import (
 "context"
 "encoding/json"
 "net/http"
 "sync"
 "time"
)

// HealthChecker 健康检查器
type HealthChecker struct {
 checks map[string]Check
 mu     sync.RWMutex
}

// Check 检查函数
type Check func(ctx context.Context) error

// NewHealthChecker 创建健康检查器
func NewHealthChecker() *HealthChecker {
 return &HealthChecker{
  checks: make(map[string]Check),
 }
}

// Register 注册检查
func (hc *HealthChecker) Register(name string, check Check) {
 hc.mu.Lock()
 defer hc.mu.Unlock()
 hc.checks[name] = check
}

// LivenessHandler 存活探针
func (hc *HealthChecker) LivenessHandler() http.HandlerFunc {
 return func(w http.ResponseWriter, r *http.Request) {
  w.WriteHeader(http.StatusOK)
  w.Write([]byte("OK"))
 }
}

// ReadinessHandler 就绪探针
func (hc *HealthChecker) ReadinessHandler() http.HandlerFunc {
 return func(w http.ResponseWriter, r *http.Request) {
  ctx, cancel := context.WithTimeout(r.Context(), 5*time.Second)
  defer cancel()
  
  hc.mu.RLock()
  checks := hc.checks
  hc.mu.RUnlock()
  
  results := make(map[string]string)
  healthy := true
  
  for name, check := range checks {
   if err := check(ctx); err != nil {
    results[name] = "unhealthy: " + err.Error()
    healthy = false
   } else {
    results[name] = "healthy"
   }
  }
  
  status := http.StatusOK
  if !healthy {
   status = http.StatusServiceUnavailable
  }
  
  w.Header().Set("Content-Type", "application/json")
  w.WriteHeader(status)
  json.NewEncoder(w).Encode(map[string]interface{}{
   "status": map[bool]string{true: "healthy", false: "unhealthy"}[healthy],
   "checks": results,
  })
 }
}

// 使用示例
func ExampleHealthCheck(db *sql.DB, redis *redis.Client) {
 health := NewHealthChecker()
 
 // 数据库检查
 health.Register("database", func(ctx context.Context) error {
  return db.PingContext(ctx)
 })
 
 // Redis检查
 health.Register("redis", func(ctx context.Context) error {
  return redis.Ping(ctx).Err()
 })
 
 http.HandleFunc("/health/live", health.LivenessHandler())
 http.HandleFunc("/health/ready", health.ReadinessHandler())
}
```

---

## 📚 云原生最佳实践

### Docker最佳实践

- ✅ 多阶段构建减小镜像体积
- ✅ 使用.dockerignore排除无关文件
- ✅ 不要在容器中运行root用户
- ✅ 使用健康检查
- ✅ 版本化镜像标签

### Kubernetes最佳实践

- ✅ 设置资源requests和limits
- ✅ 实现健康检查（liveness/readiness/startup）
- ✅ 使用Pod反亲和性提高可用性
- ✅ 配置PodDisruptionBudget
- ✅ 使用Secrets管理敏感信息

### 监控与可观测性

- ✅ 暴露Prometheus指标
- ✅ 结构化日志（JSON格式）
- ✅ 实现分布式追踪
- ✅ 设置告警规则
- ✅ 建立可视化面板

---

## 🎯 总结

Go 1.25.3云原生部署关键点：

1. **容器化**: 多阶段构建、镜像优化、健康检查
2. **K8s部署**: Deployment、Service、Ingress完整配置
3. **Helm管理**: Charts复用、多环境配置
4. **CI/CD**: 自动化测试、构建、部署流程
5. **服务网格**: Istio流量管理、熔断、重试
6. **监控**: Prometheus指标、Grafana可视化
7. **高可用**: HPA自动扩缩容、PDB保护

**云原生让Go应用更强大！**

---

<div align="center">

**掌握云原生部署，构建现代化应用**:

[📚 返回目录](../README.md) | [📖 下一章](11-服务网格进阶.md)

Made with ❤️ for Go Developers

</div>

---

**文档版本**: v1.0  
**最后更新**: 2025-10-22  
**Go版本**: Go 1.25.3  
**生产就绪**: ✅
