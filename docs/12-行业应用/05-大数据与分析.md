# Goå¤§æ•°æ®ä¸åˆ†æå®Œå…¨æŒ‡å—

> **ç®€ä»‹**: Goè¯­è¨€åœ¨å¤§æ•°æ®å¤„ç†å’Œåˆ†æé¢†åŸŸçš„å®Œæ•´å®è·µï¼Œæ¶µç›–æ•°æ®ç®¡é“ã€æµå¤„ç†ã€ETLã€å®æ—¶åˆ†æç­‰æ ¸å¿ƒåœºæ™¯

---

## ğŸ“š ç›®å½•

- [Goå¤§æ•°æ®ä¸åˆ†æå®Œå…¨æŒ‡å—](#goå¤§æ•°æ®ä¸åˆ†æå®Œå…¨æŒ‡å—)
  - [ğŸ“š ç›®å½•](#-ç›®å½•)
  - [1. å¤§æ•°æ®æ¶æ„æ¦‚è¿°](#1-å¤§æ•°æ®æ¶æ„æ¦‚è¿°)
    - [Lambdaæ¶æ„](#lambdaæ¶æ„)
    - [Kappaæ¶æ„](#kappaæ¶æ„)
    - [Goåœ¨å¤§æ•°æ®ä¸­çš„ä¼˜åŠ¿](#goåœ¨å¤§æ•°æ®ä¸­çš„ä¼˜åŠ¿)
  - [2. æ•°æ®é‡‡é›†](#2-æ•°æ®é‡‡é›†)
    - [æ—¥å¿—é‡‡é›†](#æ—¥å¿—é‡‡é›†)
    - [æ¶ˆæ¯é˜Ÿåˆ—é›†æˆ](#æ¶ˆæ¯é˜Ÿåˆ—é›†æˆ)
  - [3. æ•°æ®ç®¡é“](#3-æ•°æ®ç®¡é“)
    - [ETL Pipeline](#etl-pipeline)
    - [æ•°æ®è½¬æ¢](#æ•°æ®è½¬æ¢)
  - [4. æµå¤„ç†](#4-æµå¤„ç†)
    - [å®æ—¶æµå¤„ç†æ¡†æ¶](#å®æ—¶æµå¤„ç†æ¡†æ¶)
    - [çª—å£è®¡ç®—](#çª—å£è®¡ç®—)
  - [5. æ‰¹å¤„ç†](#5-æ‰¹å¤„ç†)
    - [MapReduceå®ç°](#mapreduceå®ç°)
    - [æ‰¹é‡æ•°æ®å¤„ç†](#æ‰¹é‡æ•°æ®å¤„ç†)
  - [6. æ•°æ®å­˜å‚¨](#6-æ•°æ®å­˜å‚¨)
    - [åˆ—å¼å­˜å‚¨](#åˆ—å¼å­˜å‚¨)
    - [æ—¶åºæ•°æ®åº“](#æ—¶åºæ•°æ®åº“)
    - [æ•°æ®æ¹–é›†æˆ](#æ•°æ®æ¹–é›†æˆ)
  - [7. æ•°æ®æŸ¥è¯¢ä¸åˆ†æ](#7-æ•°æ®æŸ¥è¯¢ä¸åˆ†æ)
    - [SQLæŸ¥è¯¢å¼•æ“](#sqlæŸ¥è¯¢å¼•æ“)
    - [èšåˆåˆ†æ](#èšåˆåˆ†æ)
  - [8. å®æ—¶è®¡ç®—](#8-å®æ—¶è®¡ç®—)
    - [å®æ—¶æŒ‡æ ‡è®¡ç®—](#å®æ—¶æŒ‡æ ‡è®¡ç®—)
    - [å®æ—¶å‘Šè­¦](#å®æ—¶å‘Šè­¦)
  - [9. æ•°æ®å¯è§†åŒ–](#9-æ•°æ®å¯è§†åŒ–)
    - [æ•°æ®API](#æ•°æ®api)
    - [å›¾è¡¨ç”Ÿæˆ](#å›¾è¡¨ç”Ÿæˆ)
  - [10. å®æˆ˜é¡¹ç›®ï¼šå®æ—¶æ—¥å¿—åˆ†æç³»ç»Ÿ](#10-å®æˆ˜é¡¹ç›®å®æ—¶æ—¥å¿—åˆ†æç³»ç»Ÿ)
    - [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
    - [æ ¸å¿ƒå®ç°](#æ ¸å¿ƒå®ç°)
  - [11. æ€§èƒ½ä¼˜åŒ–](#11-æ€§èƒ½ä¼˜åŒ–)
  - [12. æœ€ä½³å®è·µ](#12-æœ€ä½³å®è·µ)
  - [13. å¼€æºé¡¹ç›®æ¨è](#13-å¼€æºé¡¹ç›®æ¨è)

---

## 1. å¤§æ•°æ®æ¶æ„æ¦‚è¿°

### Lambdaæ¶æ„

```mermaid
graph LR
    Source[æ•°æ®æº] --> Batch[æ‰¹å¤„ç†å±‚]
    Source --> Speed[é€Ÿåº¦å±‚]
    Batch --> Serving[æœåŠ¡å±‚]
    Speed --> Serving
    Serving --> Query[æŸ¥è¯¢]
    
    style Source fill:#e1f5fe
    style Batch fill:#fff3e0
    style Speed fill:#f3e5f5
    style Serving fill:#e8f5e9
```

### Kappaæ¶æ„

```mermaid
graph LR
    Source[æ•°æ®æº] --> Stream[æµå¤„ç†å±‚]
    Stream --> Storage[å­˜å‚¨å±‚]
    Storage --> Query[æŸ¥è¯¢æœåŠ¡]
    
    style Source fill:#e1f5fe
    style Stream fill:#fff3e0
    style Storage fill:#e8f5e9
```

### Goåœ¨å¤§æ•°æ®ä¸­çš„ä¼˜åŠ¿

- âœ… **é«˜å¹¶å‘å¤„ç†**: Goroutineè½»æ¾å¤„ç†æµ·é‡å¹¶å‘ä»»åŠ¡
- âœ… **ä½å†…å­˜å ç”¨**: ç›¸æ¯”Java/Scalaæ›´èŠ‚çœèµ„æº
- âœ… **å¿«é€Ÿå¯åŠ¨**: æ— éœ€JVMé¢„çƒ­ï¼Œé€‚åˆåŠ¨æ€æ‰©ç¼©å®¹
- âœ… **ç®€å•éƒ¨ç½²**: å•ä¸€äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œæ˜“äºå®¹å™¨åŒ–
- âœ… **ä¼˜ç§€æ€§èƒ½**: æ¥è¿‘C++çš„æ‰§è¡Œæ•ˆç‡

---

## 2. æ•°æ®é‡‡é›†

### æ—¥å¿—é‡‡é›†

```go
package collector

import (
    "bufio"
    "context"
    "io"
    "os"
    "path/filepath"
    "sync"
    "time"
)

// LogCollector æ—¥å¿—é‡‡é›†å™¨
type LogCollector struct {
    paths    []string
    output   chan<- LogEntry
    watchers map[string]*FileWatcher
    mu       sync.Mutex
}

type LogEntry struct {
    Timestamp time.Time         `json:"timestamp"`
    Source    string            `json:"source"`
    Level     string            `json:"level"`
    Message   string            `json:"message"`
    Fields    map[string]string `json:"fields"`
}

func NewLogCollector(paths []string, output chan<- LogEntry) *LogCollector {
    return &LogCollector{
        paths:    paths,
        output:   output,
        watchers: make(map[string]*FileWatcher),
    }
}

// Start å¯åŠ¨æ—¥å¿—é‡‡é›†
func (c *LogCollector) Start(ctx context.Context) error {
    for _, path := range c.paths {
        matches, err := filepath.Glob(path)
        if err != nil {
            return err
        }
        
        for _, file := range matches {
            watcher := NewFileWatcher(file, c.output)
            c.watchers[file] = watcher
            go watcher.Watch(ctx)
        }
    }
    
    // ç›‘æ§æ–°æ–‡ä»¶
    go c.watchNewFiles(ctx)
    
    return nil
}

func (c *LogCollector) watchNewFiles(ctx context.Context) {
    ticker := time.NewTicker(10 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            c.checkNewFiles()
        }
    }
}

func (c *LogCollector) checkNewFiles() {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    for _, pattern := range c.paths {
        matches, _ := filepath.Glob(pattern)
        for _, file := range matches {
            if _, exists := c.watchers[file]; !exists {
                watcher := NewFileWatcher(file, c.output)
                c.watchers[file] = watcher
                go watcher.Watch(context.Background())
            }
        }
    }
}

// FileWatcher æ–‡ä»¶ç›‘æ§å™¨
type FileWatcher struct {
    path   string
    output chan<- LogEntry
    offset int64
}

func NewFileWatcher(path string, output chan<- LogEntry) *FileWatcher {
    return &FileWatcher{
        path:   path,
        output: output,
        offset: 0,
    }
}

func (w *FileWatcher) Watch(ctx context.Context) {
    ticker := time.NewTicker(1 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            w.readNewLines()
        }
    }
}

func (w *FileWatcher) readNewLines() error {
    file, err := os.Open(w.path)
    if err != nil {
        return err
    }
    defer file.Close()
    
    // è·å–æ–‡ä»¶å¤§å°
    info, err := file.Stat()
    if err != nil {
        return err
    }
    
    // æ–‡ä»¶è¢«æˆªæ–­æˆ–è½®è½¬
    if info.Size() < w.offset {
        w.offset = 0
    }
    
    // è·³è½¬åˆ°ä¸Šæ¬¡è¯»å–ä½ç½®
    file.Seek(w.offset, io.SeekStart)
    
    scanner := bufio.NewScanner(file)
    for scanner.Scan() {
        line := scanner.Text()
        entry := w.parseLine(line)
        
        select {
        case w.output <- entry:
            w.offset, _ = file.Seek(0, io.SeekCurrent)
        default:
            // ç¼“å†²åŒºæ»¡ï¼Œç­‰å¾…
        }
    }
    
    return scanner.Err()
}

func (w *FileWatcher) parseLine(line string) LogEntry {
    // ç®€åŒ–çš„æ—¥å¿—è§£æ
    return LogEntry{
        Timestamp: time.Now(),
        Source:    w.path,
        Message:   line,
        Fields:    make(map[string]string),
    }
}
```

### æ¶ˆæ¯é˜Ÿåˆ—é›†æˆ

```go
package mq

import (
    "context"
    "encoding/json"
    
    "github.com/Shopify/sarama"
)

// KafkaProducer Kafkaç”Ÿäº§è€…
type KafkaProducer struct {
    producer sarama.SyncProducer
    topic    string
}

func NewKafkaProducer(brokers []string, topic string) (*KafkaProducer, error) {
    config := sarama.NewConfig()
    config.Producer.RequiredAcks = sarama.WaitForAll
    config.Producer.Retry.Max = 5
    config.Producer.Return.Successes = true
    
    producer, err := sarama.NewSyncProducer(brokers, config)
    if err != nil {
        return nil, err
    }
    
    return &KafkaProducer{
        producer: producer,
        topic:    topic,
    }, nil
}

// Send å‘é€æ¶ˆæ¯
func (p *KafkaProducer) Send(ctx context.Context, data interface{}) error {
    payload, err := json.Marshal(data)
    if err != nil {
        return err
    }
    
    msg := &sarama.ProducerMessage{
        Topic: p.topic,
        Value: sarama.ByteEncoder(payload),
    }
    
    _, _, err = p.producer.SendMessage(msg)
    return err
}

func (p *KafkaProducer) Close() error {
    return p.producer.Close()
}

// KafkaConsumer Kafkaæ¶ˆè´¹è€…
type KafkaConsumer struct {
    consumer sarama.ConsumerGroup
    topics   []string
    handler  ConsumerHandler
}

type ConsumerHandler interface {
    Handle(ctx context.Context, message []byte) error
}

func NewKafkaConsumer(brokers []string, groupID string, topics []string, handler ConsumerHandler) (*KafkaConsumer, error) {
    config := sarama.NewConfig()
    config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategyRoundRobin
    config.Consumer.Offsets.Initial = sarama.OffsetOldest
    
    consumer, err := sarama.NewConsumerGroup(brokers, groupID, config)
    if err != nil {
        return nil, err
    }
    
    return &KafkaConsumer{
        consumer: consumer,
        topics:   topics,
        handler:  handler,
    }, nil
}

// Start å¯åŠ¨æ¶ˆè´¹
func (c *KafkaConsumer) Start(ctx context.Context) error {
    for {
        if err := c.consumer.Consume(ctx, c.topics, c); err != nil {
            return err
        }
        
        if ctx.Err() != nil {
            return ctx.Err()
        }
    }
}

// Setup å®ç°sarama.ConsumerGroupHandleræ¥å£
func (c *KafkaConsumer) Setup(sarama.ConsumerGroupSession) error {
    return nil
}

func (c *KafkaConsumer) Cleanup(sarama.ConsumerGroupSession) error {
    return nil
}

func (c *KafkaConsumer) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
    for {
        select {
        case message := <-claim.Messages():
            if message == nil {
                return nil
            }
            
            if err := c.handler.Handle(session.Context(), message.Value); err != nil {
                log.Printf("Handler error: %v", err)
            }
            
            session.MarkMessage(message, "")
            
        case <-session.Context().Done():
            return nil
        }
    }
}
```

---

## 3. æ•°æ®ç®¡é“

### ETL Pipeline

```go
package pipeline

import (
    "context"
    "sync"
)

// Pipeline æ•°æ®ç®¡é“
type Pipeline struct {
    stages []Stage
}

type Stage interface {
    Process(ctx context.Context, input <-chan interface{}) <-chan interface{}
}

func NewPipeline(stages ...Stage) *Pipeline {
    return &Pipeline{stages: stages}
}

// Run è¿è¡Œç®¡é“
func (p *Pipeline) Run(ctx context.Context, input <-chan interface{}) <-chan interface{} {
    output := input
    for _, stage := range p.stages {
        output = stage.Process(ctx, output)
    }
    return output
}

// ExtractStage æå–é˜¶æ®µ
type ExtractStage struct {
    extractor Extractor
}

type Extractor interface {
    Extract(ctx context.Context) (<-chan interface{}, error)
}

func NewExtractStage(extractor Extractor) *ExtractStage {
    return &ExtractStage{extractor: extractor}
}

func (s *ExtractStage) Process(ctx context.Context, input <-chan interface{}) <-chan interface{} {
    output, _ := s.extractor.Extract(ctx)
    return output
}

// TransformStage è½¬æ¢é˜¶æ®µ
type TransformStage struct {
    transformer Transformer
    workers     int
}

type Transformer interface {
    Transform(ctx context.Context, data interface{}) (interface{}, error)
}

func NewTransformStage(transformer Transformer, workers int) *TransformStage {
    return &TransformStage{
        transformer: transformer,
        workers:     workers,
    }
}

func (s *TransformStage) Process(ctx context.Context, input <-chan interface{}) <-chan interface{} {
    output := make(chan interface{}, 100)
    
    var wg sync.WaitGroup
    for i := 0; i < s.workers; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            for data := range input {
                transformed, err := s.transformer.Transform(ctx, data)
                if err != nil {
                    log.Printf("Transform error: %v", err)
                    continue
                }
                
                select {
                case output <- transformed:
                case <-ctx.Done():
                    return
                }
            }
        }()
    }
    
    go func() {
        wg.Wait()
        close(output)
    }()
    
    return output
}

// LoadStage åŠ è½½é˜¶æ®µ
type LoadStage struct {
    loader  Loader
    workers int
}

type Loader interface {
    Load(ctx context.Context, data interface{}) error
}

func NewLoadStage(loader Loader, workers int) *LoadStage {
    return &LoadStage{
        loader:  loader,
        workers: workers,
    }
}

func (s *LoadStage) Process(ctx context.Context, input <-chan interface{}) <-chan interface{} {
    output := make(chan interface{}, 100)
    
    var wg sync.WaitGroup
    for i := 0; i < s.workers; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            for data := range input {
                if err := s.loader.Load(ctx, data); err != nil {
                    log.Printf("Load error: %v", err)
                }
                
                select {
                case output <- data:
                case <-ctx.Done():
                    return
                }
            }
        }()
    }
    
    go func() {
        wg.Wait()
        close(output)
    }()
    
    return output
}
```

### æ•°æ®è½¬æ¢

```go
// JSONTransformer JSONæ•°æ®è½¬æ¢å™¨
type JSONTransformer struct {
    mapping map[string]string
}

func NewJSONTransformer(mapping map[string]string) *JSONTransformer {
    return &JSONTransformer{mapping: mapping}
}

func (t *JSONTransformer) Transform(ctx context.Context, data interface{}) (interface{}, error) {
    input, ok := data.(map[string]interface{})
    if !ok {
        return nil, errors.New("invalid input type")
    }
    
    output := make(map[string]interface{})
    for oldKey, newKey := range t.mapping {
        if value, exists := input[oldKey]; exists {
            output[newKey] = value
        }
    }
    
    return output, nil
}

// FilterTransformer è¿‡æ»¤è½¬æ¢å™¨
type FilterTransformer struct {
    predicate func(interface{}) bool
}

func NewFilterTransformer(predicate func(interface{}) bool) *FilterTransformer {
    return &FilterTransformer{predicate: predicate}
}

func (t *FilterTransformer) Transform(ctx context.Context, data interface{}) (interface{}, error) {
    if t.predicate(data) {
        return data, nil
    }
    return nil, errors.New("filtered out")
}

// EnrichTransformer æ•°æ®å¢å¼ºè½¬æ¢å™¨
type EnrichTransformer struct {
    enricher func(interface{}) (interface{}, error)
}

func NewEnrichTransformer(enricher func(interface{}) (interface{}, error)) *EnrichTransformer {
    return &EnrichTransformer{enricher: enricher}
}

func (t *EnrichTransformer) Transform(ctx context.Context, data interface{}) (interface{}, error) {
    return t.enricher(data)
}
```

---

## 4. æµå¤„ç†

### å®æ—¶æµå¤„ç†æ¡†æ¶

```go
package stream

import (
    "context"
    "sync"
    "time"
)

// Stream æ•°æ®æµ
type Stream struct {
    source <-chan Event
    ops    []Operator
}

type Event struct {
    Timestamp time.Time
    Data      interface{}
    Key       string
}

type Operator interface {
    Apply(ctx context.Context, input <-chan Event) <-chan Event
}

func NewStream(source <-chan Event) *Stream {
    return &Stream{
        source: source,
        ops:    make([]Operator, 0),
    }
}

// Map æ˜ å°„æ“ä½œ
func (s *Stream) Map(fn func(Event) Event) *Stream {
    s.ops = append(s.ops, &MapOperator{fn: fn})
    return s
}

// Filter è¿‡æ»¤æ“ä½œ
func (s *Stream) Filter(fn func(Event) bool) *Stream {
    s.ops = append(s.ops, &FilterOperator{fn: fn})
    return s
}

// Window çª—å£æ“ä½œ
func (s *Stream) Window(size time.Duration) *Stream {
    s.ops = append(s.ops, NewWindowOperator(size))
    return s
}

// Reduce èšåˆæ“ä½œ
func (s *Stream) Reduce(fn func([]Event) Event) *Stream {
    s.ops = append(s.ops, &ReduceOperator{fn: fn})
    return s
}

// Execute æ‰§è¡Œæµå¤„ç†
func (s *Stream) Execute(ctx context.Context) <-chan Event {
    output := s.source
    for _, op := range s.ops {
        output = op.Apply(ctx, output)
    }
    return output
}

// MapOperator æ˜ å°„æ“ä½œç¬¦
type MapOperator struct {
    fn func(Event) Event
}

func (o *MapOperator) Apply(ctx context.Context, input <-chan Event) <-chan Event {
    output := make(chan Event, 100)
    
    go func() {
        defer close(output)
        for {
            select {
            case <-ctx.Done():
                return
            case event, ok := <-input:
                if !ok {
                    return
                }
                
                result := o.fn(event)
                select {
                case output <- result:
                case <-ctx.Done():
                    return
                }
            }
        }
    }()
    
    return output
}

// FilterOperator è¿‡æ»¤æ“ä½œç¬¦
type FilterOperator struct {
    fn func(Event) bool
}

func (o *FilterOperator) Apply(ctx context.Context, input <-chan Event) <-chan Event {
    output := make(chan Event, 100)
    
    go func() {
        defer close(output)
        for {
            select {
            case <-ctx.Done():
                return
            case event, ok := <-input:
                if !ok {
                    return
                }
                
                if o.fn(event) {
                    select {
                    case output <- event:
                    case <-ctx.Done():
                        return
                    }
                }
            }
        }
    }()
    
    return output
}
```

### çª—å£è®¡ç®—

```go
// WindowOperator çª—å£æ“ä½œç¬¦
type WindowOperator struct {
    size   time.Duration
    buffer map[string][]Event
    mu     sync.Mutex
}

func NewWindowOperator(size time.Duration) *WindowOperator {
    op := &WindowOperator{
        size:   size,
        buffer: make(map[string][]Event),
    }
    
    go op.periodicFlush()
    return op
}

func (o *WindowOperator) Apply(ctx context.Context, input <-chan Event) <-chan Event {
    output := make(chan Event, 100)
    
    go func() {
        defer close(output)
        for {
            select {
            case <-ctx.Done():
                return
            case event, ok := <-input:
                if !ok {
                    return
                }
                
                o.addToWindow(event)
            }
        }
    }()
    
    return output
}

func (o *WindowOperator) addToWindow(event Event) {
    o.mu.Lock()
    defer o.mu.Unlock()
    
    key := event.Key
    o.buffer[key] = append(o.buffer[key], event)
}

func (o *WindowOperator) periodicFlush() {
    ticker := time.NewTicker(o.size)
    defer ticker.Stop()
    
    for range ticker.C {
        o.flush()
    }
}

func (o *WindowOperator) flush() {
    o.mu.Lock()
    buffer := o.buffer
    o.buffer = make(map[string][]Event)
    o.mu.Unlock()
    
    for key, events := range buffer {
        log.Printf("Window flush for key %s: %d events", key, len(events))
    }
}

// ReduceOperator èšåˆæ“ä½œç¬¦
type ReduceOperator struct {
    fn func([]Event) Event
}

func (o *ReduceOperator) Apply(ctx context.Context, input <-chan Event) <-chan Event {
    output := make(chan Event, 100)
    
    go func() {
        defer close(output)
        
        batch := make([]Event, 0, 100)
        for {
            select {
            case <-ctx.Done():
                if len(batch) > 0 {
                    result := o.fn(batch)
                    output <- result
                }
                return
                
            case event, ok := <-input:
                if !ok {
                    if len(batch) > 0 {
                        result := o.fn(batch)
                        output <- result
                    }
                    return
                }
                
                batch = append(batch, event)
                if len(batch) >= 100 {
                    result := o.fn(batch)
                    select {
                    case output <- result:
                        batch = batch[:0]
                    case <-ctx.Done():
                        return
                    }
                }
            }
        }
    }()
    
    return output
}
```

---

## 5. æ‰¹å¤„ç†

### MapReduceå®ç°

```go
package mapreduce

import (
    "context"
    "sync"
)

// MapReduce MapReduceæ¡†æ¶
type MapReduce struct {
    mappers  int
    reducers int
}

func NewMapReduce(mappers, reducers int) *MapReduce {
    return &MapReduce{
        mappers:  mappers,
        reducers: reducers,
    }
}

type MapFunc func(key, value interface{}) []KeyValue
type ReduceFunc func(key interface{}, values []interface{}) interface{}

type KeyValue struct {
    Key   interface{}
    Value interface{}
}

// Run è¿è¡ŒMapReduceä½œä¸š
func (mr *MapReduce) Run(ctx context.Context, input []KeyValue, mapFn MapFunc, reduceFn ReduceFunc) []KeyValue {
    // Mapé˜¶æ®µ
    intermediate := mr.mapPhase(ctx, input, mapFn)
    
    // Shuffleé˜¶æ®µ
    partitions := mr.shufflePhase(intermediate)
    
    // Reduceé˜¶æ®µ
    result := mr.reducePhase(ctx, partitions, reduceFn)
    
    return result
}

func (mr *MapReduce) mapPhase(ctx context.Context, input []KeyValue, mapFn MapFunc) []KeyValue {
    var mu sync.Mutex
    var wg sync.WaitGroup
    intermediate := make([]KeyValue, 0)
    
    // å°†è¾“å…¥åˆ†ç‰‡
    chunkSize := (len(input) + mr.mappers - 1) / mr.mappers
    
    for i := 0; i < mr.mappers; i++ {
        start := i * chunkSize
        end := start + chunkSize
        if end > len(input) {
            end = len(input)
        }
        
        if start >= len(input) {
            break
        }
        
        wg.Add(1)
        go func(chunk []KeyValue) {
            defer wg.Done()
            
            local := make([]KeyValue, 0)
            for _, kv := range chunk {
                results := mapFn(kv.Key, kv.Value)
                local = append(local, results...)
            }
            
            mu.Lock()
            intermediate = append(intermediate, local...)
            mu.Unlock()
        }(input[start:end])
    }
    
    wg.Wait()
    return intermediate
}

func (mr *MapReduce) shufflePhase(intermediate []KeyValue) []map[interface{}][]interface{} {
    // æŒ‰keyåˆ†åŒº
    partitions := make([]map[interface{}][]interface{}, mr.reducers)
    for i := range partitions {
        partitions[i] = make(map[interface{}][]interface{})
    }
    
    for _, kv := range intermediate {
        partition := hash(kv.Key) % mr.reducers
        partitions[partition][kv.Key] = append(partitions[partition][kv.Key], kv.Value)
    }
    
    return partitions
}

func (mr *MapReduce) reducePhase(ctx context.Context, partitions []map[interface{}][]interface{}, reduceFn ReduceFunc) []KeyValue {
    var mu sync.Mutex
    var wg sync.WaitGroup
    result := make([]KeyValue, 0)
    
    for _, partition := range partitions {
        wg.Add(1)
        go func(part map[interface{}][]interface{}) {
            defer wg.Done()
            
            local := make([]KeyValue, 0)
            for key, values := range part {
                reduced := reduceFn(key, values)
                local = append(local, KeyValue{Key: key, Value: reduced})
            }
            
            mu.Lock()
            result = append(result, local...)
            mu.Unlock()
        }(partition)
    }
    
    wg.Wait()
    return result
}

func hash(key interface{}) int {
    // ç®€åŒ–çš„hashå‡½æ•°
    return len(fmt.Sprint(key))
}

// WordCountç¤ºä¾‹
func WordCountExample() {
    mr := NewMapReduce(4, 2)
    
    input := []KeyValue{
        {Key: "doc1", Value: "hello world"},
        {Key: "doc2", Value: "hello go"},
        {Key: "doc3", Value: "world go"},
    }
    
    // Mapå‡½æ•°: å°†æ–‡æ¡£æ‹†åˆ†ä¸ºå•è¯
    mapFn := func(key, value interface{}) []KeyValue {
        text := value.(string)
        words := strings.Fields(text)
        
        result := make([]KeyValue, 0)
        for _, word := range words {
            result = append(result, KeyValue{Key: word, Value: 1})
        }
        return result
    }
    
    // Reduceå‡½æ•°: ç»Ÿè®¡å•è¯å‡ºç°æ¬¡æ•°
    reduceFn := func(key interface{}, values []interface{}) interface{} {
        count := 0
        for _ = range values {
            count++
        }
        return count
    }
    
    result := mr.Run(context.Background(), input, mapFn, reduceFn)
    
    for _, kv := range result {
        fmt.Printf("%s: %d\n", kv.Key, kv.Value)
    }
}
```

### æ‰¹é‡æ•°æ®å¤„ç†

```go
package batch

import (
    "context"
    "sync"
)

// BatchProcessor æ‰¹å¤„ç†å™¨
type BatchProcessor struct {
    batchSize int
    workers   int
    processor func([]interface{}) error
}

func NewBatchProcessor(batchSize, workers int, processor func([]interface{}) error) *BatchProcessor {
    return &BatchProcessor{
        batchSize: batchSize,
        workers:   workers,
        processor: processor,
    }
}

// Process å¤„ç†æ•°æ®
func (p *BatchProcessor) Process(ctx context.Context, input <-chan interface{}) error {
    batches := p.batchData(ctx, input)
    return p.processBatches(ctx, batches)
}

func (p *BatchProcessor) batchData(ctx context.Context, input <-chan interface{}) <-chan []interface{} {
    output := make(chan []interface{}, 10)
    
    go func() {
        defer close(output)
        
        batch := make([]interface{}, 0, p.batchSize)
        for {
            select {
            case <-ctx.Done():
                if len(batch) > 0 {
                    output <- batch
                }
                return
                
            case item, ok := <-input:
                if !ok {
                    if len(batch) > 0 {
                        output <- batch
                    }
                    return
                }
                
                batch = append(batch, item)
                if len(batch) >= p.batchSize {
                    select {
                    case output <- batch:
                        batch = make([]interface{}, 0, p.batchSize)
                    case <-ctx.Done():
                        return
                    }
                }
            }
        }
    }()
    
    return output
}

func (p *BatchProcessor) processBatches(ctx context.Context, batches <-chan []interface{}) error {
    var wg sync.WaitGroup
    errCh := make(chan error, p.workers)
    
    for i := 0; i < p.workers; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            
            for {
                select {
                case <-ctx.Done():
                    return
                case batch, ok := <-batches:
                    if !ok {
                        return
                    }
                    
                    if err := p.processor(batch); err != nil {
                        select {
                        case errCh <- err:
                        default:
                        }
                    }
                }
            }
        }()
    }
    
    wg.Wait()
    close(errCh)
    
    // è¿”å›ç¬¬ä¸€ä¸ªé”™è¯¯
    return <-errCh
}
```

---

## 6. æ•°æ®å­˜å‚¨

### åˆ—å¼å­˜å‚¨

```go
package storage

import (
    "database/sql"
    
    _ "github.com/ClickHouse/clickhouse-go"
)

// ClickHouseStore ClickHouseå­˜å‚¨
type ClickHouseStore struct {
    db *sql.DB
}

func NewClickHouseStore(dsn string) (*ClickHouseStore, error) {
    db, err := sql.Open("clickhouse", dsn)
    if err != nil {
        return nil, err
    }
    
    return &ClickHouseStore{db: db}, nil
}

// CreateTable åˆ›å»ºè¡¨
func (s *ClickHouseStore) CreateTable() error {
    query := `
        CREATE TABLE IF NOT EXISTS events (
            timestamp DateTime,
            event_type String,
            user_id UInt64,
            properties String
        ) ENGINE = MergeTree()
        PARTITION BY toYYYYMM(timestamp)
        ORDER BY (event_type, timestamp)
    `
    
    _, err := s.db.Exec(query)
    return err
}

// BatchInsert æ‰¹é‡æ’å…¥
func (s *ClickHouseStore) BatchInsert(events []Event) error {
    tx, err := s.db.Begin()
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    stmt, err := tx.Prepare(`
        INSERT INTO events (timestamp, event_type, user_id, properties) 
        VALUES (?, ?, ?, ?)
    `)
    if err != nil {
        return err
    }
    defer stmt.Close()
    
    for _, event := range events {
        _, err = stmt.Exec(
            event.Timestamp,
            event.Type,
            event.UserID,
            event.Properties,
        )
        if err != nil {
            return err
        }
    }
    
    return tx.Commit()
}

// Query æŸ¥è¯¢æ•°æ®
func (s *ClickHouseStore) Query(query string, args ...interface{}) (*sql.Rows, error) {
    return s.db.Query(query, args...)
}
```

### æ—¶åºæ•°æ®åº“

```go
// InfluxDBStore InfluxDBå­˜å‚¨
type InfluxDBStore struct {
    client influxdb2.Client
    org    string
    bucket string
}

func NewInfluxDBStore(url, token, org, bucket string) *InfluxDBStore {
    client := influxdb2.NewClient(url, token)
    
    return &InfluxDBStore{
        client: client,
        org:    org,
        bucket: bucket,
    }
}

// Write å†™å…¥æ•°æ®ç‚¹
func (s *InfluxDBStore) Write(ctx context.Context, measurement string, tags map[string]string, fields map[string]interface{}, timestamp time.Time) error {
    writeAPI := s.client.WriteAPIBlocking(s.org, s.bucket)
    
    p := influxdb2.NewPoint(measurement, tags, fields, timestamp)
    
    return writeAPI.WritePoint(ctx, p)
}

// BatchWrite æ‰¹é‡å†™å…¥
func (s *InfluxDBStore) BatchWrite(ctx context.Context, points []*influxdb2.Point) error {
    writeAPI := s.client.WriteAPIBlocking(s.org, s.bucket)
    return writeAPI.WritePoints(ctx, points)
}

// Query æŸ¥è¯¢æ•°æ®
func (s *InfluxDBStore) Query(ctx context.Context, query string) ([]map[string]interface{}, error) {
    queryAPI := s.client.QueryAPI(s.org)
    
    result, err := queryAPI.Query(ctx, query)
    if err != nil {
        return nil, err
    }
    
    var results []map[string]interface{}
    for result.Next() {
        record := result.Record()
        data := map[string]interface{}{
            "time":   record.Time(),
            "value":  record.Value(),
            "field":  record.Field(),
            "measurement": record.Measurement(),
        }
        
        for key, value := range record.Values() {
            data[key] = value
        }
        
        results = append(results, data)
    }
    
    return results, result.Err()
}

func (s *InfluxDBStore) Close() {
    s.client.Close()
}
```

### æ•°æ®æ¹–é›†æˆ

```go
package datalake

import (
    "context"
    "io"
    
    "github.com/aws/aws-sdk-go/aws"
    "github.com/aws/aws-sdk-go/aws/session"
    "github.com/aws/aws-sdk-go/service/s3"
)

// S3DataLake S3æ•°æ®æ¹–
type S3DataLake struct {
    client *s3.S3
    bucket string
}

func NewS3DataLake(region, bucket string) (*S3DataLake, error) {
    sess, err := session.NewSession(&aws.Config{
        Region: aws.String(region),
    })
    if err != nil {
        return nil, err
    }
    
    return &S3DataLake{
        client: s3.New(sess),
        bucket: bucket,
    }, nil
}

// Write å†™å…¥æ•°æ®
func (dl *S3DataLake) Write(ctx context.Context, key string, data io.Reader) error {
    _, err := dl.client.PutObjectWithContext(ctx, &s3.PutObjectInput{
        Bucket: aws.String(dl.bucket),
        Key:    aws.String(key),
        Body:   data,
    })
    return err
}

// Read è¯»å–æ•°æ®
func (dl *S3DataLake) Read(ctx context.Context, key string) (io.ReadCloser, error) {
    result, err := dl.client.GetObjectWithContext(ctx, &s3.GetObjectInput{
        Bucket: aws.String(dl.bucket),
        Key:    aws.String(key),
    })
    if err != nil {
        return nil, err
    }
    
    return result.Body, nil
}

// List åˆ—å‡ºæ–‡ä»¶
func (dl *S3DataLake) List(ctx context.Context, prefix string) ([]string, error) {
    result, err := dl.client.ListObjectsV2WithContext(ctx, &s3.ListObjectsV2Input{
        Bucket: aws.String(dl.bucket),
        Prefix: aws.String(prefix),
    })
    if err != nil {
        return nil, err
    }
    
    keys := make([]string, 0, len(result.Contents))
    for _, obj := range result.Contents {
        keys = append(keys, *obj.Key)
    }
    
    return keys, nil
}
```

---

## 7. æ•°æ®æŸ¥è¯¢ä¸åˆ†æ

### SQLæŸ¥è¯¢å¼•æ“

```go
package query

import (
    "context"
    "database/sql"
)

// QueryEngine æŸ¥è¯¢å¼•æ“
type QueryEngine struct {
    store DataStore
}

type DataStore interface {
    Query(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error)
}

func NewQueryEngine(store DataStore) *QueryEngine {
    return &QueryEngine{store: store}
}

// ExecuteQuery æ‰§è¡ŒæŸ¥è¯¢
func (e *QueryEngine) ExecuteQuery(ctx context.Context, query string, args ...interface{}) ([]map[string]interface{}, error) {
    rows, err := e.store.Query(ctx, query, args...)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    return scanRows(rows)
}

func scanRows(rows *sql.Rows) ([]map[string]interface{}, error) {
    columns, err := rows.Columns()
    if err != nil {
        return nil, err
    }
    
    var results []map[string]interface{}
    for rows.Next() {
        values := make([]interface{}, len(columns))
        valuePtrs := make([]interface{}, len(columns))
        
        for i := range values {
            valuePtrs[i] = &values[i]
        }
        
        if err := rows.Scan(valuePtrs...); err != nil {
            return nil, err
        }
        
        row := make(map[string]interface{})
        for i, col := range columns {
            row[col] = values[i]
        }
        
        results = append(results, row)
    }
    
    return results, rows.Err()
}
```

### èšåˆåˆ†æ

```go
// Aggregator èšåˆå™¨
type Aggregator struct {
    engine *QueryEngine
}

func NewAggregator(engine *QueryEngine) *Aggregator {
    return &Aggregator{engine: engine}
}

// Count è®¡æ•°
func (a *Aggregator) Count(ctx context.Context, table, condition string) (int64, error) {
    query := fmt.Sprintf("SELECT COUNT(*) as count FROM %s WHERE %s", table, condition)
    
    results, err := a.engine.ExecuteQuery(ctx, query)
    if err != nil {
        return 0, err
    }
    
    if len(results) == 0 {
        return 0, nil
    }
    
    return results[0]["count"].(int64), nil
}

// Sum æ±‚å’Œ
func (a *Aggregator) Sum(ctx context.Context, table, field, condition string) (float64, error) {
    query := fmt.Sprintf("SELECT SUM(%s) as sum FROM %s WHERE %s", field, table, condition)
    
    results, err := a.engine.ExecuteQuery(ctx, query)
    if err != nil {
        return 0, err
    }
    
    if len(results) == 0 {
        return 0, nil
    }
    
    return results[0]["sum"].(float64), nil
}

// Average å¹³å‡å€¼
func (a *Aggregator) Average(ctx context.Context, table, field, condition string) (float64, error) {
    query := fmt.Sprintf("SELECT AVG(%s) as avg FROM %s WHERE %s", field, table, condition)
    
    results, err := a.engine.ExecuteQuery(ctx, query)
    if err != nil {
        return 0, err
    }
    
    if len(results) == 0 {
        return 0, nil
    }
    
    return results[0]["avg"].(float64), nil
}

// GroupBy åˆ†ç»„èšåˆ
func (a *Aggregator) GroupBy(ctx context.Context, table, groupField, aggField, aggFunc, condition string) ([]map[string]interface{}, error) {
    query := fmt.Sprintf(
        "SELECT %s, %s(%s) as value FROM %s WHERE %s GROUP BY %s",
        groupField, aggFunc, aggField, table, condition, groupField,
    )
    
    return a.engine.ExecuteQuery(ctx, query)
}
```

---

## 8. å®æ—¶è®¡ç®—

### å®æ—¶æŒ‡æ ‡è®¡ç®—

```go
package realtime

import (
    "context"
    "sync"
    "time"
)

// MetricsCalculator å®æ—¶æŒ‡æ ‡è®¡ç®—å™¨
type MetricsCalculator struct {
    metrics map[string]*Metric
    mu      sync.RWMutex
}

type Metric struct {
    Name      string
    Value     float64
    Timestamp time.Time
    Tags      map[string]string
}

func NewMetricsCalculator() *MetricsCalculator {
    return &MetricsCalculator{
        metrics: make(map[string]*Metric),
    }
}

// Update æ›´æ–°æŒ‡æ ‡
func (c *MetricsCalculator) Update(name string, value float64, tags map[string]string) {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    key := c.metricKey(name, tags)
    c.metrics[key] = &Metric{
        Name:      name,
        Value:     value,
        Timestamp: time.Now(),
        Tags:      tags,
    }
}

// Increment é€’å¢è®¡æ•°
func (c *MetricsCalculator) Increment(name string, delta float64, tags map[string]string) {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    key := c.metricKey(name, tags)
    if metric, exists := c.metrics[key]; exists {
        metric.Value += delta
        metric.Timestamp = time.Now()
    } else {
        c.metrics[key] = &Metric{
            Name:      name,
            Value:     delta,
            Timestamp: time.Now(),
            Tags:      tags,
        }
    }
}

// Get è·å–æŒ‡æ ‡
func (c *MetricsCalculator) Get(name string, tags map[string]string) (*Metric, bool) {
    c.mu.RLock()
    defer c.mu.RUnlock()
    
    key := c.metricKey(name, tags)
    metric, exists := c.metrics[key]
    return metric, exists
}

// GetAll è·å–æ‰€æœ‰æŒ‡æ ‡
func (c *MetricsCalculator) GetAll() []*Metric {
    c.mu.RLock()
    defer c.mu.RUnlock()
    
    metrics := make([]*Metric, 0, len(c.metrics))
    for _, metric := range c.metrics {
        metrics = append(metrics, metric)
    }
    
    return metrics
}

func (c *MetricsCalculator) metricKey(name string, tags map[string]string) string {
    // ç®€åŒ–çš„keyç”Ÿæˆ
    key := name
    for k, v := range tags {
        key += fmt.Sprintf(",%s=%s", k, v)
    }
    return key
}

// RollingWindow æ»šåŠ¨çª—å£è®¡ç®—
type RollingWindow struct {
    size    time.Duration
    buckets []float64
    times   []time.Time
    mu      sync.Mutex
}

func NewRollingWindow(size time.Duration) *RollingWindow {
    return &RollingWindow{
        size:    size,
        buckets: make([]float64, 0),
        times:   make([]time.Time, 0),
    }
}

// Add æ·»åŠ æ•°æ®ç‚¹
func (w *RollingWindow) Add(value float64) {
    w.mu.Lock()
    defer w.mu.Unlock()
    
    now := time.Now()
    cutoff := now.Add(-w.size)
    
    // æ¸…é™¤è¿‡æœŸæ•°æ®
    i := 0
    for i < len(w.times) && w.times[i].Before(cutoff) {
        i++
    }
    w.buckets = w.buckets[i:]
    w.times = w.times[i:]
    
    // æ·»åŠ æ–°æ•°æ®
    w.buckets = append(w.buckets, value)
    w.times = append(w.times, now)
}

// Average è®¡ç®—å¹³å‡å€¼
func (w *RollingWindow) Average() float64 {
    w.mu.Lock()
    defer w.mu.Unlock()
    
    if len(w.buckets) == 0 {
        return 0
    }
    
    sum := 0.0
    for _, v := range w.buckets {
        sum += v
    }
    
    return sum / float64(len(w.buckets))
}

// Sum æ±‚å’Œ
func (w *RollingWindow) Sum() float64 {
    w.mu.Lock()
    defer w.mu.Unlock()
    
    sum := 0.0
    for _, v := range w.buckets {
        sum += v
    }
    
    return sum
}
```

### å®æ—¶å‘Šè­¦

```go
// AlertEngine å®æ—¶å‘Šè­¦å¼•æ“
type AlertEngine struct {
    rules   []AlertRule
    actions []AlertAction
    mu      sync.RWMutex
}

type AlertRule struct {
    Name      string
    Condition func(*Metric) bool
    Message   string
}

type AlertAction interface {
    Execute(rule *AlertRule, metric *Metric) error
}

func NewAlertEngine() *AlertEngine {
    return &AlertEngine{
        rules:   make([]AlertRule, 0),
        actions: make([]AlertAction, 0),
    }
}

// AddRule æ·»åŠ å‘Šè­¦è§„åˆ™
func (e *AlertEngine) AddRule(rule AlertRule) {
    e.mu.Lock()
    defer e.mu.Unlock()
    e.rules = append(e.rules, rule)
}

// AddAction æ·»åŠ å‘Šè­¦åŠ¨ä½œ
func (e *AlertEngine) AddAction(action AlertAction) {
    e.mu.Lock()
    defer e.mu.Unlock()
    e.actions = append(e.actions, action)
}

// Check æ£€æŸ¥æŒ‡æ ‡å¹¶è§¦å‘å‘Šè­¦
func (e *AlertEngine) Check(metric *Metric) {
    e.mu.RLock()
    rules := e.rules
    actions := e.actions
    e.mu.RUnlock()
    
    for _, rule := range rules {
        if rule.Condition(metric) {
            for _, action := range actions {
                if err := action.Execute(&rule, metric); err != nil {
                    log.Printf("Alert action failed: %v", err)
                }
            }
        }
    }
}

// EmailAlertAction é‚®ä»¶å‘Šè­¦
type EmailAlertAction struct {
    smtpServer string
    from       string
    to         []string
}

func (a *EmailAlertAction) Execute(rule *AlertRule, metric *Metric) error {
    subject := fmt.Sprintf("Alert: %s", rule.Name)
    body := fmt.Sprintf(
        "Rule: %s\nMessage: %s\nMetric: %s = %f\nTags: %v\nTimestamp: %s",
        rule.Name, rule.Message, metric.Name, metric.Value, metric.Tags, metric.Timestamp,
    )
    
    // å‘é€é‚®ä»¶
    log.Printf("Sending email alert: %s", subject)
    
    return nil
}
```

---

## 9. æ•°æ®å¯è§†åŒ–

### æ•°æ®API

```go
package api

import (
    "encoding/json"
    "net/http"
    
    "github.com/gorilla/mux"
)

// DataAPI æ•°æ®APIæœåŠ¡
type DataAPI struct {
    engine     *QueryEngine
    aggregator *Aggregator
    calculator *MetricsCalculator
}

func NewDataAPI(engine *QueryEngine, aggregator *Aggregator, calculator *MetricsCalculator) *DataAPI {
    return &DataAPI{
        engine:     engine,
        aggregator: aggregator,
        calculator: calculator,
    }
}

// SetupRoutes è®¾ç½®è·¯ç”±
func (api *DataAPI) SetupRoutes(router *mux.Router) {
    router.HandleFunc("/api/query", api.handleQuery).Methods("POST")
    router.HandleFunc("/api/aggregate", api.handleAggregate).Methods("POST")
    router.HandleFunc("/api/metrics", api.handleMetrics).Methods("GET")
}

func (api *DataAPI) handleQuery(w http.ResponseWriter, r *http.Request) {
    var req struct {
        Query string        `json:"query"`
        Args  []interface{} `json:"args"`
    }
    
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, err.Error(), http.StatusBadRequest)
        return
    }
    
    results, err := api.engine.ExecuteQuery(r.Context(), req.Query, req.Args...)
    if err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(results)
}

func (api *DataAPI) handleAggregate(w http.ResponseWriter, r *http.Request) {
    var req struct {
        Table      string `json:"table"`
        GroupField string `json:"group_field"`
        AggField   string `json:"agg_field"`
        AggFunc    string `json:"agg_func"`
        Condition  string `json:"condition"`
    }
    
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, err.Error(), http.StatusBadRequest)
        return
    }
    
    results, err := api.aggregator.GroupBy(
        r.Context(),
        req.Table,
        req.GroupField,
        req.AggField,
        req.AggFunc,
        req.Condition,
    )
    if err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(results)
}

func (api *DataAPI) handleMetrics(w http.ResponseWriter, r *http.Request) {
    metrics := api.calculator.GetAll()
    
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(metrics)
}
```

### å›¾è¡¨ç”Ÿæˆ

```go
package charts

import (
    "github.com/go-echarts/go-echarts/v2/charts"
    "github.com/go-echarts/go-echarts/v2/opts"
)

// ChartGenerator å›¾è¡¨ç”Ÿæˆå™¨
type ChartGenerator struct{}

func NewChartGenerator() *ChartGenerator {
    return &ChartGenerator{}
}

// GenerateLineChart ç”ŸæˆæŠ˜çº¿å›¾
func (g *ChartGenerator) GenerateLineChart(title string, xAxis []string, series map[string][]float64) *charts.Line {
    line := charts.NewLine()
    
    line.SetGlobalOptions(
        charts.WithTitleOpts(opts.Title{
            Title: title,
        }),
    )
    
    line.SetXAxis(xAxis)
    
    for name, data := range series {
        items := make([]opts.LineData, 0)
        for _, value := range data {
            items = append(items, opts.LineData{Value: value})
        }
        
        line.AddSeries(name, items)
    }
    
    return line
}

// GenerateBarChart ç”ŸæˆæŸ±çŠ¶å›¾
func (g *ChartGenerator) GenerateBarChart(title string, xAxis []string, series map[string][]float64) *charts.Bar {
    bar := charts.NewBar()
    
    bar.SetGlobalOptions(
        charts.WithTitleOpts(opts.Title{
            Title: title,
        }),
    )
    
    bar.SetXAxis(xAxis)
    
    for name, data := range series {
        items := make([]opts.BarData, 0)
        for _, value := range data {
            items = append(items, opts.BarData{Value: value})
        }
        
        bar.AddSeries(name, items)
    }
    
    return bar
}

// GeneratePieChart ç”Ÿæˆé¥¼å›¾
func (g *ChartGenerator) GeneratePieChart(title string, data map[string]float64) *charts.Pie {
    pie := charts.NewPie()
    
    pie.SetGlobalOptions(
        charts.WithTitleOpts(opts.Title{
            Title: title,
        }),
    )
    
    items := make([]opts.PieData, 0)
    for name, value := range data {
        items = append(items, opts.PieData{
            Name:  name,
            Value: value,
        })
    }
    
    pie.AddSeries("pie", items)
    
    return pie
}
```

---

## 10. å®æˆ˜é¡¹ç›®ï¼šå®æ—¶æ—¥å¿—åˆ†æç³»ç»Ÿ

### ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    LogFiles[æ—¥å¿—æ–‡ä»¶] --> Collector[æ—¥å¿—é‡‡é›†å™¨]
    Collector --> Kafka[Kafka]
    Kafka --> StreamProcessor[æµå¤„ç†å™¨]
    StreamProcessor --> Storage[(æ—¶åºæ•°æ®åº“)]
    StreamProcessor --> Cache[(Redisç¼“å­˜)]
    Storage --> QueryAPI[æŸ¥è¯¢API]
    Cache --> QueryAPI
    QueryAPI --> Dashboard[å¯è§†åŒ–ä»ªè¡¨æ¿]
    
    StreamProcessor --> AlertEngine[å‘Šè­¦å¼•æ“]
    AlertEngine --> Notification[é€šçŸ¥æœåŠ¡]
    
    style Collector fill:#e1f5fe
    style StreamProcessor fill:#fff3e0
    style Storage fill:#e8f5e9
    style Dashboard fill:#f3e5f5
```

### æ ¸å¿ƒå®ç°

```go
// cmd/analyzer/main.go
package main

import (
    "context"
    "log"
    "os"
    "os/signal"
    "syscall"
    
    "log-analyzer/internal/collector"
    "log-analyzer/internal/processor"
    "log-analyzer/internal/storage"
)

func main() {
    ctx, cancel := context.WithCancel(context.Background())
    defer cancel()
    
    // åˆå§‹åŒ–Kafkaç”Ÿäº§è€…
    kafkaProducer, err := mq.NewKafkaProducer(
        []string{"localhost:9092"},
        "logs",
    )
    if err != nil {
        log.Fatal(err)
    }
    defer kafkaProducer.Close()
    
    // åˆå§‹åŒ–æ—¥å¿—é‡‡é›†å™¨
    logChan := make(chan collector.LogEntry, 1000)
    logCollector := collector.NewLogCollector(
        []string{"/var/log/app/*.log"},
        logChan,
    )
    
    // å¯åŠ¨é‡‡é›†
    if err := logCollector.Start(ctx); err != nil {
        log.Fatal(err)
    }
    
    // è½¬å‘åˆ°Kafka
    go func() {
        for entry := range logChan {
            if err := kafkaProducer.Send(ctx, entry); err != nil {
                log.Printf("Failed to send to Kafka: %v", err)
            }
        }
    }()
    
    // åˆå§‹åŒ–å­˜å‚¨
    store, err := storage.NewInfluxDBStore(
        "http://localhost:8086",
        "token",
        "org",
        "logs",
    )
    if err != nil {
        log.Fatal(err)
    }
    defer store.Close()
    
    // åˆå§‹åŒ–æµå¤„ç†å™¨
    streamProcessor := processor.NewStreamProcessor(store)
    
    // å¯åŠ¨Kafkaæ¶ˆè´¹è€…
    consumer, err := mq.NewKafkaConsumer(
        []string{"localhost:9092"},
        "log-analyzer",
        []string{"logs"},
        streamProcessor,
    )
    if err != nil {
        log.Fatal(err)
    }
    
    go consumer.Start(ctx)
    
    log.Println("Log Analyzer started")
    
    // ç­‰å¾…ä¸­æ–­ä¿¡å·
    sigCh := make(chan os.Signal, 1)
    signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)
    <-sigCh
    
    log.Println("Shutting down...")
}
```

---

## 11. æ€§èƒ½ä¼˜åŒ–

1. **å¹¶å‘å¤„ç†**
   - ä½¿ç”¨Goroutineæ± æ§åˆ¶å¹¶å‘æ•°
   - åˆç†è®¾ç½®ç¼“å†²åŒºå¤§å°
   - é¿å…ä¸å¿…è¦çš„contextåˆ‡æ¢

2. **å†…å­˜ä¼˜åŒ–**
   - ä½¿ç”¨å¯¹è±¡æ± å‡å°‘GCå‹åŠ›
   - æ‰¹é‡å¤„ç†é™ä½å†…å­˜åˆ†é…
   - åŠæ—¶é‡Šæ”¾ä¸ç”¨çš„èµ„æº

3. **I/Oä¼˜åŒ–**
   - æ‰¹é‡è¯»å†™å‡å°‘ç³»ç»Ÿè°ƒç”¨
   - ä½¿ç”¨ç¼“å†²I/O
   - å¼‚æ­¥å¤„ç†I/Oæ“ä½œ

4. **ç½‘ç»œä¼˜åŒ–**
   - è¿æ¥æ± å¤ç”¨è¿æ¥
   - å‹ç¼©æ•°æ®å‡å°‘ä¼ è¾“é‡
   - æ‰¹é‡å‘é€æ•°æ®

---

## 12. æœ€ä½³å®è·µ

1. **æ•°æ®ç®¡é“è®¾è®¡**
   - âœ… æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•
   - âœ… é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
   - âœ… ç›‘æ§å’Œå¯è§‚æµ‹æ€§

2. **æµå¤„ç†**
   - âœ… å¤„ç†ä¹±åºæ•°æ®
   - âœ… ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰ï¼ˆExactly-Onceï¼‰
   - âœ… çŠ¶æ€ç®¡ç†å’Œå®¹é”™

3. **æ•°æ®å­˜å‚¨**
   - âœ… é€‰æ‹©åˆé€‚çš„å­˜å‚¨æ–¹æ¡ˆ
   - âœ… æ•°æ®åˆ†åŒºå’Œç´¢å¼•ä¼˜åŒ–
   - âœ… æ•°æ®å¤‡ä»½å’Œæ¢å¤

4. **æ€§èƒ½**
   - âœ… åˆç†ä½¿ç”¨å¹¶å‘
   - âœ… æ‰¹é‡å¤„ç†
   - âœ… ç¼“å­˜çƒ­ç‚¹æ•°æ®

---

## 13. å¼€æºé¡¹ç›®æ¨è

1. **Apache Kafka Goå®¢æˆ·ç«¯**
   - åœ°å€: <https://github.com/Shopify/sarama>
   - è¯´æ˜: é«˜æ€§èƒ½Kafkaå®¢æˆ·ç«¯

2. **Benthos**
   - åœ°å€: <https://github.com/benthosdev/benthos>
   - è¯´æ˜: æµå¤„ç†ç‘å£«å†›åˆ€

3. **Vector**
   - åœ°å€: <https://github.com/vectordotdev/vector>
   - è¯´æ˜: é«˜æ€§èƒ½å¯è§‚æµ‹æ€§æ•°æ®ç®¡é“

4. **Watermill**
   - åœ°å€: <https://github.com/ThreeDotsLabs/watermill>
   - è¯´æ˜: Goäº‹ä»¶é©±åŠ¨åº”ç”¨åº“

5. **Pachyderm**
   - åœ°å€: <https://github.com/pachyderm/pachyderm>
   - è¯´æ˜: å®¹å™¨åŒ–æ•°æ®ç®¡é“

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Go Documentation Team  
**æœ€åæ›´æ–°**: 2025å¹´10æœˆ24æ—¥  
**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ  
**é€‚ç”¨ç‰ˆæœ¬**: Go 1.21+
