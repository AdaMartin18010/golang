# Goä¸AIé›†æˆ

**éš¾åº¦**: é«˜çº§ | **é¢„è®¡é˜…è¯»**: 20åˆ†é’Ÿ | **å‰ç½®çŸ¥è¯†**: GoåŸºç¡€ã€AI/MLæ¦‚å¿µ

---

## ğŸ“‹ ç›®å½•

- [1. ğŸ“– æ¦‚å¿µä»‹ç»](#1--æ¦‚å¿µä»‹ç»)
- [2. ğŸ¯ ä¸ºä»€ä¹ˆä½¿ç”¨GoåšAI](#2--ä¸ºä»€ä¹ˆä½¿ç”¨goåšai)
- [3. ğŸ’¡ æœ€ä½³å®è·µ](#3--æœ€ä½³å®è·µ)
- [4. ğŸ“š ç›¸å…³èµ„æº](#4--ç›¸å…³èµ„æº)

---

## 1. ğŸ“– æ¦‚å¿µä»‹ç»

Goåœ¨AIé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¢é•¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡å‹éƒ¨ç½²ã€æ¨ç†æœåŠ¡å’Œæ•°æ®å¤„ç†æ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»å¦‚ä½•å°†Goä¸AI/MLç”Ÿæ€ç³»ç»Ÿé›†æˆã€‚

---

## 2. ğŸ¯ ä¸ºä»€ä¹ˆä½¿ç”¨GoåšAI

### ä¼˜åŠ¿
- **é«˜æ€§èƒ½**: ç¼–è¯‘å‹è¯­è¨€ï¼Œæ¥è¿‘Cçš„æ€§èƒ½
- **å¹¶å‘**: å¤©ç„¶æ”¯æŒé«˜å¹¶å‘ï¼Œé€‚åˆæ¨ç†æœåŠ¡
- **éƒ¨ç½²ç®€å•**: å•ä¸€äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œæ— ä¾èµ–
- **äº‘åŸç”Ÿ**: å®¹å™¨åŒ–éƒ¨ç½²çš„ç†æƒ³é€‰æ‹©

### é€‚ç”¨åœºæ™¯
- æ¨¡å‹æ¨ç†æœåŠ¡
- æ•°æ®é¢„å¤„ç†ç®¡é“
- AIæ¨¡å‹APIç½‘å…³
- å®æ—¶é¢„æµ‹ç³»ç»Ÿ

---

## ğŸ”§ è°ƒç”¨Pythonæ¨¡å‹

### 1. é€šè¿‡HTTP API

```go
package main

import (
    "bytes"
    "encoding/json"
    "net/http"
)

type PredictionRequest struct {
    Data []float64 `json:"data"`
}

type PredictionResponse struct {
    Prediction float64 `json:"prediction"`
    Confidence float64 `json:"confidence"`
}

// è°ƒç”¨Python Flaskæ¨¡å‹æœåŠ¡
func callPythonModel(data []float64) (*PredictionResponse, error) {
    url := "http://localhost:5000/predict"
    
    reqBody := PredictionRequest{Data: data}
    jsonData, _ := json.Marshal(reqBody)
    
    resp, err := http.Post(url, "application/json", bytes.NewBuffer(jsonData))
    if err != nil {
        return nil, err
    }
    defer resp.Body.Close()
    
    var result PredictionResponse
    json.NewDecoder(resp.Body).Decode(&result)
    
    return &result, nil
}

// ä½¿ç”¨ç¤ºä¾‹
func main() {
    data := []float64{1.0, 2.0, 3.0, 4.0}
    result, err := callPythonModel(data)
    if err != nil {
        log.Fatal(err)
    }
    
    fmt.Printf("Prediction: %.2f (Confidence: %.2f%%)\n", 
        result.Prediction, result.Confidence*100)
}
```

---

### 2. é€šè¿‡gRPC

```protobuf
// prediction.proto
syntax = "proto3";

service Predictor {
  rpc Predict(PredictRequest) returns (PredictResponse);
}

message PredictRequest {
  repeated float features = 1;
}

message PredictResponse {
  float prediction = 1;
  float confidence = 2;
}
```

```go
// Goå®¢æˆ·ç«¯
import (
    "context"
    pb "path/to/prediction"
    "google.golang.org/grpc"
)

func callGRPCModel(features []float32) (*pb.PredictResponse, error) {
    conn, err := grpc.Dial("localhost:50051", grpc.WithInsecure())
    if err != nil {
        return nil, err
    }
    defer conn.Close()
    
    client := pb.NewPredictorClient(conn)
    
    req := &pb.PredictRequest{Features: features}
    resp, err := client.Predict(context.Background(), req)
    
    return resp, err
}
```

---

## ğŸš€ TensorFlowé›†æˆ

```go
import (
    tf "github.com/tensorflow/tensorflow/tensorflow/go"
    "github.com/tensorflow/tensorflow/tensorflow/go/op"
)

type TFModel struct {
    model   *tf.SavedModel
    session *tf.Session
}

func LoadTFModel(modelPath string) (*TFModel, error) {
    model, err := tf.LoadSavedModel(modelPath, []string{"serve"}, nil)
    if err != nil {
        return nil, err
    }
    
    return &TFModel{
        model:   model,
        session: model.Session,
    }, nil
}

func (m *TFModel) Predict(input [][]float32) ([]float32, error) {
    // åˆ›å»ºè¾“å…¥tensor
    tensor, err := tf.NewTensor(input)
    if err != nil {
        return nil, err
    }
    
    // è¿è¡Œæ¨ç†
    results, err := m.session.Run(
        map[tf.Output]*tf.Tensor{
            m.model.Graph.Operation("input").Output(0): tensor,
        },
        []tf.Output{
            m.model.Graph.Operation("output").Output(0),
        },
        nil,
    )
    
    if err != nil {
        return nil, err
    }
    
    return results[0].Value().([]float32), nil
}

// ä½¿ç”¨ç¤ºä¾‹
func example() {
    model, _ := LoadTFModel("./saved_model")
    defer model.session.Close()
    
    input := [][]float32{{1.0, 2.0, 3.0}}
    output, _ := model.Predict(input)
    
    fmt.Printf("Prediction: %v\n", output)
}
```

---

## ğŸ”® ONNX Runtime

```go
import "github.com/yalue/onnxruntime_go"

type ONNXModel struct {
    session *onnxruntime.Session
    input   *onnxruntime.Tensor
    output  *onnxruntime.Tensor
}

func LoadONNXModel(modelPath string) (*ONNXModel, error) {
    // åˆå§‹åŒ–ONNX Runtime
    err := onnxruntime.InitializeEnvironment()
    if err != nil {
        return nil, err
    }
    
    // åŠ è½½æ¨¡å‹
    session, err := onnxruntime.NewSession(modelPath, nil)
    if err != nil {
        return nil, err
    }
    
    return &ONNXModel{session: session}, nil
}

func (m *ONNXModel) Predict(data []float32) ([]float32, error) {
    // åˆ›å»ºè¾“å…¥tensor
    inputShape := []int64{1, int64(len(data))}
    inputTensor, err := onnxruntime.NewTensor(inputShape, data)
    if err != nil {
        return nil, err
    }
    defer inputTensor.Destroy()
    
    // è¿è¡Œæ¨ç†
    outputs, err := m.session.Run([]onnxruntime.Value{inputTensor})
    if err != nil {
        return nil, err
    }
    defer outputs[0].Destroy()
    
    // æå–ç»“æœ
    outputTensor := outputs[0].GetTensor()
    return outputTensor.GetData().([]float32), nil
}
```

---

## ğŸŒ æ„å»ºæ¨ç†æœåŠ¡

```go
package main

import (
    "encoding/json"
    "net/http"
    "sync"
)

type ModelServer struct {
    model *ONNXModel
    pool  *sync.Pool
}

func NewModelServer(modelPath string) (*ModelServer, error) {
    model, err := LoadONNXModel(modelPath)
    if err != nil {
        return nil, err
    }
    
    return &ModelServer{
        model: model,
        pool: &sync.Pool{
            New: func() interface{} {
                return make([]float32, 0, 100)
            },
        },
    }, nil
}

func (ms *ModelServer) PredictHandler(w http.ResponseWriter, r *http.Request) {
    var req struct {
        Features []float32 `json:"features"`
    }
    
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, err.Error(), http.StatusBadRequest)
        return
    }
    
    // æ¨ç†
    result, err := ms.model.Predict(req.Features)
    if err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    
    // è¿”å›ç»“æœ
    json.NewEncoder(w).Encode(map[string]interface{}{
        "prediction": result,
    })
}

func main() {
    server, _ := NewModelServer("model.onnx")
    
    http.HandleFunc("/predict", server.PredictHandler)
    http.ListenAndServe(":8080", nil)
}
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ‰¹é‡æ¨ç†
```go
type BatchProcessor struct {
    batchSize int
    timeout   time.Duration
    queue     chan Request
}

func (bp *BatchProcessor) Process() {
    batch := make([]Request, 0, bp.batchSize)
    timer := time.NewTimer(bp.timeout)
    
    for {
        select {
        case req := <-bp.queue:
            batch = append(batch, req)
            if len(batch) >= bp.batchSize {
                bp.processBatch(batch)
                batch = batch[:0]
                timer.Reset(bp.timeout)
            }
        case <-timer.C:
            if len(batch) > 0 {
                bp.processBatch(batch)
                batch = batch[:0]
            }
            timer.Reset(bp.timeout)
        }
    }
}
```

### 2. æ¨¡å‹ç¼“å­˜
```go
type ModelCache struct {
    models map[string]*ONNXModel
    mu     sync.RWMutex
}

func (mc *ModelCache) Get(modelID string) (*ONNXModel, error) {
    mc.mu.RLock()
    model, exists := mc.models[modelID]
    mc.mu.RUnlock()
    
    if exists {
        return model, nil
    }
    
    // åŠ è½½æ¨¡å‹
    mc.mu.Lock()
    defer mc.mu.Unlock()
    
    model, err := LoadONNXModel(modelID)
    if err != nil {
        return nil, err
    }
    
    mc.models[modelID] = model
    return model, nil
}
```

---

## ğŸ“š ç›¸å…³èµ„æº

- [TensorFlow Go](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go)
- [ONNX Runtime Go](https://github.com/yalue/onnxruntime_go)
- [GoLearn](https://github.com/sjwhitworth/golearn)

**ä¸‹ä¸€æ­¥**: [02-æœºå™¨å­¦ä¹ åº“](./02-æœºå™¨å­¦ä¹ åº“.md)

---

**æœ€åæ›´æ–°**: 2025-10-28

