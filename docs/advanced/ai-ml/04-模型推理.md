# 模型推理

**版本**: v1.0
**更新日期**: 2025-10-29
**适用于**: Go 1.25.3

---

## 📋 目录

- [1. 📖 高性能推理服务](#1.-高性能推理服务)[2. 🚀 模型缓存与预热](#2.-模型缓存与预热)[3. 💡 A/B测试](#3.-ab测试)[4. 📊 推理监控](#4.-推理监控)[5. 📚 相关资源](#5.-相关资源)相关资源](#相关资源)

---

## 📖 高性能推理服务

```go
package inference

import (
    "context"
    "sync"
    "time"
)

type InferenceServer struct {
    model      Model
    batchSize  int
    timeout    time.Duration

    requests   chan *Request
    results    map[string]chan *Response
    mu         sync.RWMutex
}

type Request struct {
    ID   string
    Data []float32
}

type Response struct {
    ID     string
    Result []float32
    Error  error
}

func NewInferenceServer(model Model, batchSize int, timeout time.Duration) *InferenceServer {
    return &InferenceServer{
        model:     model,
        batchSize: batchSize,
        timeout:   timeout,
        requests:  make(chan *Request, 1000),
        results:   make(map[string]chan *Response),
    }
}

// 批量处理
func (is *InferenceServer) Start(ctx context.Context) {
    go func() {
        batch := make([]*Request, 0, is.batchSize)
        timer := time.NewTimer(is.timeout)

        for {
            select {
            case req := <-is.requests:
                batch = append(batch, req)

                if len(batch) >= is.batchSize {
                    is.processBatch(batch)
                    batch = batch[:0]
                    timer.Reset(is.timeout)
                }

            case <-timer.C:
                if len(batch) > 0 {
                    is.processBatch(batch)
                    batch = batch[:0]
                }
                timer.Reset(is.timeout)

            case <-ctx.Done():
                return
            }
        }
    }()
}

func (is *InferenceServer) processBatch(batch []*Request) {
    // 合并输入
    inputs := make([][]float32, len(batch))
    for i, req := range batch {
        inputs[i] = req.Data
    }

    // 批量推理
    outputs, err := is.model.PredictBatch(inputs)

    // 分发结果
    for i, req := range batch {
        is.mu.RLock()
        ch := is.results[req.ID]
        is.mu.RUnlock()

        if ch != nil {
            resp := &Response{
                ID:    req.ID,
                Error: err,
            }
            if err == nil && i < len(outputs) {
                resp.Result = outputs[i]
            }
            ch <- resp
        }
    }
}

// Predict提供同步API
func (is *InferenceServer) Predict(data []float32) ([]float32, error) {
    reqID := generateID()
    respCh := make(chan *Response, 1)

    is.mu.Lock()
    is.results[reqID] = respCh
    is.mu.Unlock()

    defer func() {
        is.mu.Lock()
        delete(is.results, reqID)
        is.mu.Unlock()
    }()

    // 发送请求
    is.requests <- &Request{
        ID:   reqID,
        Data: data,
    }

    // 等待结果
    select {
    case resp := <-respCh:
        if resp.Error != nil {
            return nil, resp.Error
        }
        return resp.Result, nil
    case <-time.After(5 * time.Second):
        return nil, errors.New("timeout")
    }
}
```

---

## 🚀 模型缓存与预热

```go
type ModelWarmer struct {
    model     Model
    warmupNum int
}

func (mw *ModelWarmer) Warmup() error {
    // 生成假数据进行预热
    dummyInput := make([]float32, mw.model.InputSize())

    for i := 0; i < mw.warmupNum; i++ {
        _, err := mw.model.Predict(dummyInput)
        if err != nil {
            return err
        }
    }

    log.Printf("Model warmed up with %d iterations\n", mw.warmupNum)
    return nil
}
```

---

## 💡 A/B测试

```go
type ABTester struct {
    modelA    Model
    modelB    Model
    trafficB  float64 // B模型流量比例
}

func (ab *ABTester) Predict(data []float32) ([]float32, error) {
    // 随机选择模型
    if rand.Float64() < ab.trafficB {
        return ab.modelB.Predict(data)
    }
    return ab.modelA.Predict(data)
}
```

---

## 📊 推理监控

```go
type InferenceMetrics struct {
    totalRequests  uint64
    successCount   uint64
    errorCount     uint64
    totalLatency   time.Duration
    mu             sync.Mutex
}

func (im *InferenceMetrics) RecordRequest(latency time.Duration, err error) {
    im.mu.Lock()
    defer im.mu.Unlock()

    im.totalRequests++
    im.totalLatency += latency

    if err != nil {
        im.errorCount++
    } else {
        im.successCount++
    }
}

func (im *InferenceMetrics) Stats() map[string]interface{} {
    im.mu.Lock()
    defer im.mu.Unlock()

    avgLatency := time.Duration(0)
    if im.totalRequests > 0 {
        avgLatency = im.totalLatency / time.Duration(im.totalRequests)
    }

    return map[string]interface{}{
        "total_requests": im.totalRequests,
        "success_count":  im.successCount,
        "error_count":    im.errorCount,
        "avg_latency_ms": avgLatency.Milliseconds(),
        "error_rate":     float64(im.errorCount) / float64(im.totalRequests),
    }
}
```

---

## 📚 相关资源

- [ONNX Runtime](https://onnxruntime.ai/)
- [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)

**下一步**: [05-数据处理](./05-数据处理.md)

---

**最后更新**: 2025-10-29
