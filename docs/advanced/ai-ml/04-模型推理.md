# æ¨¡å‹æ¨ç†

**éš¾åº¦**: ä¸­çº§ | **é¢„è®¡é˜…è¯»**: 15åˆ†é’Ÿ

---

## ğŸ“– é«˜æ€§èƒ½æ¨ç†æœåŠ¡

```go
package inference

import (
    "context"
    "sync"
    "time"
)

type InferenceServer struct {
    model      Model
    batchSize  int
    timeout    time.Duration
    
    requests   chan *Request
    results    map[string]chan *Response
    mu         sync.RWMutex
}

type Request struct {
    ID   string
    Data []float32
}

type Response struct {
    ID     string
    Result []float32
    Error  error
}

func NewInferenceServer(model Model, batchSize int, timeout time.Duration) *InferenceServer {
    return &InferenceServer{
        model:     model,
        batchSize: batchSize,
        timeout:   timeout,
        requests:  make(chan *Request, 1000),
        results:   make(map[string]chan *Response),
    }
}

// æ‰¹é‡å¤„ç†
func (is *InferenceServer) Start(ctx context.Context) {
    go func() {
        batch := make([]*Request, 0, is.batchSize)
        timer := time.NewTimer(is.timeout)
        
        for {
            select {
            case req := <-is.requests:
                batch = append(batch, req)
                
                if len(batch) >= is.batchSize {
                    is.processBatch(batch)
                    batch = batch[:0]
                    timer.Reset(is.timeout)
                }
                
            case <-timer.C:
                if len(batch) > 0 {
                    is.processBatch(batch)
                    batch = batch[:0]
                }
                timer.Reset(is.timeout)
                
            case <-ctx.Done():
                return
            }
        }
    }()
}

func (is *InferenceServer) processBatch(batch []*Request) {
    // åˆå¹¶è¾“å…¥
    inputs := make([][]float32, len(batch))
    for i, req := range batch {
        inputs[i] = req.Data
    }
    
    // æ‰¹é‡æ¨ç†
    outputs, err := is.model.PredictBatch(inputs)
    
    // åˆ†å‘ç»“æœ
    for i, req := range batch {
        is.mu.RLock()
        ch := is.results[req.ID]
        is.mu.RUnlock()
        
        if ch != nil {
            resp := &Response{
                ID:    req.ID,
                Error: err,
            }
            if err == nil && i < len(outputs) {
                resp.Result = outputs[i]
            }
            ch <- resp
        }
    }
}

// Predictæä¾›åŒæ­¥API
func (is *InferenceServer) Predict(data []float32) ([]float32, error) {
    reqID := generateID()
    respCh := make(chan *Response, 1)
    
    is.mu.Lock()
    is.results[reqID] = respCh
    is.mu.Unlock()
    
    defer func() {
        is.mu.Lock()
        delete(is.results, reqID)
        is.mu.Unlock()
    }()
    
    // å‘é€è¯·æ±‚
    is.requests <- &Request{
        ID:   reqID,
        Data: data,
    }
    
    // ç­‰å¾…ç»“æœ
    select {
    case resp := <-respCh:
        if resp.Error != nil {
            return nil, resp.Error
        }
        return resp.Result, nil
    case <-time.After(5 * time.Second):
        return nil, errors.New("timeout")
    }
}
```

---

## ğŸš€ æ¨¡å‹ç¼“å­˜ä¸é¢„çƒ­

```go
type ModelWarmer struct {
    model     Model
    warmupNum int
}

func (mw *ModelWarmer) Warmup() error {
    // ç”Ÿæˆå‡æ•°æ®è¿›è¡Œé¢„çƒ­
    dummyInput := make([]float32, mw.model.InputSize())
    
    for i := 0; i < mw.warmupNum; i++ {
        _, err := mw.model.Predict(dummyInput)
        if err != nil {
            return err
        }
    }
    
    log.Printf("Model warmed up with %d iterations\n", mw.warmupNum)
    return nil
}
```

---

## ğŸ’¡ A/Bæµ‹è¯•

```go
type ABTester struct {
    modelA    Model
    modelB    Model
    trafficB  float64 // Bæ¨¡å‹æµé‡æ¯”ä¾‹
}

func (ab *ABTester) Predict(data []float32) ([]float32, error) {
    // éšæœºé€‰æ‹©æ¨¡å‹
    if rand.Float64() < ab.trafficB {
        return ab.modelB.Predict(data)
    }
    return ab.modelA.Predict(data)
}
```

---

## ğŸ“Š æ¨ç†ç›‘æ§

```go
type InferenceMetrics struct {
    totalRequests  uint64
    successCount   uint64
    errorCount     uint64
    totalLatency   time.Duration
    mu             sync.Mutex
}

func (im *InferenceMetrics) RecordRequest(latency time.Duration, err error) {
    im.mu.Lock()
    defer im.mu.Unlock()
    
    im.totalRequests++
    im.totalLatency += latency
    
    if err != nil {
        im.errorCount++
    } else {
        im.successCount++
    }
}

func (im *InferenceMetrics) Stats() map[string]interface{} {
    im.mu.Lock()
    defer im.mu.Unlock()
    
    avgLatency := time.Duration(0)
    if im.totalRequests > 0 {
        avgLatency = im.totalLatency / time.Duration(im.totalRequests)
    }
    
    return map[string]interface{}{
        "total_requests": im.totalRequests,
        "success_count":  im.successCount,
        "error_count":    im.errorCount,
        "avg_latency_ms": avgLatency.Milliseconds(),
        "error_rate":     float64(im.errorCount) / float64(im.totalRequests),
    }
}
```

---

## ğŸ“š ç›¸å…³èµ„æº

- [ONNX Runtime](https://onnxruntime.ai/)
- [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)

**ä¸‹ä¸€æ­¥**: [05-æ•°æ®å¤„ç†](./05-æ•°æ®å¤„ç†.md)

---

**æœ€åæ›´æ–°**: 2025-10-28

