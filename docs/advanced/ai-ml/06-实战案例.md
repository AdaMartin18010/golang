# å®æˆ˜æ¡ˆä¾‹

**éš¾åº¦**: é«˜çº§ | **é¢„è®¡é˜…è¯»**: 20åˆ†é’Ÿ

---

## ğŸ“– å®Œæ•´æ¨ç†æœåŠ¡

```go
package main

import (
    "encoding/json"
    "log"
    "net/http"
    "time"
    
    "github.com/gorilla/mux"
)

type MLService struct {
    server  *InferenceServer
    metrics *InferenceMetrics
}

func NewMLService(modelPath string) (*MLService, error) {
    // åŠ è½½æ¨¡å‹
    model, err := LoadONNXModel(modelPath)
    if err != nil {
        return nil, err
    }
    
    // æ¨¡å‹é¢„çƒ­
    warmer := &ModelWarmer{model: model, warmupNum: 10}
    warmer.Warmup()
    
    // åˆ›å»ºæ¨ç†æœåŠ¡
    server := NewInferenceServer(model, 32, 100*time.Millisecond)
    server.Start(context.Background())
    
    return &MLService{
        server:  server,
        metrics: &InferenceMetrics{},
    }, nil
}

func (ms *MLService) PredictHandler(w http.ResponseWriter, r *http.Request) {
    start := time.Now()
    
    var req struct {
        Features []float32 `json:"features"`
    }
    
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, err.Error(), http.StatusBadRequest)
        ms.metrics.RecordRequest(time.Since(start), err)
        return
    }
    
    // æ¨ç†
    result, err := ms.server.Predict(req.Features)
    if err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
        ms.metrics.RecordRequest(time.Since(start), err)
        return
    }
    
    ms.metrics.RecordRequest(time.Since(start), nil)
    
    json.NewEncoder(w).Encode(map[string]interface{}{
        "prediction": result,
        "latency_ms": time.Since(start).Milliseconds(),
    })
}

func (ms *MLService) MetricsHandler(w http.ResponseWriter, r *http.Request) {
    json.NewEncoder(w).Encode(ms.metrics.Stats())
}

func (ms *MLService) HealthHandler(w http.ResponseWriter, r *http.Request) {
    w.WriteHeader(http.StatusOK)
    json.NewEncoder(w).Encode(map[string]string{
        "status": "healthy",
    })
}

func main() {
    service, err := NewMLService("model.onnx")
    if err != nil {
        log.Fatal(err)
    }
    
    r := mux.NewRouter()
    r.HandleFunc("/predict", service.PredictHandler).Methods("POST")
    r.HandleFunc("/metrics", service.MetricsHandler).Methods("GET")
    r.HandleFunc("/health", service.HealthHandler).Methods("GET")
    
    log.Println("Starting ML service on :8080")
    log.Fatal(http.ListenAndServe(":8080", r))
}
```

---

## ğŸ”„ æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

```go
type ModelRegistry struct {
    models map[string]*ModelVersion
    mu     sync.RWMutex
}

type ModelVersion struct {
    Version string
    Model   Model
    Metrics *ModelMetrics
}

func (mr *ModelRegistry) RegisterModel(version string, model Model) {
    mr.mu.Lock()
    defer mr.mu.Unlock()
    
    mr.models[version] = &ModelVersion{
        Version: version,
        Model:   model,
        Metrics: &ModelMetrics{},
    }
}

func (mr *ModelRegistry) GetModel(version string) (Model, error) {
    mr.mu.RLock()
    defer mr.mu.RUnlock()
    
    mv, exists := mr.models[version]
    if !exists {
        return nil, fmt.Errorf("model version %s not found", version)
    }
    
    return mv.Model, nil
}
```

---

## ğŸ“Š åœ¨çº¿å­¦ä¹ 

```go
type OnlineLearner struct {
    model      Model
    buffer     []TrainingSample
    bufferSize int
    retrainAt  int
}

type TrainingSample struct {
    Features []float32
    Label    float32
}

func (ol *OnlineLearner) Predict(features []float32) (float32, error) {
    return ol.model.Predict(features)
}

func (ol *OnlineLearner) AddSample(features []float32, label float32) {
    ol.buffer = append(ol.buffer, TrainingSample{
        Features: features,
        Label:    label,
    })
    
    // è¾¾åˆ°é˜ˆå€¼æ—¶é‡æ–°è®­ç»ƒ
    if len(ol.buffer) >= ol.retrainAt {
        go ol.retrain()
    }
}

func (ol *OnlineLearner) retrain() {
    log.Println("Retraining model with new samples...")
    
    // ä½¿ç”¨ç¼“å†²åŒºæ•°æ®é‡æ–°è®­ç»ƒ
    newModel := ol.model.Clone()
    newModel.Train(ol.buffer)
    
    // åŸå­æ›¿æ¢æ¨¡å‹
    ol.model = newModel
    
    // æ¸…ç©ºç¼“å†²åŒº
    ol.buffer = ol.buffer[:0]
    
    log.Println("Model retrained successfully")
}
```

---

## ğŸ¯ å®æ—¶ç‰¹å¾æå–

```go
type FeatureExtractor struct {
    cache *cache.Cache
}

func (fe *FeatureExtractor) Extract(ctx context.Context, userID string) ([]float32, error) {
    // æ£€æŸ¥ç¼“å­˜
    if cached, found := fe.cache.Get(userID); found {
        return cached.([]float32), nil
    }
    
    // æå–ç‰¹å¾
    features := make([]float32, 0)
    
    // ç”¨æˆ·åŸºæœ¬ç‰¹å¾
    userFeatures, _ := fe.getUserFeatures(ctx, userID)
    features = append(features, userFeatures...)
    
    // è¡Œä¸ºç‰¹å¾
    behaviorFeatures, _ := fe.getBehaviorFeatures(ctx, userID)
    features = append(features, behaviorFeatures...)
    
    // ç¼“å­˜
    fe.cache.Set(userID, features, 5*time.Minute)
    
    return features, nil
}
```

---

## ğŸ“š ç›¸å…³èµ„æº

- [Production ML Systems](https://developers.google.com/machine-learning/crash-course/production-ml-systems)
- [MLOps](https://ml-ops.org/)

---

**æœ€åæ›´æ–°**: 2025-10-28

