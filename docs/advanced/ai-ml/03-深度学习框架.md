# 深度学习框架

**版本**: v1.0
**更新日期**: 2025-10-29
**适用于**: Go 1.25.3

---

## 📋 目录

- [1. 📖 Gorgonia深度学习](#1.-gorgonia深度学习)[2. 🔮 卷积神经网络](#2.-卷积神经网络)[3. 🎓 训练技巧](#3.-训练技巧)[4. 📚 相关资源](#4.-相关资源)源](#相关资源)

---

## 📖 Gorgonia深度学习

```go
import (
    G "gorgonia.org/gorgonia"
    "gorgonia.org/tensor"
)

// 全连接神经网络
type FCNet struct {
    g *G.ExprGraph

    w1, w2, w3 *G.Node
    b1, b2, b3 *G.Node

    out *G.Node
}

func NewFCNet(input, hidden1, hidden2, output int) *FCNet {
    g := G.NewGraph()

    // 第一层权重和偏置
    w1 := G.NewMatrix(g, tensor.Float64,
        G.WithShape(input, hidden1),
        G.WithName("w1"),
        G.WithInit(G.GlorotU(1.0)))
    b1 := G.NewMatrix(g, tensor.Float64,
        G.WithShape(1, hidden1),
        G.WithName("b1"),
        G.WithInit(G.Zeroes()))

    // 第二层
    w2 := G.NewMatrix(g, tensor.Float64,
        G.WithShape(hidden1, hidden2),
        G.WithName("w2"),
        G.WithInit(G.GlorotU(1.0)))
    b2 := G.NewMatrix(g, tensor.Float64,
        G.WithShape(1, hidden2),
        G.WithName("b2"),
        G.WithInit(G.Zeroes()))

    // 输出层
    w3 := G.NewMatrix(g, tensor.Float64,
        G.WithShape(hidden2, output),
        G.WithName("w3"),
        G.WithInit(G.GlorotU(1.0)))
    b3 := G.NewMatrix(g, tensor.Float64,
        G.WithShape(1, output),
        G.WithName("b3"),
        G.WithInit(G.Zeroes()))

    return &FCNet{
        g:  g,
        w1: w1, b1: b1,
        w2: w2, b2: b2,
        w3: w3, b3: b3,
    }
}

func (nn *FCNet) Forward(x *G.Node) (*G.Node, error) {
    // Layer 1: x * w1 + b1
    l1 := G.Must(G.Mul(x, nn.w1))
    l1 = G.Must(G.BroadcastAdd(l1, nn.b1, nil, []byte{0}))
    l1 = G.Must(G.Rectify(l1)) // ReLU

    // Layer 2
    l2 := G.Must(G.Mul(l1, nn.w2))
    l2 = G.Must(G.BroadcastAdd(l2, nn.b2, nil, []byte{0}))
    l2 = G.Must(G.Rectify(l2))

    // Output
    out := G.Must(G.Mul(l2, nn.w3))
    out = G.Must(G.BroadcastAdd(out, nn.b3, nil, []byte{0}))

    nn.out = out
    return out, nil
}

// 训练
func (nn *FCNet) Train(X, Y tensor.Tensor, epochs int, lr float64) error {
    x := G.NewMatrix(nn.g, tensor.Float64,
        G.WithShape(X.Shape()...),
        G.WithValue(X))

    y := G.NewMatrix(nn.g, tensor.Float64,
        G.WithShape(Y.Shape()...),
        G.WithValue(Y))

    // 前向传播
    pred, _ := nn.Forward(x)

    // 损失函数 (MSE)
    diff := G.Must(G.Sub(pred, y))
    sqDiff := G.Must(G.Square(diff))
    loss := G.Must(G.Mean(sqDiff))

    // 反向传播
    if _, err := G.Grad(loss, nn.w1, nn.w2, nn.w3, nn.b1, nn.b2, nn.b3); err != nil {
        return err
    }

    // 优化器
    vm := G.NewTapeMachine(nn.g, G.BindDualValues(nn.w1, nn.w2, nn.w3, nn.b1, nn.b2, nn.b3))
    solver := G.NewVanillaSolver(G.WithLearnRate(lr))

    // 训练循环
    for epoch := 0; epoch < epochs; epoch++ {
        vm.RunAll()
        solver.Step(G.NodesToValueGrads([]*G.Node{nn.w1, nn.w2, nn.w3, nn.b1, nn.b2, nn.b3}))
        vm.Reset()

        if epoch%100 == 0 {
            fmt.Printf("Epoch %d, Loss: %v\n", epoch, loss.Value())
        }
    }

    return nil
}
```

---

## 🔮 卷积神经网络

```go
// CNN组件
func Conv2D(input *G.Node, kernel, stride, pad int) *G.Node {
    w := G.NewTensor(g, tensor.Float64, 4,
        G.WithShape(kernel, kernel, inputChannels, outputChannels),
        G.WithName("conv_w"),
        G.WithInit(G.GlorotU(1.0)))

    return G.Must(G.Conv2d(input, w,
        tensor.Shape{kernel, kernel},
        []int{pad, pad},
        []int{stride, stride},
        []int{1, 1}))
}

func MaxPool2D(input *G.Node, size, stride int) *G.Node {
    return G.Must(G.MaxPool2D(input,
        tensor.Shape{size, size},
        []int{0, 0},
        []int{stride, stride}))
}

// CNN网络
type CNN struct {
    g *G.ExprGraph
}

func (cnn *CNN) Build(input *G.Node) *G.Node {
    // Conv1
    conv1 := Conv2D(input, 3, 1, 1)
    conv1 = G.Must(G.Rectify(conv1))
    pool1 := MaxPool2D(conv1, 2, 2)

    // Conv2
    conv2 := Conv2D(pool1, 3, 1, 1)
    conv2 = G.Must(G.Rectify(conv2))
    pool2 := MaxPool2D(conv2, 2, 2)

    // Flatten
    flat := G.Must(G.Reshape(pool2, tensor.Shape{batchSize, -1}))

    // FC layers
    fc1 := fullyConnected(flat, 128)
    fc1 = G.Must(G.Rectify(fc1))

    output := fullyConnected(fc1, 10)

    return output
}
```

---

## 🎓 训练技巧

```go
// Dropout
func Dropout(x *G.Node, prob float64) *G.Node {
    return G.Must(G.Dropout(x, prob))
}

// Batch Normalization
func BatchNorm(x *G.Node) *G.Node {
    return G.Must(G.BatchNorm(x, 1e-5, 0.9))
}

// Learning Rate Schedule
type LRScheduler struct {
    initial float64
    decay   float64
}

func (lr *LRScheduler) Step(epoch int) float64 {
    return lr.initial * math.Pow(lr.decay, float64(epoch))
}
```

---

## 📚 相关资源

- [Gorgonia](https://gorgonia.org/)
- [Gorgonia Examples](https://github.com/gorgonia/gorgonia/tree/master/examples)

**下一步**: [04-模型推理](./04-模型推理.md)

---

**最后更新**: 2025-10-29
