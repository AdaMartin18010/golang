# æ•°æ®æµæ¶æ„ï¼ˆDataflow Architectureï¼‰

> **ç®€ä»‹**: æ•°æ®æµå¤„ç†æ¶æ„è®¾è®¡ï¼Œæ¶µç›–å®æ—¶æµå¤„ç†ã€æ‰¹å¤„ç†å’Œæ··åˆæ•°æ®å¤„ç†

**ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¥æœŸ**: 2025-10-29
**é€‚ç”¨äº**: Go 1.25.3

---

## ğŸ“‹ ç›®å½•

- [æ•°æ®æµæ¶æ„ï¼ˆDataflow Architectureï¼‰](#æ•°æ®æµæ¶æ„dataflow-architecture)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ç›®å½•](#ç›®å½•)
  - [2. å›½é™…æ ‡å‡†ä¸å‘å±•å†ç¨‹](#2-å›½é™…æ ‡å‡†ä¸å‘å±•å†ç¨‹)
    - [ä¸»æµæ•°æ®æµå¤„ç†å¹³å°](#ä¸»æµæ•°æ®æµå¤„ç†å¹³å°)
    - [å‘å±•å†ç¨‹](#å‘å±•å†ç¨‹)
    - [å›½é™…æƒå¨é“¾æ¥](#å›½é™…æƒå¨é“¾æ¥)
  - [3. å…¸å‹åº”ç”¨åœºæ™¯ä¸éœ€æ±‚åˆ†æ](#3-å…¸å‹åº”ç”¨åœºæ™¯ä¸éœ€æ±‚åˆ†æ)
    - [å®æ—¶æ•°æ®åˆ†æ](#å®æ—¶æ•°æ®åˆ†æ)
    - [ç›‘æ§ä¸å‘Šè­¦](#ç›‘æ§ä¸å‘Šè­¦)
    - [å®æ—¶ETL](#å®æ—¶etl)
  - [4. é¢†åŸŸå»ºæ¨¡ä¸UMLç±»å›¾](#4-é¢†åŸŸå»ºæ¨¡ä¸umlç±»å›¾)
    - [æ ¸å¿ƒå®ä½“å»ºæ¨¡](#æ ¸å¿ƒå®ä½“å»ºæ¨¡)
    - [æ•°æ®æµå¤„ç†æ¶æ„](#æ•°æ®æµå¤„ç†æ¶æ„)
  - [5. æ¶æ„æ¨¡å¼ä¸è®¾è®¡åŸåˆ™](#5-æ¶æ„æ¨¡å¼ä¸è®¾è®¡åŸåˆ™)
    - [Lambdaæ¶æ„ vs Kappaæ¶æ„](#lambdaæ¶æ„-vs-kappaæ¶æ„)
      - [Lambdaæ¶æ„ (æ‰¹å¤„ç† + æµå¤„ç†)](#lambdaæ¶æ„-æ‰¹å¤„ç†--æµå¤„ç†)
      - [Kappaæ¶æ„ (çº¯æµå¤„ç†)](#kappaæ¶æ„-çº¯æµå¤„ç†)
    - [æµå¤„ç†æ ¸å¿ƒæ¦‚å¿µ](#æµå¤„ç†æ ¸å¿ƒæ¦‚å¿µ)
      - [æ—¶é—´è¯­ä¹‰ (Time Semantics)](#æ—¶é—´è¯­ä¹‰-time-semantics)
      - [çª—å£æœºåˆ¶ (Windowing)](#çª—å£æœºåˆ¶-windowing)
  - [6. Golangä¸»æµå®ç°ä¸ä»£ç ç¤ºä¾‹](#6-golangä¸»æµå®ç°ä¸ä»£ç ç¤ºä¾‹)
    - [åŸºç¡€æ•°æ®æµå¤„ç†æ¡†æ¶](#åŸºç¡€æ•°æ®æµå¤„ç†æ¡†æ¶)
    - [çª—å£èšåˆå¤„ç†ç¤ºä¾‹](#çª—å£èšåˆå¤„ç†ç¤ºä¾‹)
  - [7. åˆ†å¸ƒå¼æŒ‘æˆ˜ä¸ä¸»æµè§£å†³æ–¹æ¡ˆ](#7-åˆ†å¸ƒå¼æŒ‘æˆ˜ä¸ä¸»æµè§£å†³æ–¹æ¡ˆ)
    - [çŠ¶æ€ä¸€è‡´æ€§ä¸æ£€æŸ¥ç‚¹æœºåˆ¶](#çŠ¶æ€ä¸€è‡´æ€§ä¸æ£€æŸ¥ç‚¹æœºåˆ¶)
    - [èƒŒå‹æ§åˆ¶ä¸æµé‡æ•´å½¢](#èƒŒå‹æ§åˆ¶ä¸æµé‡æ•´å½¢)
    - [äº‹ä»¶æ—¶é—´å¤„ç†ä¸æ°´ä½çº¿æœºåˆ¶](#äº‹ä»¶æ—¶é—´å¤„ç†ä¸æ°´ä½çº¿æœºåˆ¶)
  - [8. å·¥ç¨‹ç»“æ„ä¸CI/CDå®è·µ](#8-å·¥ç¨‹ç»“æ„ä¸cicdå®è·µ)
    - [å…¸å‹é¡¹ç›®ç»“æ„](#å…¸å‹é¡¹ç›®ç»“æ„)
    - [å®¹å™¨åŒ–éƒ¨ç½²](#å®¹å™¨åŒ–éƒ¨ç½²)
  - [9. å½¢å¼åŒ–å»ºæ¨¡ä¸æ•°å­¦è¡¨è¾¾](#9-å½¢å¼åŒ–å»ºæ¨¡ä¸æ•°å­¦è¡¨è¾¾)
    - [æµå¤„ç†æ•°å­¦æ¨¡å‹](#æµå¤„ç†æ•°å­¦æ¨¡å‹)
  - [10. ç›¸å…³æ¶æ„ä¸»é¢˜](#10-ç›¸å…³æ¶æ„ä¸»é¢˜)
  - [11. æ‰©å±•é˜…è¯»ä¸å‚è€ƒæ–‡çŒ®](#11-æ‰©å±•é˜…è¯»ä¸å‚è€ƒæ–‡çŒ®)

## ç›®å½•

- [æ•°æ®æµæ¶æ„ï¼ˆDataflow Architectureï¼‰](#æ•°æ®æµæ¶æ„dataflow-architecture)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ç›®å½•](#ç›®å½•)
  - [2. å›½é™…æ ‡å‡†ä¸å‘å±•å†ç¨‹](#2-å›½é™…æ ‡å‡†ä¸å‘å±•å†ç¨‹)
    - [ä¸»æµæ•°æ®æµå¤„ç†å¹³å°](#ä¸»æµæ•°æ®æµå¤„ç†å¹³å°)
    - [å‘å±•å†ç¨‹](#å‘å±•å†ç¨‹)
    - [å›½é™…æƒå¨é“¾æ¥](#å›½é™…æƒå¨é“¾æ¥)
  - [3. å…¸å‹åº”ç”¨åœºæ™¯ä¸éœ€æ±‚åˆ†æ](#3-å…¸å‹åº”ç”¨åœºæ™¯ä¸éœ€æ±‚åˆ†æ)
    - [å®æ—¶æ•°æ®åˆ†æ](#å®æ—¶æ•°æ®åˆ†æ)
    - [ç›‘æ§ä¸å‘Šè­¦](#ç›‘æ§ä¸å‘Šè­¦)
    - [å®æ—¶ETL](#å®æ—¶etl)
  - [4. é¢†åŸŸå»ºæ¨¡ä¸UMLç±»å›¾](#4-é¢†åŸŸå»ºæ¨¡ä¸umlç±»å›¾)
    - [æ ¸å¿ƒå®ä½“å»ºæ¨¡](#æ ¸å¿ƒå®ä½“å»ºæ¨¡)
    - [æ•°æ®æµå¤„ç†æ¶æ„](#æ•°æ®æµå¤„ç†æ¶æ„)
  - [5. æ¶æ„æ¨¡å¼ä¸è®¾è®¡åŸåˆ™](#5-æ¶æ„æ¨¡å¼ä¸è®¾è®¡åŸåˆ™)
    - [Lambdaæ¶æ„ vs Kappaæ¶æ„](#lambdaæ¶æ„-vs-kappaæ¶æ„)
      - [Lambdaæ¶æ„ (æ‰¹å¤„ç† + æµå¤„ç†)](#lambdaæ¶æ„-æ‰¹å¤„ç†--æµå¤„ç†)
      - [Kappaæ¶æ„ (çº¯æµå¤„ç†)](#kappaæ¶æ„-çº¯æµå¤„ç†)
    - [æµå¤„ç†æ ¸å¿ƒæ¦‚å¿µ](#æµå¤„ç†æ ¸å¿ƒæ¦‚å¿µ)
      - [æ—¶é—´è¯­ä¹‰ (Time Semantics)](#æ—¶é—´è¯­ä¹‰-time-semantics)
      - [çª—å£æœºåˆ¶ (Windowing)](#çª—å£æœºåˆ¶-windowing)
  - [6. Golangä¸»æµå®ç°ä¸ä»£ç ç¤ºä¾‹](#6-golangä¸»æµå®ç°ä¸ä»£ç ç¤ºä¾‹)
    - [åŸºç¡€æ•°æ®æµå¤„ç†æ¡†æ¶](#åŸºç¡€æ•°æ®æµå¤„ç†æ¡†æ¶)
    - [çª—å£èšåˆå¤„ç†ç¤ºä¾‹](#çª—å£èšåˆå¤„ç†ç¤ºä¾‹)
  - [7. åˆ†å¸ƒå¼æŒ‘æˆ˜ä¸ä¸»æµè§£å†³æ–¹æ¡ˆ](#7-åˆ†å¸ƒå¼æŒ‘æˆ˜ä¸ä¸»æµè§£å†³æ–¹æ¡ˆ)
    - [çŠ¶æ€ä¸€è‡´æ€§ä¸æ£€æŸ¥ç‚¹æœºåˆ¶](#çŠ¶æ€ä¸€è‡´æ€§ä¸æ£€æŸ¥ç‚¹æœºåˆ¶)
    - [èƒŒå‹æ§åˆ¶ä¸æµé‡æ•´å½¢](#èƒŒå‹æ§åˆ¶ä¸æµé‡æ•´å½¢)
    - [äº‹ä»¶æ—¶é—´å¤„ç†ä¸æ°´ä½çº¿æœºåˆ¶](#äº‹ä»¶æ—¶é—´å¤„ç†ä¸æ°´ä½çº¿æœºåˆ¶)
  - [8. å·¥ç¨‹ç»“æ„ä¸CI/CDå®è·µ](#8-å·¥ç¨‹ç»“æ„ä¸cicdå®è·µ)
    - [å…¸å‹é¡¹ç›®ç»“æ„](#å…¸å‹é¡¹ç›®ç»“æ„)
    - [å®¹å™¨åŒ–éƒ¨ç½²](#å®¹å™¨åŒ–éƒ¨ç½²)
  - [9. å½¢å¼åŒ–å»ºæ¨¡ä¸æ•°å­¦è¡¨è¾¾](#9-å½¢å¼åŒ–å»ºæ¨¡ä¸æ•°å­¦è¡¨è¾¾)
    - [æµå¤„ç†æ•°å­¦æ¨¡å‹](#æµå¤„ç†æ•°å­¦æ¨¡å‹)
  - [10. ç›¸å…³æ¶æ„ä¸»é¢˜](#10-ç›¸å…³æ¶æ„ä¸»é¢˜)
  - [11. æ‰©å±•é˜…è¯»ä¸å‚è€ƒæ–‡çŒ®](#11-æ‰©å±•é˜…è¯»ä¸å‚è€ƒæ–‡çŒ®)

## 2. å›½é™…æ ‡å‡†ä¸å‘å±•å†ç¨‹

### ä¸»æµæ•°æ®æµå¤„ç†å¹³å°

- **Apache Flink**: æµæ‰¹ä¸€ä½“çš„åˆ†å¸ƒå¼å¤„ç†å¼•æ“
- **Apache Beam**: ç»Ÿä¸€çš„æµæ‰¹å¤„ç†ç¼–ç¨‹æ¨¡å‹
- **Apache Kafka Streams**: è½»é‡çº§æµå¤„ç†åº“
- **Apache Pulsar**: äº‘åŸç”Ÿåˆ†å¸ƒå¼æ¶ˆæ¯æµå¹³å°
- **Google Cloud Dataflow**: æ— æœåŠ¡å™¨æ•°æ®å¤„ç†æœåŠ¡
- **Amazon Kinesis**: å®æ—¶æ•°æ®æµå¤„ç†æœåŠ¡
- **Azure Stream Analytics**: äº‘ç«¯å®æ—¶åˆ†ææœåŠ¡

### å‘å±•å†ç¨‹

- **2000s**: ä¼ ç»Ÿæ‰¹å¤„ç†ï¼ˆHadoop MapReduceï¼‰ä¸ºä¸»
- **2010s**: æµå¤„ç†å¼•æ“å…´èµ·ï¼ˆStorm, Kafka, Flinkï¼‰
- **2015s**: æµæ‰¹ä¸€ä½“åŒ–æ¶æ„æˆç†Ÿ
- **2020s**: äº‘åŸç”Ÿã€Serverlessæµå¤„ç†æ™®åŠ

### å›½é™…æƒå¨é“¾æ¥

- [Apache Flink](https://flink.apache.org/)
- [Apache Beam](https://beam.apache.org/)
- [Apache Kafka](https://kafka.apache.org/)
- [Google Cloud Dataflow](https://cloud.google.com/dataflow)

---

## 3. å…¸å‹åº”ç”¨åœºæ™¯ä¸éœ€æ±‚åˆ†æ

### å®æ—¶æ•°æ®åˆ†æ

- **ç”¨ä¾‹**: ç”¨æˆ·è¡Œä¸ºåˆ†æã€å®æ—¶æ¨èç³»ç»Ÿ
- **éœ€æ±‚**: æ¯«ç§’çº§å»¶è¿Ÿã€é«˜ååé‡ã€å¤æ‚çª—å£è®¡ç®—

### ç›‘æ§ä¸å‘Šè­¦

- **ç”¨ä¾‹**: ç³»ç»Ÿç›‘æ§ã€æ¬ºè¯ˆæ£€æµ‹ã€å¼‚å¸¸æ£€æµ‹
- **éœ€æ±‚**: æµå¼CEPï¼ˆå¤æ‚äº‹ä»¶å¤„ç†ï¼‰ã€å®æ—¶é˜ˆå€¼æ£€æµ‹

### å®æ—¶ETL

- **ç”¨ä¾‹**: æ•°æ®æ¹–å®æ—¶å…¥æ¹–ã€æ•°æ®ä»“åº“å®æ—¶æ›´æ–°
- **éœ€æ±‚**: æ•°æ®æ ¼å¼è½¬æ¢ã€è´¨é‡æ£€æŸ¥ã€å®¹é”™æ¢å¤

---

## 4. é¢†åŸŸå»ºæ¨¡ä¸UMLç±»å›¾

### æ ¸å¿ƒå®ä½“å»ºæ¨¡

```mermaid
    class DataStream {
        +string ID
        +string Topic
        +Schema schema
        +PartitionStrategy partitionStrategy
        +process(Event) Result
    }

    class Event {
        +string ID
        +string Type
        +interface{} Payload
        +time.Time EventTime
        +time.Time ProcessingTime
        +map[string]string Headers
    }

    class StreamProcessor {
        +string ID
        +ProcessorType type
        +StateStore state
        +WindowManager windowManager
        +process(Event) Result
        +checkpoint() error
    }

    class StateStore {
        +string ID
        +map[string]interface{} data
        +get(key string) interface{}
        +put(key string, value interface{})
        +checkpoint() Snapshot
        +restore(Snapshot) error
    }

    class WindowManager {
        +WindowType type
        +time.Duration size
        +time.Duration slide
        +trigger(Event) bool
        +aggregate([]Event) Result
    }

    DataStream "1" *-- "*" Event
    StreamProcessor "1" *-- "1" StateStore
    StreamProcessor "1" *-- "1" WindowManager
    StreamProcessor --> DataStream : consumes
```

### æ•°æ®æµå¤„ç†æ¶æ„

```mermaid
    subgraph "æ•°æ®æº (Sources)"
        DS1[IoTè®¾å¤‡]
        DS2[Webæ—¥å¿—]
        DS3[æ•°æ®åº“CDC]
    end

    subgraph "æ¶ˆæ¯å±‚ (Message Layer)"
        MQ[Kafka/Pulsar<br/>åˆ†åŒºæ¶ˆæ¯é˜Ÿåˆ—]
    end

    subgraph "æµå¤„ç†å±‚ (Stream Processing)"
        SP1[è¿‡æ»¤å™¨<br/>Filter]
        SP2[èšåˆå™¨<br/>Aggregator]
        SP3[çª—å£å¤„ç†<br/>Windowing]
        SP4[è¿æ¥å™¨<br/>Join]
    end

    subgraph "çŠ¶æ€ç®¡ç† (State Management)"
        SS[åˆ†å¸ƒå¼çŠ¶æ€å­˜å‚¨<br/>RocksDB/Redis]
        CP[æ£€æŸ¥ç‚¹æœºåˆ¶<br/>Checkpointing]
    end

    subgraph "æ•°æ®æ¥æ”¶å™¨ (Sinks)"
        SK1[æ•°æ®åº“]
        SK2[ç¼“å­˜]
        SK3[æœç´¢å¼•æ“]
        SK4[æ–‡ä»¶ç³»ç»Ÿ]
    end

    DS1 --> MQ
    DS2 --> MQ
    DS3 --> MQ

    MQ --> SP1
    SP1 --> SP2
    SP2 --> SP3
    SP3 --> SP4

    SP1 -.-> SS
    SP2 -.-> SS
    SP3 -.-> SS
    SP4 -.-> SS

    SS -.-> CP

    SP4 --> SK1
    SP4 --> SK2
    SP4 --> SK3
    SP4 --> SK4

    style MQ fill:#e1f5fe
    style SS fill:#f3e5f5
    style CP fill:#fff3e0
```

---

## 5. æ¶æ„æ¨¡å¼ä¸è®¾è®¡åŸåˆ™

### Lambdaæ¶æ„ vs Kappaæ¶æ„

#### Lambdaæ¶æ„ (æ‰¹å¤„ç† + æµå¤„ç†)

- **ä¼˜åŠ¿**: é«˜ç²¾åº¦æ‰¹å¤„ç† + ä½å»¶è¿Ÿæµå¤„ç†
- **åŠ£åŠ¿**: ä»£ç é‡å¤ã€ç»´æŠ¤å¤æ‚

#### Kappaæ¶æ„ (çº¯æµå¤„ç†)

- **ä¼˜åŠ¿**: ç»Ÿä¸€çš„æµå¤„ç†èŒƒå¼ï¼Œç®€åŒ–æ¶æ„
- **åŠ£åŠ¿**: å¯¹æµå¤„ç†å¼•æ“è¦æ±‚é«˜

```mermaid
    subgraph "Lambdaæ¶æ„"
        L_Data[æ•°æ®æº] --> L_Batch[æ‰¹å¤„ç†å±‚<br/>Hadoop/Spark]
        L_Data --> L_Stream[æµå¤„ç†å±‚<br/>Flink/Storm]
        L_Batch --> L_Serving[æœåŠ¡å±‚<br/>HBase/Cassandra]
        L_Stream --> L_Serving
        L_Serving --> L_Query[æŸ¥è¯¢æ¥å£]
    end

    subgraph "Kappaæ¶æ„"
        K_Data[æ•°æ®æº] --> K_Stream[ç»Ÿä¸€æµå¤„ç†<br/>Kafka+Flink]
        K_Stream --> K_Serving[æœåŠ¡å±‚<br/>å®æ—¶+å†å²æ•°æ®]
        K_Serving --> K_Query[æŸ¥è¯¢æ¥å£]
    end

    style L_Batch fill:#ffcdd2
    style L_Stream fill:#c8e6c9
    style K_Stream fill:#c8e6c9
```

### æµå¤„ç†æ ¸å¿ƒæ¦‚å¿µ

#### æ—¶é—´è¯­ä¹‰ (Time Semantics)

- **Event Time**: äº‹ä»¶å®é™…å‘ç”Ÿçš„æ—¶é—´
- **Processing Time**: ç³»ç»Ÿå¤„ç†äº‹ä»¶çš„æ—¶é—´
- **Ingestion Time**: äº‹ä»¶è¿›å…¥ç³»ç»Ÿçš„æ—¶é—´

#### çª—å£æœºåˆ¶ (Windowing)

- **æ»šåŠ¨çª—å£ (Tumbling Window)**: å›ºå®šå¤§å°ï¼Œä¸é‡å 
- **æ»‘åŠ¨çª—å£ (Sliding Window)**: å›ºå®šå¤§å°ï¼Œæœ‰é‡å 
- **ä¼šè¯çª—å£ (Session Window)**: åŸºäºç”¨æˆ·æ´»åŠ¨çš„åŠ¨æ€çª—å£

---

## 6. Golangä¸»æµå®ç°ä¸ä»£ç ç¤ºä¾‹

### åŸºç¡€æ•°æ®æµå¤„ç†æ¡†æ¶

```go
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"
    "sync"
    "time"
)

// Event è¡¨ç¤ºæµä¸­çš„ä¸€ä¸ªäº‹ä»¶
type Event struct {
    ID           string                 `json:"id"`
    Type         string                 `json:"type"`
    Payload      map[string]interface{} `json:"payload"`
    EventTime    time.Time              `json:"event_time"`
    ProcessingTime time.Time            `json:"processing_time"`
}

// StreamProcessor æµå¤„ç†å™¨æ¥å£
type StreamProcessor interface {
    Process(ctx context.Context, event *Event) (*Event, error)
    Name() string
}

// FilterProcessor è¿‡æ»¤å¤„ç†å™¨
type FilterProcessor struct {
    name      string
    predicate func(*Event) bool
}

func NewFilterProcessor(name string, predicate func(*Event) bool) *FilterProcessor {
    return &FilterProcessor{
        name:      name,
        predicate: predicate,
    }
}

func (fp *FilterProcessor) Process(ctx context.Context, event *Event) (*Event, error) {
    if fp.predicate(event) {
        return event, nil
    }
    return nil, nil // è¿‡æ»¤æ‰äº‹ä»¶
}

func (fp *FilterProcessor) Name() string {
    return fp.name
}

// MapProcessor è½¬æ¢å¤„ç†å™¨
type MapProcessor struct {
    name      string
    transform func(*Event) *Event
}

func NewMapProcessor(name string, transform func(*Event) *Event) *MapProcessor {
    return &MapProcessor{
        name:      name,
        transform: transform,
    }
}

func (mp *MapProcessor) Process(ctx context.Context, event *Event) (*Event, error) {
    return mp.transform(event), nil
}

func (mp *MapProcessor) Name() string {
    return mp.name
}

// DataflowPipeline æ•°æ®æµå¤„ç†ç®¡é“
type DataflowPipeline struct {
    name       string
    processors []StreamProcessor
    inputChan  chan *Event
    outputChan chan *Event
    errorChan  chan error
    stopChan   chan struct{}
    wg         sync.WaitGroup
}

func NewDataflowPipeline(name string) *DataflowPipeline {
    return &DataflowPipeline{
        name:       name,
        processors: make([]StreamProcessor, 0),
        inputChan:  make(chan *Event, 1000),
        outputChan: make(chan *Event, 1000),
        errorChan:  make(chan error, 100),
        stopChan:   make(chan struct{}),
    }
}

func (dp *DataflowPipeline) AddProcessor(processor StreamProcessor) {
    dp.processors = append(dp.processors, processor)
}

func (dp *DataflowPipeline) Start(ctx context.Context) {
    dp.wg.Add(1)
    go dp.processEvents(ctx)
}

func (dp *DataflowPipeline) processEvents(ctx context.Context) {
    defer dp.wg.Done()

    for {
        select {
        case event := <-dp.inputChan:
            processedEvent := event
            var err error

            // ä¾æ¬¡é€šè¿‡æ‰€æœ‰å¤„ç†å™¨
            for _, processor := range dp.processors {
                if processedEvent == nil {
                    break // äº‹ä»¶è¢«è¿‡æ»¤æ‰
                }

                processedEvent, err = processor.Process(ctx, processedEvent)
                if err != nil {
                    dp.errorChan <- fmt.Errorf("processor %s failed: %w", processor.Name(), err)
                    break
                }
            }

            // å¦‚æœäº‹ä»¶æœªè¢«è¿‡æ»¤ä¸”å¤„ç†æˆåŠŸï¼Œå‘é€åˆ°è¾“å‡ºé€šé“
            if processedEvent != nil && err == nil {
                processedEvent.ProcessingTime = time.Now()
                dp.outputChan <- processedEvent
            }

        case <-dp.stopChan:
            return
        case <-ctx.Done():
            return
        }
    }
}

func (dp *DataflowPipeline) SendEvent(event *Event) {
    select {
    case dp.inputChan <- event:
    default:
        log.Printf("Pipeline %s input buffer full, dropping event %s", dp.name, event.ID)
    }
}

func (dp *DataflowPipeline) GetOutputChan() <-chan *Event {
    return dp.outputChan
}

func (dp *DataflowPipeline) GetErrorChan() <-chan error {
    return dp.errorChan
}

func (dp *DataflowPipeline) Stop() {
    close(dp.stopChan)
    dp.wg.Wait()
    close(dp.outputChan)
    close(dp.errorChan)
}
```

### çª—å£èšåˆå¤„ç†ç¤ºä¾‹

```go
// WindowManager çª—å£ç®¡ç†å™¨
type WindowManager struct {
    windowSize time.Duration
    slideSize  time.Duration
    windows    map[string]*Window
    mu         sync.RWMutex
}

type Window struct {
    StartTime time.Time
    EndTime   time.Time
    Events    []*Event
    mu        sync.Mutex
}

func NewWindowManager(windowSize, slideSize time.Duration) *WindowManager {
    return &WindowManager{
        windowSize: windowSize,
        slideSize:  slideSize,
        windows:    make(map[string]*Window),
    }
}

func (wm *WindowManager) AddEvent(event *Event) []*Window {
    wm.mu.Lock()
    defer wm.mu.Unlock()

    var triggeredWindows []*Window
    windowKey := wm.getWindowKey(event.EventTime)

    // è·å–æˆ–åˆ›å»ºçª—å£
    window, exists := wm.windows[windowKey]
    if !exists {
        startTime := wm.getWindowStart(event.EventTime)
        window = &Window{
            StartTime: startTime,
            EndTime:   startTime.Add(wm.windowSize),
            Events:    make([]*Event, 0),
        }
        wm.windows[windowKey] = window
    }

    // æ·»åŠ äº‹ä»¶åˆ°çª—å£
    window.mu.Lock()
    window.Events = append(window.Events, event)
    window.mu.Unlock()

    // æ£€æŸ¥æ˜¯å¦æœ‰çª—å£å¯ä»¥è§¦å‘
    now := time.Now()
    for key, win := range wm.windows {
        if now.After(win.EndTime) {
            triggeredWindows = append(triggeredWindows, win)
            delete(wm.windows, key)
        }
    }

    return triggeredWindows
}

func (wm *WindowManager) getWindowKey(eventTime time.Time) string {
    windowStart := wm.getWindowStart(eventTime)
    return fmt.Sprintf("%d", windowStart.Unix())
}

func (wm *WindowManager) getWindowStart(eventTime time.Time) time.Time {
    return eventTime.Truncate(wm.slideSize)
}

// AggregateProcessor èšåˆå¤„ç†å™¨
type AggregateProcessor struct {
    name          string
    windowManager *WindowManager
    aggregateFunc func([]*Event) map[string]interface{}
}

func NewAggregateProcessor(name string, windowSize, slideSize time.Duration,
    aggregateFunc func([]*Event) map[string]interface{}) *AggregateProcessor {
    return &AggregateProcessor{
        name:          name,
        windowManager: NewWindowManager(windowSize, slideSize),
        aggregateFunc: aggregateFunc,
    }
}

func (ap *AggregateProcessor) Process(ctx context.Context, event *Event) (*Event, error) {
    triggeredWindows := ap.windowManager.AddEvent(event)

    // å¯¹è§¦å‘çš„çª—å£è¿›è¡Œèšåˆå¤„ç†
    for _, window := range triggeredWindows {
        aggregatedData := ap.aggregateFunc(window.Events)

        // åˆ›å»ºèšåˆç»“æœäº‹ä»¶
        aggregatedEvent := &Event{
            ID:           fmt.Sprintf("agg_%d_%d", window.StartTime.Unix(), window.EndTime.Unix()),
            Type:         "aggregated",
            Payload:      aggregatedData,
            EventTime:    window.EndTime,
            ProcessingTime: time.Now(),
        }

        // è¿™é‡Œå¯ä»¥å‘é€èšåˆç»“æœåˆ°ä¸‹æ¸¸
        log.Printf("Window [%s, %s] aggregated: %+v",
            window.StartTime.Format(time.RFC3339),
            window.EndTime.Format(time.RFC3339),
            aggregatedData)
    }

    return event, nil // è¿”å›åŸå§‹äº‹ä»¶ç»§ç»­å¤„ç†é“¾
}

func (ap *AggregateProcessor) Name() string {
    return ap.name
}

// ä½¿ç”¨ç¤ºä¾‹
func main() {
    ctx := context.Background()

    // åˆ›å»ºæ•°æ®æµå¤„ç†ç®¡é“
    pipeline := NewDataflowPipeline("user_analytics")

    // æ·»åŠ è¿‡æ»¤å™¨ï¼šåªå¤„ç†ç”¨æˆ·ç‚¹å‡»äº‹ä»¶
    clickFilter := NewFilterProcessor("click_filter", func(event *Event) bool {
        return event.Type == "user_click"
    })
    pipeline.AddProcessor(clickFilter)

    // æ·»åŠ è½¬æ¢å™¨ï¼šæå–ç”¨æˆ·ID
    userExtractor := NewMapProcessor("user_extractor", func(event *Event) *Event {
        if userID, ok := event.Payload["user_id"].(string); ok {
            event.Payload["extracted_user_id"] = userID
        }
        return event
    })
    pipeline.AddProcessor(userExtractor)

    // æ·»åŠ çª—å£èšåˆå™¨ï¼š5åˆ†é’Ÿçª—å£ï¼Œ1åˆ†é’Ÿæ»‘åŠ¨
    aggregator := NewAggregateProcessor("click_aggregator",
        5*time.Minute, 1*time.Minute,
        func(events []*Event) map[string]interface{} {
            userClicks := make(map[string]int)
            for _, event := range events {
                if userID, ok := event.Payload["extracted_user_id"].(string); ok {
                    userClicks[userID]++
                }
            }
            return map[string]interface{}{
                "total_clicks": len(events),
                "unique_users": len(userClicks),
                "user_clicks":  userClicks,
            }
        })
    pipeline.AddProcessor(aggregator)

    // å¯åŠ¨ç®¡é“
    pipeline.Start(ctx)

    // æ¨¡æ‹Ÿå‘é€äº‹ä»¶
    go func() {
        for i := 0; i < 100; i++ {
            event := &Event{
                ID:        fmt.Sprintf("event_%d", i),
                Type:      "user_click",
                Payload:   map[string]interface{}{"user_id": fmt.Sprintf("user_%d", i%10)},
                EventTime: time.Now(),
            }
            pipeline.SendEvent(event)
            time.Sleep(100 * time.Millisecond)
        }
    }()

    // å¤„ç†è¾“å‡ºå’Œé”™è¯¯
    go func() {
        for {
            select {
            case output := <-pipeline.GetOutputChan():
                log.Printf("Pipeline output: %+v", output)
            case err := <-pipeline.GetErrorChan():
                log.Printf("Pipeline error: %v", err)
            }
        }
    }()

    // è¿è¡Œ10ç§’ååœæ­¢
    time.Sleep(10 * time.Second)
    pipeline.Stop()
}
```

---

## 7. åˆ†å¸ƒå¼æŒ‘æˆ˜ä¸ä¸»æµè§£å†³æ–¹æ¡ˆ

### çŠ¶æ€ä¸€è‡´æ€§ä¸æ£€æŸ¥ç‚¹æœºåˆ¶

åœ¨åˆ†å¸ƒå¼æµå¤„ç†ä¸­ï¼Œç»´æŠ¤çŠ¶æ€ä¸€è‡´æ€§æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚éœ€è¦å®ç°åˆ†å¸ƒå¼å¿«ç…§å’Œå®¹é”™æ¢å¤æœºåˆ¶ã€‚

```go
// StateSnapshot çŠ¶æ€å¿«ç…§
type StateSnapshot struct {
    ID        string
    Timestamp time.Time
    Data      map[string]interface{}
    Checksum  string
}

// CheckpointManager æ£€æŸ¥ç‚¹ç®¡ç†å™¨
type CheckpointManager struct {
    interval     time.Duration
    storage      CheckpointStorage
    lastSnapshot *StateSnapshot
    mu           sync.RWMutex
}

type CheckpointStorage interface {
    Save(snapshot *StateSnapshot) error
    Load(id string) (*StateSnapshot, error)
    List() ([]*StateSnapshot, error)
}

func NewCheckpointManager(interval time.Duration, storage CheckpointStorage) *CheckpointManager {
    return &CheckpointManager{
        interval: interval,
        storage:  storage,
    }
}

func (cm *CheckpointManager) StartCheckpointing(ctx context.Context, stateProvider func() map[string]interface{}) {
    ticker := time.NewTicker(cm.interval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            if err := cm.createCheckpoint(stateProvider()); err != nil {
                log.Printf("Checkpoint creation failed: %v", err)
            }
        case <-ctx.Done():
            return
        }
    }
}

func (cm *CheckpointManager) createCheckpoint(state map[string]interface{}) error {
    cm.mu.Lock()
    defer cm.mu.Unlock()

    snapshot := &StateSnapshot{
        ID:        fmt.Sprintf("checkpoint_%d", time.Now().Unix()),
        Timestamp: time.Now(),
        Data:      state,
        Checksum:  cm.calculateChecksum(state),
    }

    if err := cm.storage.Save(snapshot); err != nil {
        return err
    }

    cm.lastSnapshot = snapshot
    log.Printf("Checkpoint created: %s", snapshot.ID)
    return nil
}

func (cm *CheckpointManager) calculateChecksum(state map[string]interface{}) string {
    data, _ := json.Marshal(state)
    return fmt.Sprintf("%x", sha256.Sum256(data))
}
```

### èƒŒå‹æ§åˆ¶ä¸æµé‡æ•´å½¢

å½“ä¸‹æ¸¸å¤„ç†èƒ½åŠ›ä¸è¶³æ—¶ï¼Œéœ€è¦èƒŒå‹æœºåˆ¶ä¿æŠ¤ç³»ç»Ÿç¨³å®šæ€§ã€‚

```go
// BackpressureController èƒŒå‹æ§åˆ¶å™¨
type BackpressureController struct {
    maxQueueSize    int
    currentLoad     int64
    rateLimiter     *rate.Limiter
    adaptiveControl bool
    metrics         *BackpressureMetrics
    mu              sync.RWMutex
}

type BackpressureMetrics struct {
    DroppedEvents   int64
    DelayedEvents   int64
    ThroughputLimit int64
}

func NewBackpressureController(maxQueueSize int, rateLimit rate.Limit) *BackpressureController {
    return &BackpressureController{
        maxQueueSize:    maxQueueSize,
        rateLimiter:     rate.NewLimiter(rateLimit, int(rateLimit)),
        adaptiveControl: true,
        metrics:         &BackpressureMetrics{},
    }
}

func (bpc *BackpressureController) CanProcess() bool {
    // æ£€æŸ¥é˜Ÿåˆ—å®¹é‡
    if atomic.LoadInt64(&bpc.currentLoad) >= int64(bpc.maxQueueSize) {
        atomic.AddInt64(&bpc.metrics.DroppedEvents, 1)
        return false
    }

    // æ£€æŸ¥é€Ÿç‡é™åˆ¶
    if !bpc.rateLimiter.Allow() {
        atomic.AddInt64(&bpc.metrics.DelayedEvents, 1)
        return false
    }

    return true
}

func (bpc *BackpressureController) StartProcessing() {
    atomic.AddInt64(&bpc.currentLoad, 1)
}

func (bpc *BackpressureController) FinishProcessing() {
    atomic.AddInt64(&bpc.currentLoad, -1)
}

func (bpc *BackpressureController) AdaptiveAdjust(processingLatency time.Duration) {
    if !bpc.adaptiveControl {
        return
    }

    bpc.mu.Lock()
    defer bpc.mu.Unlock()

    // åŸºäºå¤„ç†å»¶è¿ŸåŠ¨æ€è°ƒæ•´é€Ÿç‡é™åˆ¶
    if processingLatency > 100*time.Millisecond {
        // é™ä½é€Ÿç‡é™åˆ¶
        newLimit := bpc.rateLimiter.Limit() * 0.9
        if newLimit < 1 {
            newLimit = 1
        }
        bpc.rateLimiter.SetLimit(newLimit)
    } else if processingLatency < 10*time.Millisecond {
        // å¢åŠ é€Ÿç‡é™åˆ¶
        newLimit := bpc.rateLimiter.Limit() * 1.1
        bpc.rateLimiter.SetLimit(newLimit)
    }
}
```

### äº‹ä»¶æ—¶é—´å¤„ç†ä¸æ°´ä½çº¿æœºåˆ¶

å¤„ç†ä¹±åºäº‹ä»¶å’Œå»¶è¿Ÿæ•°æ®æ˜¯æµå¤„ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

```go
// Watermark æ°´ä½çº¿ï¼Œç”¨äºå¤„ç†äº‹ä»¶æ—¶é—´
type Watermark struct {
    Timestamp time.Time
    Source    string
}

// WatermarkManager æ°´ä½çº¿ç®¡ç†å™¨
type WatermarkManager struct {
    sources           map[string]time.Time
    globalWatermark   time.Time
    maxOutOfOrder     time.Duration
    watermarkInterval time.Duration
    listeners         []func(Watermark)
    mu                sync.RWMutex
}

func NewWatermarkManager(maxOutOfOrder, watermarkInterval time.Duration) *WatermarkManager {
    return &WatermarkManager{
        sources:           make(map[string]time.Time),
        maxOutOfOrder:     maxOutOfOrder,
        watermarkInterval: watermarkInterval,
        listeners:         make([]func(Watermark), 0),
    }
}

func (wm *WatermarkManager) UpdateSourceWatermark(source string, timestamp time.Time) {
    wm.mu.Lock()
    defer wm.mu.Unlock()

    wm.sources[source] = timestamp

    // è®¡ç®—å…¨å±€æ°´ä½çº¿ï¼ˆæ‰€æœ‰æºçš„æœ€å°æ—¶é—´æˆ³ï¼‰
    minTimestamp := time.Now()
    for _, ts := range wm.sources {
        if ts.Before(minTimestamp) {
            minTimestamp = ts
        }
    }

    // å‡å»æœ€å¤§ä¹±åºæ—¶é—´ä½œä¸ºå®‰å…¨è¾¹ç•Œ
    newWatermark := minTimestamp.Add(-wm.maxOutOfOrder)

    if newWatermark.After(wm.globalWatermark) {
        wm.globalWatermark = newWatermark

        // é€šçŸ¥æ‰€æœ‰ç›‘å¬å™¨
        watermark := Watermark{
            Timestamp: newWatermark,
            Source:    "global",
        }

        for _, listener := range wm.listeners {
            go listener(watermark)
        }
    }
}

func (wm *WatermarkManager) AddWatermarkListener(listener func(Watermark)) {
    wm.mu.Lock()
    defer wm.mu.Unlock()
    wm.listeners = append(wm.listeners, listener)
}

func (wm *WatermarkManager) GetGlobalWatermark() time.Time {
    wm.mu.RLock()
    defer wm.mu.RUnlock()
    return wm.globalWatermark
}

func (wm *WatermarkManager) IsEventLate(eventTime time.Time) bool {
    return eventTime.Before(wm.GetGlobalWatermark())
}
```

---

## 8. å·¥ç¨‹ç»“æ„ä¸CI/CDå®è·µ

### å…¸å‹é¡¹ç›®ç»“æ„

```text
dataflow-service/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ processor/          # æµå¤„ç†å™¨å…¥å£
â”‚   â””â”€â”€ admin/             # ç®¡ç†å·¥å…·
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ pipeline/          # æ•°æ®æµç®¡é“
â”‚   â”œâ”€â”€ processor/         # å¤„ç†å™¨å®ç°
â”‚   â”œâ”€â”€ state/            # çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ checkpoint/       # æ£€æŸ¥ç‚¹æœºåˆ¶
â”‚   â””â”€â”€ metrics/          # ç›‘æ§æŒ‡æ ‡
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ event/            # äº‹ä»¶å®šä¹‰
â”‚   â””â”€â”€ window/           # çª—å£ç®¡ç†
â”œâ”€â”€ deployments/
â”‚   â”œâ”€â”€ kubernetes/       # K8sé…ç½®
â”‚   â””â”€â”€ docker/          # å®¹å™¨é…ç½®
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ pipeline.yaml    # ç®¡é“é…ç½®
â””â”€â”€ scripts/
    â””â”€â”€ start.sh         # å¯åŠ¨è„šæœ¬
```

### å®¹å™¨åŒ–éƒ¨ç½²

```dockerfile

# Dockerfile

FROM golang:1.19-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -o dataflow-processor ./cmd/processor

FROM alpine:latest
RUN apk --no-cache add ca-certificates
WORKDIR /root/

COPY --from=builder /app/dataflow-processor .
COPY --from=builder /app/configs ./configs

CMD ["./dataflow-processor"]
```

---

## 9. å½¢å¼åŒ–å»ºæ¨¡ä¸æ•°å­¦è¡¨è¾¾

### æµå¤„ç†æ•°å­¦æ¨¡å‹

**æ•°æ®æµå®šä¹‰**ï¼š
\[ S = \{(e_i, t_i) | i \in \mathbb{N}, t_i \leq t_{i+1}\} \]

å…¶ä¸­ $e_i$ æ˜¯äº‹ä»¶ï¼Œ$t_i$ æ˜¯äº‹ä»¶æ—¶é—´æˆ³ã€‚

**çª—å£å‡½æ•°**ï¼š
\[ W(S, w, s) = \{(e_i, t_i) \in S | t_{start} \leq t_i < t_{end}\} \]

å…¶ä¸­ $w$ æ˜¯çª—å£å¤§å°ï¼Œ$s$ æ˜¯æ»‘åŠ¨é—´éš”ã€‚

**èšåˆå‡½æ•°**ï¼š
\[ A(W) = f(\{e_i | (e_i, t_i) \in W\}) \]

---

## 10. ç›¸å…³æ¶æ„ä¸»é¢˜

- [**æ¶ˆæ¯é˜Ÿåˆ—æ¶æ„ (Message Queue Architecture)**](./architecture_message_queue_golang.md): æ•°æ®æµæ¶æ„çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼Œæä¾›é«˜ååé‡çš„äº‹ä»¶ä¼ è¾“ã€‚
- [**äº‹ä»¶é©±åŠ¨æ¶æ„ (Event-Driven Architecture)**](./architecture_event_driven_golang.md): æ•°æ®æµå¤„ç†æ˜¯äº‹ä»¶é©±åŠ¨æ¶æ„çš„é‡è¦å®ç°æ¨¡å¼ã€‚
- [**å¾®æœåŠ¡æ¶æ„ (Microservice Architecture)**](./architecture_microservice_golang.md): æ•°æ®æµå¤„ç†å¸¸ç”¨äºå¾®æœåŠ¡é—´çš„å®æ—¶æ•°æ®åŒæ­¥å’Œåˆ†æã€‚
- [**DevOpsä¸è¿ç»´æ¶æ„ (DevOps & Operations Architecture)**](./architecture_devops_golang.md): æ•°æ®æµç³»ç»Ÿçš„ç›‘æ§ã€å‘Šè­¦å’Œè‡ªåŠ¨è¿ç»´æ˜¯å…³é”®çš„è¿ç»´æŒ‘æˆ˜ã€‚

## 11. æ‰©å±•é˜…è¯»ä¸å‚è€ƒæ–‡çŒ®

1. "Streaming Systems" - Tyler Akidau, Slava Chernyak, Reuven Lax
2. "Designing Data-Intensive Applications" - Martin Kleppmann
3. "Apache Flink Documentation" - [https://flink.apache.org/](https://flink.apache.org/)
4. "Apache Beam Programming Guide" - [https://beam.apache.org/](https://beam.apache.org/)
5. "Kafka Streams in Action" - William Bejeck

---

- æœ¬æ–‡æ¡£ä¸¥æ ¼å¯¹æ ‡å›½é™…ä¸»æµæ ‡å‡†ï¼Œé‡‡ç”¨å¤šè¡¨å¾è¾“å‡ºï¼Œä¾¿äºåç»­æ–­ç‚¹ç»­å†™å’Œæ‰¹é‡å¤„ç†ã€‚*

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Go Documentation Team
**æœ€åæ›´æ–°**: 2025-10-29
**æ–‡æ¡£çŠ¶æ€**: å®Œæˆ
**é€‚ç”¨ç‰ˆæœ¬**: Go 1.25.3+
