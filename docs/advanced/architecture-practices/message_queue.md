# æ¶ˆæ¯é˜Ÿåˆ—æ¶æ„ï¼ˆGolangå›½é™…ä¸»æµå®è·µï¼‰

> **ç®€ä»‹**: å¼‚æ­¥æ¶ˆæ¯ä¼ é€’å’Œæµå¤„ç†æ¶æ„ï¼Œå®ç°ç³»ç»Ÿè§£è€¦å’Œå¯é é€šä¿¡


## ğŸ“‹ ç›®å½•


- [ç›®å½•](#ç›®å½•)
- [2. å›½é™…æ ‡å‡†ä¸å‘å±•å†ç¨‹](#2-å›½é™…æ ‡å‡†ä¸å‘å±•å†ç¨‹)
  - [ä¸»æµæ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ](#ä¸»æµæ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ)
  - [å‘å±•å†ç¨‹](#å‘å±•å†ç¨‹)
  - [å›½é™…æƒå¨é“¾æ¥](#å›½é™…æƒå¨é“¾æ¥)
- [3. æ ¸å¿ƒæ¶æ„æ¨¡å¼ä¸è®¾è®¡åŸåˆ™](#3-æ ¸å¿ƒæ¶æ„æ¨¡å¼ä¸è®¾è®¡åŸåˆ™)
  - [æ¶ˆæ¯é˜Ÿåˆ—åŸºç¡€æ¶æ„æ¨¡å¼](#æ¶ˆæ¯é˜Ÿåˆ—åŸºç¡€æ¶æ„æ¨¡å¼)
  - [æ¶ˆæ¯äº¤ä»˜ä¿è¯ (Message Delivery Guarantees)](#æ¶ˆæ¯äº¤ä»˜ä¿è¯-message-delivery-guarantees)
    - [At-most-once (æœ€å¤šä¸€æ¬¡)](#at-most-once-æœ€å¤šä¸€æ¬¡)
    - [At-least-once (è‡³å°‘ä¸€æ¬¡)](#at-least-once-è‡³å°‘ä¸€æ¬¡)
    - [Exactly-once (ç²¾ç¡®ä¸€æ¬¡)](#exactly-once-ç²¾ç¡®ä¸€æ¬¡)
  - [åˆ†åŒºä¸å¹¶è¡Œå¤„ç†](#åˆ†åŒºä¸å¹¶è¡Œå¤„ç†)
- [4. å¯é æ€§ä¿è¯](#4-å¯é æ€§ä¿è¯)
  - [æ¶ˆæ¯æŒä¹…åŒ–](#æ¶ˆæ¯æŒä¹…åŒ–)
  - [æ¶ˆæ¯ç¡®è®¤æœºåˆ¶](#æ¶ˆæ¯ç¡®è®¤æœºåˆ¶)
- [5. æ€§èƒ½ä¼˜åŒ–](#5-æ€§èƒ½ä¼˜åŒ–)
  - [æ‰¹é‡å¤„ç†](#æ‰¹é‡å¤„ç†)
  - [æ¶ˆæ¯å‹ç¼©](#æ¶ˆæ¯å‹ç¼©)
- [6. ç›‘æ§ä¸å¯è§‚æµ‹æ€§](#6-ç›‘æ§ä¸å¯è§‚æµ‹æ€§)
  - [æ¶ˆæ¯é˜Ÿåˆ—ç›‘æ§](#æ¶ˆæ¯é˜Ÿåˆ—ç›‘æ§)
- [7. åˆ†å¸ƒå¼æŒ‘æˆ˜ä¸ä¸»æµè§£å†³æ–¹æ¡ˆ](#7-åˆ†å¸ƒå¼æŒ‘æˆ˜ä¸ä¸»æµè§£å†³æ–¹æ¡ˆ)
  - [æ¶ˆæ¯é‡è¯•ä¸é€€é¿ç­–ç•¥](#æ¶ˆæ¯é‡è¯•ä¸é€€é¿ç­–ç•¥)
  - [æ¶ˆæ¯å»é‡ä¸é‡å¤æ£€æµ‹](#æ¶ˆæ¯å»é‡ä¸é‡å¤æ£€æµ‹)
  - [èƒŒå‹æ§åˆ¶ (Backpressure Control)](#èƒŒå‹æ§åˆ¶-backpressure-control)
  - [æ¶ˆæ¯åºåˆ—åŒ–ä¸å‹ç¼©](#æ¶ˆæ¯åºåˆ—åŒ–ä¸å‹ç¼©)
- [8. å®é™…æ¡ˆä¾‹åˆ†æ](#8-å®é™…æ¡ˆä¾‹åˆ†æ)
  - [é«˜å¹¶å‘ç”µå•†æ¶ˆæ¯ç³»ç»Ÿ](#é«˜å¹¶å‘ç”µå•†æ¶ˆæ¯ç³»ç»Ÿ)
  - [å®æ—¶æ—¥å¿—å¤„ç†ç³»ç»Ÿ](#å®æ—¶æ—¥å¿—å¤„ç†ç³»ç»Ÿ)
- [9. æœªæ¥è¶‹åŠ¿ä¸å›½é™…å‰æ²¿](#9-æœªæ¥è¶‹åŠ¿ä¸å›½é™…å‰æ²¿)
- [10. å›½é™…æƒå¨èµ„æºä¸å¼€æºç»„ä»¶å¼•ç”¨](#10-å›½é™…æƒå¨èµ„æºä¸å¼€æºç»„ä»¶å¼•ç”¨)
  - [æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ](#æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ)
  - [äº‘åŸç”Ÿæ¶ˆæ¯æœåŠ¡](#äº‘åŸç”Ÿæ¶ˆæ¯æœåŠ¡)
  - [æ¶ˆæ¯å¤„ç†æ¡†æ¶](#æ¶ˆæ¯å¤„ç†æ¡†æ¶)
- [11. ç›¸å…³æ¶æ„ä¸»é¢˜](#11-ç›¸å…³æ¶æ„ä¸»é¢˜)
- [12. æ‰©å±•é˜…è¯»ä¸å‚è€ƒæ–‡çŒ®](#12-æ‰©å±•é˜…è¯»ä¸å‚è€ƒæ–‡çŒ®)

## ç›®å½•

---

## 2. å›½é™…æ ‡å‡†ä¸å‘å±•å†ç¨‹

### ä¸»æµæ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ

- **Apache Kafka**: åˆ†å¸ƒå¼æµå¤„ç†å¹³å°
- **RabbitMQ**: ä¼ä¸šçº§æ¶ˆæ¯ä»£ç†
- **Apache Pulsar**: äº‘åŸç”Ÿæ¶ˆæ¯æµå¹³å°
- **Redis Streams**: å†…å­˜æ¶ˆæ¯æµ
- **Amazon SQS/SNS**: äº‘åŸç”Ÿæ¶ˆæ¯æœåŠ¡
- **Google Cloud Pub/Sub**: å®æ—¶æ¶ˆæ¯æœåŠ¡

### å‘å±•å†ç¨‹

- **1980s**: æ—©æœŸæ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿï¼ˆIBM MQï¼‰
- **2000s**: JMSæ ‡å‡†ã€ActiveMQå…´èµ·
- **2010s**: RabbitMQã€Kafkaæ™®åŠ
- **2015s**: äº‘åŸç”Ÿæ¶ˆæ¯æœåŠ¡
- **2020s**: å®æ—¶æµå¤„ç†ã€äº‹ä»¶é©±åŠ¨æ¶æ„

### å›½é™…æƒå¨é“¾æ¥

- [Apache Kafka](https://kafka.apache.org/)
- [RabbitMQ](https://www.rabbitmq.com/)
- [Apache Pulsar](https://pulsar.apache.org/)
- [Redis Streams](https://redis.io/topics/streams-intro)

---

## 3. æ ¸å¿ƒæ¶æ„æ¨¡å¼ä¸è®¾è®¡åŸåˆ™

### æ¶ˆæ¯é˜Ÿåˆ—åŸºç¡€æ¶æ„æ¨¡å¼

æ¶ˆæ¯é˜Ÿåˆ—é‡‡ç”¨**ç”Ÿäº§è€…-æ¶ˆè´¹è€…**æ¨¡å¼ï¼Œé€šè¿‡ä¸­é—´çš„æ¶ˆæ¯ä»£ç†ï¼ˆMessage Brokerï¼‰å®ç°å¼‚æ­¥é€šä¿¡å’Œè§£è€¦ã€‚

```mermaid
    subgraph "ç”Ÿäº§è€… (Producers)"
        P1[æœåŠ¡ A]
        P2[æœåŠ¡ B]
        P3[æœåŠ¡ C]
    end

    subgraph "æ¶ˆæ¯ä»£ç† (Message Broker)"
        T1[ä¸»é¢˜: orders]
        T2[ä¸»é¢˜: payments]
        T3[ä¸»é¢˜: notifications]
        DLQ[æ­»ä¿¡é˜Ÿåˆ— DLQ]
    end

    subgraph "æ¶ˆè´¹è€… (Consumers)"
        C1[è®¢å•å¤„ç†æœåŠ¡]
        C2[æ”¯ä»˜æœåŠ¡]
        C3[é€šçŸ¥æœåŠ¡]
        C4[å®¡è®¡æœåŠ¡]
    end

    P1 -- "å‘å¸ƒæ¶ˆæ¯" --> T1
    P2 -- "å‘å¸ƒæ¶ˆæ¯" --> T2
    P3 -- "å‘å¸ƒæ¶ˆæ¯" --> T3

    T1 -- "æ¶ˆè´¹æ¶ˆæ¯" --> C1
    T2 -- "æ¶ˆè´¹æ¶ˆæ¯" --> C2
    T3 -- "æ¶ˆè´¹æ¶ˆæ¯" --> C3
    T1 -- "æ¶ˆè´¹æ¶ˆæ¯" --> C4
    T2 -- "æ¶ˆè´¹æ¶ˆæ¯" --> C4

    T1 -- "å¤±è´¥æ¶ˆæ¯" --> DLQ
    T2 -- "å¤±è´¥æ¶ˆæ¯" --> DLQ
    T3 -- "å¤±è´¥æ¶ˆæ¯" --> DLQ

    style P1 fill:#e1f5fe
    style P2 fill:#e1f5fe
    style P3 fill:#e1f5fe
    style C1 fill:#f3e5f5
    style C2 fill:#f3e5f5
    style C3 fill:#f3e5f5
    style C4 fill:#f3e5f5
    style DLQ fill:#ffebee
```

### æ¶ˆæ¯äº¤ä»˜ä¿è¯ (Message Delivery Guarantees)

ä¸åŒçš„æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿæä¾›ä¸åŒçº§åˆ«çš„å¯é æ€§ä¿è¯ï¼š

#### At-most-once (æœ€å¤šä¸€æ¬¡)

æ¶ˆæ¯å¯èƒ½ä¼šä¸¢å¤±ï¼Œä½†ç»ä¸ä¼šé‡å¤ã€‚é€‚ç”¨äºå¯¹æ•°æ®ä¸¢å¤±å®¹å¿ã€ä½†ä¸èƒ½æ¥å—é‡å¤å¤„ç†çš„åœºæ™¯ï¼ˆå¦‚æ—¥å¿—è®°å½•ï¼‰ã€‚

#### At-least-once (è‡³å°‘ä¸€æ¬¡)

æ¶ˆæ¯ç»ä¸ä¼šä¸¢å¤±ï¼Œä½†å¯èƒ½ä¼šé‡å¤ã€‚è¿™æ˜¯æœ€å¸¸è§çš„ä¿è¯çº§åˆ«ã€‚éœ€è¦æ¶ˆè´¹è€…å®ç°**å¹‚ç­‰æ€§**å¤„ç†ã€‚

#### Exactly-once (ç²¾ç¡®ä¸€æ¬¡)

æ¶ˆæ¯æ—¢ä¸ä¼šä¸¢å¤±ä¹Ÿä¸ä¼šé‡å¤ã€‚å®ç°æˆæœ¬æœ€é«˜ï¼Œé€šå¸¸éœ€è¦åˆ†å¸ƒå¼äº‹åŠ¡æ”¯æŒã€‚

```go
// å®ç°å¹‚ç­‰æ¶ˆè´¹è€…çš„ç¤ºä¾‹
type IdempotentConsumer struct {
    processedMessages map[string]bool
    mu                sync.RWMutex
    storage           IdempotencyStorage
}

func (ic *IdempotentConsumer) ProcessMessage(ctx context.Context, msg *Message) error {
    // 1. æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²ç»å¤„ç†è¿‡
    if ic.hasProcessed(msg.ID) {
        log.Printf("Message %s already processed, skipping", msg.ID)
        return nil
    }

    // 2. åœ¨æ•°æ®åº“äº‹åŠ¡ä¸­å¤„ç†æ¶ˆæ¯å¹¶è®°å½•çŠ¶æ€
    tx, err := ic.storage.BeginTransaction()
    if err != nil {
        return err
    }
    defer tx.Rollback()

    // ä¸šåŠ¡é€»è¾‘å¤„ç†
    if err := ic.handleBusinessLogic(ctx, msg); err != nil {
        return err
    }

    // è®°å½•æ¶ˆæ¯å·²å¤„ç†
    if err := ic.storage.MarkProcessed(tx, msg.ID); err != nil {
        return err
    }

    return tx.Commit()
}

func (ic *IdempotentConsumer) hasProcessed(messageID string) bool {
    ic.mu.RLock()
    defer ic.mu.RUnlock()
    return ic.processedMessages[messageID]
}
```

### åˆ†åŒºä¸å¹¶è¡Œå¤„ç†

ä¸ºäº†å¤„ç†å¤§è§„æ¨¡æ¶ˆæ¯æµï¼Œç°ä»£æ¶ˆæ¯é˜Ÿåˆ—é‡‡ç”¨**åˆ†åŒº (Partitioning)** ç­–ç•¥ï¼Œå…è®¸å¹¶è¡Œå¤„ç†ã€‚

```mermaid
    subgraph "ä¸»é¢˜: user-events (åˆ†åŒº)"
        P0[åˆ†åŒº 0<br/>ç”¨æˆ· A, D, G...]
        P1[åˆ†åŒº 1<br/>ç”¨æˆ· B, E, H...]
        P2[åˆ†åŒº 2<br/>ç”¨æˆ· C, F, I...]
    end

    subgraph "æ¶ˆè´¹è€…ç»„: analytics"
        C0[æ¶ˆè´¹è€… 0]
        C1[æ¶ˆè´¹è€… 1]
        C2[æ¶ˆè´¹è€… 2]
    end

    P0 --> C0
    P1 --> C1
    P2 --> C2

    style P0 fill:#e8f5e8
    style P1 fill:#e8f5e8
    style P2 fill:#e8f5e8
    style C0 fill:#fff8e1
    style C1 fill:#fff8e1
    style C2 fill:#fff8e1
```

## 4. å¯é æ€§ä¿è¯

### æ¶ˆæ¯æŒä¹…åŒ–

```go
type MessageStorage struct {
    // å†…å­˜å­˜å‚¨
    MemoryStore *MemoryStore
    
    // ç£ç›˜å­˜å‚¨
    DiskStore *DiskStore
    
    // å‹ç¼©å­˜å‚¨
    CompressedStore *CompressedStore
    
    // å¤‡ä»½å­˜å‚¨
    BackupStore *BackupStore
}

type StorageStrategy struct {
    // å­˜å‚¨çº§åˆ«
    Level StorageLevel
    
    // åŒæ­¥ç­–ç•¥
    SyncPolicy SyncPolicy
    
    // å‹ç¼©ç­–ç•¥
    CompressionPolicy CompressionPolicy
    
    // æ¸…ç†ç­–ç•¥
    CleanupPolicy CleanupPolicy
}

type StorageLevel int

const (
    MemoryOnly StorageLevel = iota
    MemoryAndDisk
    DiskOnly
    Compressed
)

func (ms *MessageStorage) Store(topic string, partition int, message *Message) error {
    // 1. å†™å…¥å†…å­˜
    if err := ms.MemoryStore.Store(topic, partition, message); err != nil {
        return err
    }
    
    // 2. æ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦å†™å…¥ç£ç›˜
    if ms.shouldWriteToDisk(message) {
        if err := ms.DiskStore.Store(topic, partition, message); err != nil {
            return err
        }
    }
    
    // 3. å¼‚æ­¥å‹ç¼©
    if ms.shouldCompress(message) {
        go ms.CompressedStore.Store(topic, partition, message)
    }
    
    // 4. å¼‚æ­¥å¤‡ä»½
    go ms.BackupStore.Store(topic, partition, message)
    
    return nil
}

func (ms *MessageStorage) Fetch(topic string, partition int, offset int64) (*Message, error) {
    // 1. ä»å†…å­˜æŸ¥æ‰¾
    if message := ms.MemoryStore.Fetch(topic, partition, offset); message != nil {
        return message, nil
    }
    
    // 2. ä»ç£ç›˜æŸ¥æ‰¾
    if message := ms.DiskStore.Fetch(topic, partition, offset); message != nil {
        return message, nil
    }
    
    // 3. ä»å‹ç¼©å­˜å‚¨æŸ¥æ‰¾
    if message := ms.CompressedStore.Fetch(topic, partition, offset); message != nil {
        return message, nil
    }
    
    return nil, nil
}
```

### æ¶ˆæ¯ç¡®è®¤æœºåˆ¶

```go
type MessageAcknowledgment struct {
    // ç¡®è®¤ç­–ç•¥
    Policy AcknowledgmentPolicy
    
    // ç¡®è®¤å­˜å‚¨
    Store *AckStore
    
    // é‡è¯•æœºåˆ¶
    RetryManager *RetryManager
    
    // æ­»ä¿¡é˜Ÿåˆ—
    DeadLetterQueue *DeadLetterQueue
}

type AcknowledgmentPolicy int

const (
    AtMostOnce AcknowledgmentPolicy = iota
    AtLeastOnce
    ExactlyOnce
)

type MessageAck struct {
    MessageID   string
    ConsumerID  string
    Topic       string
    Partition   int
    Offset      int64
    Status      AckStatus
    Timestamp   time.Time
    RetryCount  int
}

func (ma *MessageAcknowledgment) Acknowledge(ctx context.Context, messageID string, consumerID string) error {
    // 1. è®°å½•ç¡®è®¤
    ack := &MessageAck{
        MessageID:  messageID,
        ConsumerID: consumerID,
        Status:     AckStatusAcknowledged,
        Timestamp:  time.Now(),
    }
    
    if err := ma.Store.Store(ack); err != nil {
        return err
    }
    
    // 2. æ›´æ–°åç§»é‡
    if err := ma.updateOffset(messageID, consumerID); err != nil {
        return err
    }
    
    return nil
}

func (ma *MessageAcknowledgment) HandleFailure(ctx context.Context, messageID string, consumerID string, error error) error {
    // 1. æ£€æŸ¥é‡è¯•æ¬¡æ•°
    retryCount := ma.RetryManager.GetRetryCount(messageID, consumerID)
    
    if retryCount < ma.Policy.MaxRetries {
        // 2. é‡è¯•æ¶ˆæ¯
        ma.RetryManager.ScheduleRetry(messageID, consumerID, retryCount+1)
        return nil
    }
    
    // 3. å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
    return ma.DeadLetterQueue.Send(messageID, consumerID, error)
}
```

## 5. æ€§èƒ½ä¼˜åŒ–

### æ‰¹é‡å¤„ç†

```go
type BatchProcessor struct {
    // æ‰¹å¤„ç†é…ç½®
    Config *BatchConfig
    
    // æ‰¹å¤„ç†é˜Ÿåˆ—
    Queue *BatchQueue
    
    // æ‰¹å¤„ç†æ‰§è¡Œå™¨
    Executor *BatchExecutor
    
    // æ‰¹å¤„ç†ç›‘æ§
    Monitor *BatchMonitor
}

type BatchConfig struct {
    MaxBatchSize    int
    MaxWaitTime     time.Duration
    MaxBufferSize   int
    Compression     bool
    Parallelism     int
}

type Batch struct {
    ID          string
    Messages    []*Message
    Size        int
    Created     time.Time
    Status      BatchStatus
}

func (bp *BatchProcessor) ProcessBatch(ctx context.Context, messages []*Message) error {
    // 1. åˆ›å»ºæ‰¹æ¬¡
    batch := &Batch{
        ID:       uuid.New().String(),
        Messages: messages,
        Size:     len(messages),
        Created:  time.Now(),
        Status:   BatchStatusProcessing,
    }
    
    // 2. å‹ç¼©æ‰¹æ¬¡
    if bp.Config.Compression {
        batch = bp.compressBatch(batch)
    }
    
    // 3. å¹¶è¡Œå¤„ç†
    if bp.Config.Parallelism > 1 {
        return bp.processBatchParallel(ctx, batch)
    }
    
    // 4. ä¸²è¡Œå¤„ç†
    return bp.processBatchSequential(ctx, batch)
}

func (bp *BatchProcessor) processBatchParallel(ctx context.Context, batch *Batch) error {
    // 1. åˆ†å‰²æ‰¹æ¬¡
    subBatches := bp.splitBatch(batch, bp.Config.Parallelism)
    
    // 2. å¹¶è¡Œå¤„ç†
    var wg sync.WaitGroup
    errors := make(chan error, len(subBatches))
    
    for _, subBatch := range subBatches {
        wg.Add(1)
        go func(sb *Batch) {
            defer wg.Done()
            if err := bp.Executor.Execute(ctx, sb); err != nil {
                errors <- err
            }
        }(subBatch)
    }
    
    wg.Wait()
    close(errors)
    
    // 3. æ£€æŸ¥é”™è¯¯
    for err := range errors {
        if err != nil {
            return err
        }
    }
    
    return nil
}
```

### æ¶ˆæ¯å‹ç¼©

```go
type MessageCompressor struct {
    // å‹ç¼©ç®—æ³•
    Algorithms map[string]CompressionAlgorithm
    
    // å‹ç¼©ç­–ç•¥
    Strategy *CompressionStrategy
    
    // å‹ç¼©ç¼“å­˜
    Cache *CompressionCache
    
    // å‹ç¼©ç›‘æ§
    Monitor *CompressionMonitor
}

type CompressionAlgorithm interface {
    Compress(data []byte) ([]byte, error)
    Decompress(data []byte) ([]byte, error)
    Name() string
}

type GzipCompressor struct{}

func (gc *GzipCompressor) Compress(data []byte) ([]byte, error) {
    var buf bytes.Buffer
    gz := gzip.NewWriter(&buf)
    
    if _, err := gz.Write(data); err != nil {
        return nil, err
    }
    
    if err := gz.Close(); err != nil {
        return nil, err
    }
    
    return buf.Bytes(), nil
}

func (gc *GzipCompressor) Decompress(data []byte) ([]byte, error) {
    gz, err := gzip.NewReader(bytes.NewReader(data))
    if err != nil {
        return nil, err
    }
    defer gz.Close()
    
    return ioutil.ReadAll(gz)
}

func (gc *GzipCompressor) Name() string {
    return "gzip"
}

type SnappyCompressor struct{}

func (sc *SnappyCompressor) Compress(data []byte) ([]byte, error) {
    return snappy.Encode(nil, data), nil
}

func (sc *SnappyCompressor) Decompress(data []byte) ([]byte, error) {
    return snappy.Decode(nil, data)
}

func (sc *SnappyCompressor) Name() string {
    return "snappy"
}

func (mc *MessageCompressor) CompressMessage(message *Message) error {
    // 1. é€‰æ‹©å‹ç¼©ç®—æ³•
    algorithm := mc.Strategy.SelectAlgorithm(message)
    
    // 2. æ£€æŸ¥ç¼“å­˜
    if cached := mc.Cache.Get(message.ID, algorithm.Name()); cached != nil {
        message.Value = cached
        return nil
    }
    
    // 3. æ‰§è¡Œå‹ç¼©
    compressed, err := algorithm.Compress(message.Value)
    if err != nil {
        return err
    }
    
    // 4. æ›´æ–°æ¶ˆæ¯
    message.Value = compressed
    message.Headers["compression"] = algorithm.Name()
    
    // 5. ç¼“å­˜ç»“æœ
    mc.Cache.Set(message.ID, algorithm.Name(), compressed)
    
    // 6. æ›´æ–°ç»Ÿè®¡
    mc.Monitor.RecordCompression(message.ID, len(message.Value), len(compressed))
    
    return nil
}
```

## 6. ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### æ¶ˆæ¯é˜Ÿåˆ—ç›‘æ§

```go
type QueueMonitor struct {
    // æ€§èƒ½æŒ‡æ ‡
    PerformanceMetrics *PerformanceMetrics
    
    // ä¸šåŠ¡æŒ‡æ ‡
    BusinessMetrics *BusinessMetrics
    
    // ç³»ç»ŸæŒ‡æ ‡
    SystemMetrics *SystemMetrics
    
    // å‘Šè­¦ç®¡ç†
    AlertManager *AlertManager
    
    // ä»ªè¡¨æ¿
    Dashboard *Dashboard
}

type PerformanceMetrics struct {
    // ååé‡
    Throughput *ThroughputMetrics
    
    // å»¶è¿Ÿ
    Latency *LatencyMetrics
    
    // é˜Ÿåˆ—æ·±åº¦
    QueueDepth *QueueDepthMetrics
    
    // é”™è¯¯ç‡
    ErrorRate *ErrorRateMetrics
}

type ThroughputMetrics struct {
    MessagesPerSecond float64
    BytesPerSecond    float64
    Producers         int
    Consumers         int
    Topics            int
    Partitions        int
}

type LatencyMetrics struct {
    PublishLatency    time.Duration
    ConsumeLatency    time.Duration
    EndToEndLatency   time.Duration
    Percentile95      time.Duration
    Percentile99      time.Duration
}

func (qm *QueueMonitor) CollectMetrics(ctx context.Context) (*QueueMetrics, error) {
    metrics := &QueueMetrics{
        Timestamp: time.Now(),
    }
    
    // 1. æ”¶é›†æ€§èƒ½æŒ‡æ ‡
    perfMetrics, err := qm.PerformanceMetrics.Collect(ctx)
    if err != nil {
        return nil, err
    }
    metrics.Performance = perfMetrics
    
    // 2. æ”¶é›†ä¸šåŠ¡æŒ‡æ ‡
    bizMetrics, err := qm.BusinessMetrics.Collect(ctx)
    if err != nil {
        return nil, err
    }
    metrics.Business = bizMetrics
    
    // 3. æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
    sysMetrics, err := qm.SystemMetrics.Collect(ctx)
    if err != nil {
        return nil, err
    }
    metrics.System = sysMetrics
    
    // 4. æ£€æŸ¥å‘Šè­¦
    qm.checkAlerts(metrics)
    
    return metrics, nil
}

func (qm *QueueMonitor) checkAlerts(metrics *QueueMetrics) {
    // 1. æ£€æŸ¥ååé‡å‘Šè­¦
    if metrics.Performance.Throughput.MessagesPerSecond < qm.AlertManager.ThroughputThreshold {
        qm.AlertManager.SendAlert(&Alert{
            Type:      "LowThroughput",
            Severity:  "Warning",
            Message:   fmt.Sprintf("Throughput below threshold: %.2f msg/s", metrics.Performance.Throughput.MessagesPerSecond),
            Timestamp: time.Now(),
        })
    }
    
    // 2. æ£€æŸ¥å»¶è¿Ÿå‘Šè­¦
    if metrics.Performance.Latency.EndToEndLatency > qm.AlertManager.LatencyThreshold {
        qm.AlertManager.SendAlert(&Alert{
            Type:      "HighLatency",
            Severity:  "Warning",
            Message:   fmt.Sprintf("Latency above threshold: %v", metrics.Performance.Latency.EndToEndLatency),
            Timestamp: time.Now(),
        })
    }
    
    // 3. æ£€æŸ¥é”™è¯¯ç‡å‘Šè­¦
    if metrics.Performance.ErrorRate.Rate > qm.AlertManager.ErrorRateThreshold {
        qm.AlertManager.SendAlert(&Alert{
            Type:      "HighErrorRate",
            Severity:  "Critical",
            Message:   fmt.Sprintf("Error rate above threshold: %.2f%%", metrics.Performance.ErrorRate.Rate*100),
            Timestamp: time.Now(),
        })
    }
}
```

## 7. åˆ†å¸ƒå¼æŒ‘æˆ˜ä¸ä¸»æµè§£å†³æ–¹æ¡ˆ

### æ¶ˆæ¯é‡è¯•ä¸é€€é¿ç­–ç•¥

å½“æ¶ˆæ¯å¤„ç†å¤±è´¥æ—¶ï¼Œéœ€è¦æ™ºèƒ½çš„é‡è¯•æœºåˆ¶æ¥æé«˜ç³»ç»Ÿçš„éŸ§æ€§ã€‚

```go
type RetryConfig struct {
    MaxRetries      int
    InitialBackoff  time.Duration
    MaxBackoff      time.Duration
    BackoffMultiplier float64
    Jitter          bool
}

type RetryProcessor struct {
    config   *RetryConfig
    dlqTopic string
}

func (rp *RetryProcessor) ProcessWithRetry(ctx context.Context, msg *Message, handler MessageHandler) error {
    var lastErr error
    
    for attempt := 0; attempt <= rp.config.MaxRetries; attempt++ {
        if attempt > 0 {
            // è®¡ç®—é€€é¿æ—¶é—´
            backoff := rp.calculateBackoff(attempt)
            
            select {
            case <-time.After(backoff):
                // ç»§ç»­é‡è¯•
            case <-ctx.Done():
                return ctx.Err()
            }
        }

        if err := handler.Handle(ctx, msg); err != nil {
            lastErr = err
            log.Printf("Message processing failed (attempt %d/%d): %v", 
                      attempt+1, rp.config.MaxRetries+1, err)
            continue
        }
        
        // æˆåŠŸå¤„ç†
        return nil
    }

    // æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œå‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
    log.Printf("Message processing failed after %d attempts, sending to DLQ", rp.config.MaxRetries+1)
    return rp.sendToDLQ(ctx, msg, lastErr)
}

func (rp *RetryProcessor) calculateBackoff(attempt int) time.Duration {
    backoff := time.Duration(float64(rp.config.InitialBackoff) * 
                           math.Pow(rp.config.BackoffMultiplier, float64(attempt-1)))
    
    if backoff > rp.config.MaxBackoff {
        backoff = rp.config.MaxBackoff
    }
    
    // æ·»åŠ æŠ–åŠ¨ä»¥é¿å…é›·ç¾¤æ•ˆåº”
    if rp.config.Jitter {
        jitter := time.Duration(rand.Float64() * float64(backoff) * 0.1)
        backoff += jitter
    }
    
    return backoff
}
```

### æ¶ˆæ¯å»é‡ä¸é‡å¤æ£€æµ‹

åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œç½‘ç»œæ•…éšœå¯èƒ½å¯¼è‡´æ¶ˆæ¯é‡å¤ã€‚éœ€è¦å®ç°æ¶ˆæ¯å»é‡æœºåˆ¶ã€‚

```go
type DeduplicationManager struct {
    bloomFilter   *BloomFilter
    recentHashes  *LRUCache
    hashFunction  hash.Hash
}

func NewDeduplicationManager(expectedItems int, falsePositiveRate float64) *DeduplicationManager {
    return &DeduplicationManager{
        bloomFilter:  NewBloomFilter(expectedItems, falsePositiveRate),
        recentHashes: NewLRUCache(expectedItems / 10),
        hashFunction: sha256.New(),
    }
}

func (dm *DeduplicationManager) IsDuplicate(msg *Message) bool {
    msgHash := dm.calculateHash(msg)
    
    // 1. å¿«é€Ÿæ£€æŸ¥ï¼šå¸ƒéš†è¿‡æ»¤å™¨
    if !dm.bloomFilter.Contains(msgHash) {
        // è‚¯å®šä¸æ˜¯é‡å¤æ¶ˆæ¯
        dm.bloomFilter.Add(msgHash)
        dm.recentHashes.Add(msgHash, true)
        return false
    }
    
    // 2. ç²¾ç¡®æ£€æŸ¥ï¼šLRUç¼“å­˜
    if dm.recentHashes.Contains(msgHash) {
        return true
    }
    
    // 3. å¯èƒ½æ˜¯æ–°æ¶ˆæ¯ï¼ˆå¸ƒéš†è¿‡æ»¤å™¨è¯¯æŠ¥ï¼‰
    dm.recentHashes.Add(msgHash, true)
    return false
}

func (dm *DeduplicationManager) calculateHash(msg *Message) string {
    dm.hashFunction.Reset()
    
    // åŸºäºæ¶ˆæ¯å†…å®¹å’Œå…³é”®å…ƒæ•°æ®è®¡ç®—å“ˆå¸Œ
    dm.hashFunction.Write([]byte(msg.Topic))
    dm.hashFunction.Write([]byte(msg.Key))
    dm.hashFunction.Write(msg.Value)
    
    return hex.EncodeToString(dm.hashFunction.Sum(nil))
}
```

### èƒŒå‹æ§åˆ¶ (Backpressure Control)

å½“æ¶ˆè´¹è€…å¤„ç†é€Ÿåº¦è·Ÿä¸ä¸Šç”Ÿäº§è€…æ—¶ï¼Œéœ€è¦èƒŒå‹æœºåˆ¶æ¥ä¿æŠ¤ç³»ç»Ÿã€‚

```go
type BackpressureController struct {
    maxQueueSize     int
    currentQueueSize int64
    rateLimiter      *rate.Limiter
    metrics          *BackpressureMetrics
    mu               sync.RWMutex
}

func (bpc *BackpressureController) CanAcceptMessage() bool {
    bpc.mu.RLock()
    current := atomic.LoadInt64(&bpc.currentQueueSize)
    bpc.mu.RUnlock()
    
    // æ£€æŸ¥é˜Ÿåˆ—å®¹é‡
    if current >= int64(bpc.maxQueueSize) {
        bpc.metrics.RecordRejection("queue_full")
        return false
    }
    
    // æ£€æŸ¥é€Ÿç‡é™åˆ¶
    if !bpc.rateLimiter.Allow() {
        bpc.metrics.RecordRejection("rate_limited")
        return false
    }
    
    return true
}

func (bpc *BackpressureController) MessageReceived() {
    atomic.AddInt64(&bpc.currentQueueSize, 1)
}

func (bpc *BackpressureController) MessageProcessed() {
    atomic.AddInt64(&bpc.currentQueueSize, -1)
}
```

### æ¶ˆæ¯åºåˆ—åŒ–ä¸å‹ç¼©

åœ¨é«˜ååé‡åœºæ™¯ä¸‹ï¼Œæ¶ˆæ¯åºåˆ—åŒ–å’Œå‹ç¼©å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚

```go
type MessageSerializer struct {
    compressionEnabled bool
    compressionLevel   int
    serializationFormat string // "json", "protobuf", "avro"
}

func (ms *MessageSerializer) Serialize(data interface{}) ([]byte, error) {
    var serialized []byte
    var err error
    
    // 1. åºåˆ—åŒ–
    switch ms.serializationFormat {
    case "json":
        serialized, err = json.Marshal(data)
    case "protobuf":
        if pb, ok := data.(proto.Message); ok {
            serialized, err = proto.Marshal(pb)
        } else {
            return nil, fmt.Errorf("data is not a protobuf message")
        }
    default:
        return nil, fmt.Errorf("unsupported serialization format: %s", ms.serializationFormat)
    }
    
    if err != nil {
        return nil, err
    }
    
    // 2. å‹ç¼©ï¼ˆå¯é€‰ï¼‰
    if ms.compressionEnabled {
        return ms.compress(serialized)
    }
    
    return serialized, nil
}

func (ms *MessageSerializer) compress(data []byte) ([]byte, error) {
    var buf bytes.Buffer
    
    writer, err := gzip.NewWriterLevel(&buf, ms.compressionLevel)
    if err != nil {
        return nil, err
    }
    
    if _, err := writer.Write(data); err != nil {
        return nil, err
    }
    
    if err := writer.Close(); err != nil {
        return nil, err
    }
    
    return buf.Bytes(), nil
}
```

## 8. å®é™…æ¡ˆä¾‹åˆ†æ

### é«˜å¹¶å‘ç”µå•†æ¶ˆæ¯ç³»ç»Ÿ

**åœºæ™¯**: æ”¯æŒç™¾ä¸‡çº§æ¶ˆæ¯å¤„ç†çš„ç”µå•†è®¢å•ç³»ç»Ÿ

```go
type ECommerceMessageSystem struct {
    // è®¢å•æ¶ˆæ¯é˜Ÿåˆ—
    OrderQueue *MessageQueue
    
    // åº“å­˜æ¶ˆæ¯é˜Ÿåˆ—
    InventoryQueue *MessageQueue
    
    // æ”¯ä»˜æ¶ˆæ¯é˜Ÿåˆ—
    PaymentQueue *MessageQueue
    
    // é€šçŸ¥æ¶ˆæ¯é˜Ÿåˆ—
    NotificationQueue *MessageQueue
    
    // äº‹ä»¶æ€»çº¿
    EventBus *EventBus
}

type OrderProcessor struct {
    orderQueue    *MessageQueue
    inventoryQueue *MessageQueue
    paymentQueue   *MessageQueue
    eventBus       *EventBus
}

func (op *OrderProcessor) ProcessOrder(ctx context.Context, order *Order) error {
    // 1. å‘å¸ƒè®¢å•åˆ›å»ºäº‹ä»¶
    orderEvent := &OrderEvent{
        Type:      "OrderCreated",
        OrderID:   order.ID,
        UserID:    order.UserID,
        Amount:    order.Amount,
        Timestamp: time.Now(),
    }
    
    if err := op.orderQueue.Publish(ctx, "orders", &Message{
        Key:   order.ID,
        Value: orderEvent.ToJSON(),
    }); err != nil {
        return err
    }
    
    // 2. æ£€æŸ¥åº“å­˜
    inventoryEvent := &InventoryEvent{
        Type:      "InventoryCheck",
        OrderID:   order.ID,
        Items:     order.Items,
        Timestamp: time.Now(),
    }
    
    if err := op.inventoryQueue.Publish(ctx, "inventory", &Message{
        Key:   order.ID,
        Value: inventoryEvent.ToJSON(),
    }); err != nil {
        return err
    }
    
    // 3. å¤„ç†æ”¯ä»˜
    paymentEvent := &PaymentEvent{
        Type:      "PaymentProcess",
        OrderID:   order.ID,
        Amount:    order.Amount,
        Method:    order.PaymentMethod,
        Timestamp: time.Now(),
    }
    
    if err := op.paymentQueue.Publish(ctx, "payments", &Message{
        Key:   order.ID,
        Value: paymentEvent.ToJSON(),
    }); err != nil {
        return err
    }
    
    return nil
}
```

### å®æ—¶æ—¥å¿—å¤„ç†ç³»ç»Ÿ

**åœºæ™¯**: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ—¥å¿—æ”¶é›†ä¸åˆ†æ

```go
type LogProcessingSystem struct {
    // æ—¥å¿—æ”¶é›†å™¨
    Collectors []*LogCollector
    
    // æ—¥å¿—é˜Ÿåˆ—
    LogQueue *MessageQueue
    
    // æ—¥å¿—å¤„ç†å™¨
    Processors []*LogProcessor
    
    // æ—¥å¿—å­˜å‚¨
    Storage *LogStorage
    
    // å®æ—¶åˆ†æ
    Analyzer *LogAnalyzer
}

type LogCollector struct {
    ID          string
    Sources     []LogSource
    Filter      *LogFilter
    Formatter   *LogFormatter
    queue       *MessageQueue
}

type LogSource struct {
    Type        string
    Path        string
    Pattern     string
    Format      string
    Filters     []string
}

func (lc *LogCollector) CollectLogs(ctx context.Context) error {
    for _, source := range lc.Sources {
        go func(s LogSource) {
            lc.collectFromSource(ctx, s)
        }(source)
    }
    return nil
}

func (lc *LogCollector) collectFromSource(ctx context.Context, source LogSource) {
    // 1. è¯»å–æ—¥å¿—æ–‡ä»¶
    files, err := filepath.Glob(source.Pattern)
    if err != nil {
        return
    }
    
    for _, file := range files {
        // 2. ç›‘æ§æ–‡ä»¶å˜åŒ–
        watcher, err := fsnotify.NewWatcher()
        if err != nil {
            continue
        }
        defer watcher.Close()
        
        if err := watcher.Add(file); err != nil {
            continue
        }
        
        // 3. å¤„ç†æ–‡ä»¶å˜åŒ–
        for {
            select {
            case event := <-watcher.Events:
                if event.Op&fsnotify.Write == fsnotify.Write {
                    lc.processLogFile(ctx, file, source)
                }
            case <-ctx.Done():
                return
            }
        }
    }
}

func (lc *LogCollector) processLogFile(ctx context.Context, filepath string, source LogSource) error {
    // 1. è¯»å–æ–°æ—¥å¿—è¡Œ
    file, err := os.Open(filepath)
    if err != nil {
        return err
    }
    defer file.Close()
    
    scanner := bufio.NewScanner(file)
    for scanner.Scan() {
        line := scanner.Text()
        
        // 2. è¿‡æ»¤æ—¥å¿—
        if !lc.Filter.Match(line, source.Filters) {
            continue
        }
        
        // 3. æ ¼å¼åŒ–æ—¥å¿—
        formatted := lc.Formatter.Format(line, source.Format)
        
        // 4. å‘é€åˆ°é˜Ÿåˆ—
        message := &Message{
            Topic: "logs",
            Key:   source.Type,
            Value: []byte(formatted),
            Headers: map[string]string{
                "source": source.Type,
                "file":   filepath,
            },
            Timestamp: time.Now(),
        }
        
        if err := lc.queue.Publish(ctx, "logs", message); err != nil {
            return err
        }
    }
    
    return scanner.Err()
}
```

## 9. æœªæ¥è¶‹åŠ¿ä¸å›½é™…å‰æ²¿

- **äº‘åŸç”Ÿæ¶ˆæ¯æœåŠ¡**
- **äº‹ä»¶æµå¤„ç†ä¸CEP**
- **AI/MLé©±åŠ¨çš„æ¶ˆæ¯è·¯ç”±**
- **è¾¹ç¼˜è®¡ç®—æ¶ˆæ¯å¤„ç†**
- **é‡å­æ¶ˆæ¯é˜Ÿåˆ—**
- **å¤šæ¨¡æ€æ¶ˆæ¯æ”¯æŒ**

## 10. å›½é™…æƒå¨èµ„æºä¸å¼€æºç»„ä»¶å¼•ç”¨

### æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ

- [Apache Kafka](https://kafka.apache.org/) - åˆ†å¸ƒå¼æµå¤„ç†å¹³å°
- [RabbitMQ](https://www.rabbitmq.com/) - ä¼ä¸šçº§æ¶ˆæ¯ä»£ç†
- [Apache Pulsar](https://pulsar.apache.org/) - äº‘åŸç”Ÿæ¶ˆæ¯æµå¹³å°
- [Redis Streams](https://redis.io/topics/streams-intro) - å†…å­˜æ¶ˆæ¯æµ

### äº‘åŸç”Ÿæ¶ˆæ¯æœåŠ¡

- [Amazon SQS](https://aws.amazon.com/sqs/) - ç®€å•é˜Ÿåˆ—æœåŠ¡
- [Amazon SNS](https://aws.amazon.com/sns/) - ç®€å•é€šçŸ¥æœåŠ¡
- [Google Cloud Pub/Sub](https://cloud.google.com/pubsub) - å®æ—¶æ¶ˆæ¯æœåŠ¡
- [Azure Service Bus](https://azure.microsoft.com/services/service-bus/) - ä¼ä¸šæ¶ˆæ¯æœåŠ¡

### æ¶ˆæ¯å¤„ç†æ¡†æ¶

- [Apache Storm](https://storm.apache.org/) - å®æ—¶æµå¤„ç†
- [Apache Flink](https://flink.apache.org/) - æµå¤„ç†å¼•æ“
- [Apache Spark Streaming](https://spark.apache.org/streaming/) - å¾®æ‰¹å¤„ç†

## 11. ç›¸å…³æ¶æ„ä¸»é¢˜

- [**äº‹ä»¶é©±åŠ¨æ¶æ„ (Event-Driven Architecture)**](./architecture_event_driven_golang.md): æ¶ˆæ¯é˜Ÿåˆ—æ˜¯å®ç°äº‹ä»¶é©±åŠ¨æ¶æ„çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚
- [**å¾®æœåŠ¡æ¶æ„ (Microservice Architecture)**](./architecture_microservice_golang.md): æ¶ˆæ¯é˜Ÿåˆ—ä¸ºå¾®æœåŠ¡é—´çš„å¼‚æ­¥é€šä¿¡æä¾›äº†å¯é çš„è§£è€¦æœºåˆ¶ã€‚
- [**æ•°æ®æµæ¶æ„ (Dataflow Architecture)**](./architecture_dataflow_golang.md): æ¶ˆæ¯é˜Ÿåˆ—æ˜¯æ„å»ºå®æ—¶æ•°æ®å¤„ç†ç®¡é“çš„å…³é”®ç»„ä»¶ã€‚
- [**DevOpsä¸è¿ç»´æ¶æ„ (DevOps & Operations Architecture)**](./architecture_devops_golang.md): æ¶ˆæ¯é˜Ÿåˆ—çš„ç›‘æ§ã€å‘Šè­¦å’Œè‡ªåŠ¨è¿ç»´æ˜¯DevOpså®è·µçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

## 12. æ‰©å±•é˜…è¯»ä¸å‚è€ƒæ–‡çŒ®

1. "Kafka: The Definitive Guide" - Neha Narkhede, Gwen Shapira, Todd Palino
2. "Designing Data-Intensive Applications" - Martin Kleppmann
3. "Event Streaming with Kafka" - Alexander Dean
4. "RabbitMQ in Action" - Alvaro Videla, Jason J.W. Williams
5. "Stream Processing with Apache Flink" - Fabian Hueske, Vasiliki Kalavri

---

- æœ¬æ–‡æ¡£ä¸¥æ ¼å¯¹æ ‡å›½é™…ä¸»æµæ ‡å‡†ï¼Œé‡‡ç”¨å¤šè¡¨å¾è¾“å‡ºï¼Œä¾¿äºåç»­æ–­ç‚¹ç»­å†™å’Œæ‰¹é‡å¤„ç†ã€‚*

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Go Documentation Team  
**æœ€åæ›´æ–°**: 2025å¹´10æœˆ20æ—¥  
**æ–‡æ¡£çŠ¶æ€**: å®Œæˆ  
**é€‚ç”¨ç‰ˆæœ¬**: Go 1.25.3+
