# Go 1.25.3æœç´¢å¼•æ“ä¸å…¨æ–‡æ£€ç´¢å®Œæ•´å®æˆ˜

> **ElasticsearchÂ·BleveÂ·å€’æ’ç´¢å¼•Â·åˆ†è¯å™¨Â·ç›¸å…³æ€§æ’åº**  
> ğŸ“ **éš¾åº¦**ï¼šâ­â­â­â­â­ (é«˜çº§)  
> ğŸ¯ **ç›®æ ‡**ï¼šæ„å»ºç”Ÿäº§çº§æœç´¢å¼•æ“  
> â± **é¢„è®¡æ—¶é—´**ï¼š6-8å°æ—¶  
> ğŸ”§ **æŠ€æœ¯æ ˆ**ï¼šElasticsearchÂ·BleveÂ·TF-IDFÂ·BM25Â·ä¸­æ–‡åˆ†è¯Â·å‘é‡æ£€ç´¢

---


## ğŸ“‹ ç›®å½•


- [ç›®å½•](#ç›®å½•)
- [1. æœç´¢å¼•æ“åŸç†](#1-æœç´¢å¼•æ“åŸç†)
  - [1.1 æœç´¢å¼•æ“æ¶æ„](#11-æœç´¢å¼•æ“æ¶æ„)
- [2. å€’æ’ç´¢å¼•](#2-å€’æ’ç´¢å¼•)
  - [2.1 å€’æ’ç´¢å¼•åŸç†](#21-å€’æ’ç´¢å¼•åŸç†)
- [3. Bleveæœ¬åœ°æœç´¢](#3-bleveæœ¬åœ°æœç´¢)
  - [3.1 Bleveå…¨æ–‡æœç´¢](#31-bleveå…¨æ–‡æœç´¢)
  - [3.2 Bleveé«˜çº§æŸ¥è¯¢](#32-bleveé«˜çº§æŸ¥è¯¢)
- [4. Elasticsearché›†æˆ](#4-elasticsearché›†æˆ)
  - [4.1 Elasticsearchå®¢æˆ·ç«¯](#41-elasticsearchå®¢æˆ·ç«¯)
  - [4.2 Elasticsearché«˜çº§æŸ¥è¯¢](#42-elasticsearché«˜çº§æŸ¥è¯¢)
- [5. ä¸­æ–‡åˆ†è¯](#5-ä¸­æ–‡åˆ†è¯)
  - [5.1 jiebaä¸­æ–‡åˆ†è¯](#51-jiebaä¸­æ–‡åˆ†è¯)
- [6. ç›¸å…³æ€§æ’åº](#6-ç›¸å…³æ€§æ’åº)
  - [6.1 TF-IDFç®—æ³•](#61-tf-idfç®—æ³•)
  - [6.2 BM25ç®—æ³•](#62-bm25ç®—æ³•)
- [7. å‘é‡æ£€ç´¢](#7-å‘é‡æ£€ç´¢)
  - [7.1 å‘é‡æœç´¢](#71-å‘é‡æœç´¢)
- [8. æœç´¢å»ºè®®](#8-æœç´¢å»ºè®®)
  - [8.1 è‡ªåŠ¨è¡¥å…¨](#81-è‡ªåŠ¨è¡¥å…¨)
- [9. æ€§èƒ½ä¼˜åŒ–](#9-æ€§èƒ½ä¼˜åŒ–)
  - [9.1 ç´¢å¼•ä¼˜åŒ–](#91-ç´¢å¼•ä¼˜åŒ–)
- [10. ç”Ÿäº§éƒ¨ç½²](#10-ç”Ÿäº§éƒ¨ç½²)
  - [10.1 Dockeréƒ¨ç½²](#101-dockeréƒ¨ç½²)
- [æ€»ç»“](#æ€»ç»“)
  - [âœ… æ ¸å¿ƒæˆæœ](#-æ ¸å¿ƒæˆæœ)
  - [ğŸŒŸ Go 1.25.3åº”ç”¨](#-go-1253åº”ç”¨)
  - [ğŸ“Š æ¶æ„äº®ç‚¹](#-æ¶æ„äº®ç‚¹)
  - [ğŸ¯ é€‚ç”¨åœºæ™¯](#-é€‚ç”¨åœºæ™¯)
- [ä¸‹ä¸€æ­¥å­¦ä¹ ](#ä¸‹ä¸€æ­¥å­¦ä¹ )

## ç›®å½•

- [Go 1.25.3æœç´¢å¼•æ“ä¸å…¨æ–‡æ£€ç´¢å®Œæ•´å®æˆ˜](#go-1253æœç´¢å¼•æ“ä¸å…¨æ–‡æ£€ç´¢å®Œæ•´å®æˆ˜)
  - [ç›®å½•](#ç›®å½•)
  - [1. æœç´¢å¼•æ“åŸç†](#1-æœç´¢å¼•æ“åŸç†)
    - [1.1 æœç´¢å¼•æ“æ¶æ„](#11-æœç´¢å¼•æ“æ¶æ„)
  - [2. å€’æ’ç´¢å¼•](#2-å€’æ’ç´¢å¼•)
    - [2.1 å€’æ’ç´¢å¼•åŸç†](#21-å€’æ’ç´¢å¼•åŸç†)
  - [3. Bleveæœ¬åœ°æœç´¢](#3-bleveæœ¬åœ°æœç´¢)
    - [3.1 Bleveå…¨æ–‡æœç´¢](#31-bleveå…¨æ–‡æœç´¢)
    - [3.2 Bleveé«˜çº§æŸ¥è¯¢](#32-bleveé«˜çº§æŸ¥è¯¢)
  - [4. Elasticsearché›†æˆ](#4-elasticsearché›†æˆ)
    - [4.1 Elasticsearchå®¢æˆ·ç«¯](#41-elasticsearchå®¢æˆ·ç«¯)
    - [4.2 Elasticsearché«˜çº§æŸ¥è¯¢](#42-elasticsearché«˜çº§æŸ¥è¯¢)
  - [5. ä¸­æ–‡åˆ†è¯](#5-ä¸­æ–‡åˆ†è¯)
    - [5.1 jiebaä¸­æ–‡åˆ†è¯](#51-jiebaä¸­æ–‡åˆ†è¯)
  - [6. ç›¸å…³æ€§æ’åº](#6-ç›¸å…³æ€§æ’åº)
    - [6.1 TF-IDFç®—æ³•](#61-tf-idfç®—æ³•)
    - [6.2 BM25ç®—æ³•](#62-bm25ç®—æ³•)
  - [7. å‘é‡æ£€ç´¢](#7-å‘é‡æ£€ç´¢)
    - [7.1 å‘é‡æœç´¢](#71-å‘é‡æœç´¢)
  - [8. æœç´¢å»ºè®®](#8-æœç´¢å»ºè®®)
    - [8.1 è‡ªåŠ¨è¡¥å…¨](#81-è‡ªåŠ¨è¡¥å…¨)
  - [9. æ€§èƒ½ä¼˜åŒ–](#9-æ€§èƒ½ä¼˜åŒ–)
    - [9.1 ç´¢å¼•ä¼˜åŒ–](#91-ç´¢å¼•ä¼˜åŒ–)
  - [10. ç”Ÿäº§éƒ¨ç½²](#10-ç”Ÿäº§éƒ¨ç½²)
    - [10.1 Dockeréƒ¨ç½²](#101-dockeréƒ¨ç½²)
  - [æ€»ç»“](#æ€»ç»“)
    - [âœ… æ ¸å¿ƒæˆæœ](#-æ ¸å¿ƒæˆæœ)
    - [ğŸŒŸ Go 1.25.3åº”ç”¨](#-go-1253åº”ç”¨)
    - [ğŸ“Š æ¶æ„äº®ç‚¹](#-æ¶æ„äº®ç‚¹)
    - [ğŸ¯ é€‚ç”¨åœºæ™¯](#-é€‚ç”¨åœºæ™¯)
  - [ä¸‹ä¸€æ­¥å­¦ä¹ ](#ä¸‹ä¸€æ­¥å­¦ä¹ )

---

## 1. æœç´¢å¼•æ“åŸç†

### 1.1 æœç´¢å¼•æ“æ¶æ„

```go
package search

// æœç´¢å¼•æ“æ ¸å¿ƒæ¦‚å¿µ
/*
æœç´¢å¼•æ“å·¥ä½œæµç¨‹:

1. ç´¢å¼•é˜¶æ®µ (Indexing):
   æ–‡æ¡£ â†’ åˆ†è¯ (Tokenization) â†’ å€’æ’ç´¢å¼• (Inverted Index)

2. æŸ¥è¯¢é˜¶æ®µ (Querying):
   æŸ¥è¯¢ â†’ è§£æ â†’ å€’æ’ç´¢å¼•æŸ¥æ‰¾ â†’ ç›¸å…³æ€§æ’åº â†’ è¿”å›ç»“æœ

æ ¸å¿ƒç»„ä»¶:
  â€¢ çˆ¬è™« (Crawler): è·å–æ–‡æ¡£
  â€¢ åˆ†è¯å™¨ (Tokenizer): æ–‡æœ¬åˆ†è¯
  â€¢ ç´¢å¼•å™¨ (Indexer): æ„å»ºå€’æ’ç´¢å¼•
  â€¢ æŸ¥è¯¢å¼•æ“ (Query Engine): å¤„ç†æŸ¥è¯¢
  â€¢ æ’åºå™¨ (Ranker): ç›¸å…³æ€§æ’åº

å…³é”®æŠ€æœ¯:
  âœ… å€’æ’ç´¢å¼• (Inverted Index)
  âœ… TF-IDFç›¸å…³æ€§ç®—æ³•
  âœ… BM25æ’åºç®—æ³•
  âœ… ä¸­æ–‡åˆ†è¯ (jieba, sego)
  âœ… å‘é‡æ£€ç´¢ (Vector Search)
  âœ… æ¨¡ç³Šæœç´¢ (Fuzzy Search)
*/

// SearchEngine æœç´¢å¼•æ“æ¥å£
type SearchEngine interface {
 // ç´¢å¼•æ–‡æ¡£
 IndexDocument(doc Document) error
 BatchIndex(docs []Document) error

 // æœç´¢
 Search(query string, opts SearchOptions) (*SearchResult, error)

 // åˆ é™¤
 Delete(id string) error

 // æ›´æ–°
 Update(doc Document) error

 // å…³é—­
 Close() error
}

// Document æ–‡æ¡£
type Document struct {
 ID      string                 `json:"id"`
 Title   string                 `json:"title"`
 Content string                 `json:"content"`
 Fields  map[string]interface{} `json:"fields"`
 Score   float64                `json:"score,omitempty"`
}

// SearchOptions æœç´¢é€‰é¡¹
type SearchOptions struct {
 From      int               // åˆ†é¡µèµ·å§‹
 Size      int               // æ¯é¡µå¤§å°
 Sort      []SortField       // æ’åºå­—æ®µ
 Highlight bool              // æ˜¯å¦é«˜äº®
 Filters   map[string]string // è¿‡æ»¤æ¡ä»¶
}

// SortField æ’åºå­—æ®µ
type SortField struct {
 Field string
 Desc  bool
}

// SearchResult æœç´¢ç»“æœ
type SearchResult struct {
 Total int64      `json:"total"`
 Hits  []Document `json:"hits"`
 Took  int64      `json:"took_ms"` // è€—æ—¶(æ¯«ç§’)
}
```

---

## 2. å€’æ’ç´¢å¼•

### 2.1 å€’æ’ç´¢å¼•åŸç†

```go
package index

import (
 "strings"
 "sync"
)

// å€’æ’ç´¢å¼• (Inverted Index)
/*
æ­£æ’ç´¢å¼• (Forward Index):
  DocID â†’ Words
  Doc1  â†’ ["Go", "programming", "language"]
  Doc2  â†’ ["Go", "is", "fast"]

å€’æ’ç´¢å¼• (Inverted Index):
  Word         â†’ DocIDs
  "Go"         â†’ [Doc1, Doc2]
  "programming" â†’ [Doc1]
  "language"   â†’ [Doc1]
  "is"         â†’ [Doc2]
  "fast"       â†’ [Doc2]

ä¼˜åŠ¿:
  âœ… O(1)æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ–‡æ¡£
  âœ… æ”¯æŒå¸ƒå°”æŸ¥è¯¢ (AND, OR, NOT)
  âœ… æ”¯æŒçŸ­è¯­æŸ¥è¯¢
*/

// InvertedIndex å€’æ’ç´¢å¼•
type InvertedIndex struct {
 mu     sync.RWMutex
 index  map[string]PostingList // term -> posting list
 docs   map[string]*Document    // docID -> document
 docCount int
}

// PostingList å€’æ’åˆ—è¡¨
type PostingList struct {
 DocIDs   []string  // æ–‡æ¡£IDåˆ—è¡¨
 Positions [][]int  // è¯åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®
}

// NewInvertedIndex åˆ›å»ºå€’æ’ç´¢å¼•
func NewInvertedIndex() *InvertedIndex {
 return &InvertedIndex{
  index: make(map[string]PostingList),
  docs:  make(map[string]*Document),
 }
}

// IndexDocument ç´¢å¼•æ–‡æ¡£
func (idx *InvertedIndex) IndexDocument(doc Document) error {
 idx.mu.Lock()
 defer idx.mu.Unlock()

 // å­˜å‚¨æ–‡æ¡£
 idx.docs[doc.ID] = &doc
 idx.docCount++

 // åˆ†è¯
 tokens := tokenize(doc.Content)

 // æ„å»ºå€’æ’ç´¢å¼•
 for pos, token := range tokens {
  token = strings.ToLower(token) // è½¬å°å†™

  posting := idx.index[token]
  
  // æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
  docIndex := -1
  for i, docID := range posting.DocIDs {
   if docID == doc.ID {
    docIndex = i
    break
   }
  }

  if docIndex == -1 {
   // æ–°æ–‡æ¡£
   posting.DocIDs = append(posting.DocIDs, doc.ID)
   posting.Positions = append(posting.Positions, []int{pos})
  } else {
   // å·²å­˜åœ¨æ–‡æ¡£ï¼Œæ·»åŠ ä½ç½®
   posting.Positions[docIndex] = append(posting.Positions[docIndex], pos)
  }

  idx.index[token] = posting
 }

 return nil
}

// Search æœç´¢
func (idx *InvertedIndex) Search(query string) []Document {
 idx.mu.RLock()
 defer idx.mu.RUnlock()

 // åˆ†è¯æŸ¥è¯¢
 queryTokens := tokenize(query)

 // æŸ¥æ‰¾åŒ…å«æ‰€æœ‰æŸ¥è¯¢è¯çš„æ–‡æ¡£ (ANDæŸ¥è¯¢)
 var resultDocIDs []string

 for i, token := range queryTokens {
  token = strings.ToLower(token)

  posting, ok := idx.index[token]
  if !ok {
   return []Document{} // æŸä¸ªè¯ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºç»“æœ
  }

  if i == 0 {
   resultDocIDs = posting.DocIDs
  } else {
   // å–äº¤é›†
   resultDocIDs = intersection(resultDocIDs, posting.DocIDs)
  }

  if len(resultDocIDs) == 0 {
   break
  }
 }

 // è·å–æ–‡æ¡£
 var results []Document
 for _, docID := range resultDocIDs {
  if doc, ok := idx.docs[docID]; ok {
   results = append(results, *doc)
  }
 }

 return results
}

// SearchOR ORæŸ¥è¯¢
func (idx *InvertedIndex) SearchOR(query string) []Document {
 idx.mu.RLock()
 defer idx.mu.RUnlock()

 queryTokens := tokenize(query)

 // æ”¶é›†æ‰€æœ‰æ–‡æ¡£ID (å»é‡)
 docIDSet := make(map[string]bool)

 for _, token := range queryTokens {
  token = strings.ToLower(token)

  if posting, ok := idx.index[token]; ok {
   for _, docID := range posting.DocIDs {
    docIDSet[docID] = true
   }
  }
 }

 // è·å–æ–‡æ¡£
 var results []Document
 for docID := range docIDSet {
  if doc, ok := idx.docs[docID]; ok {
   results = append(results, *doc)
  }
 }

 return results
}

// PhraseSearch çŸ­è¯­æŸ¥è¯¢
func (idx *InvertedIndex) PhraseSearch(phrase string) []Document {
 idx.mu.RLock()
 defer idx.mu.RUnlock()

 tokens := tokenize(phrase)
 if len(tokens) == 0 {
  return []Document{}
 }

 // è·å–ç¬¬ä¸€ä¸ªè¯çš„posting list
 firstToken := strings.ToLower(tokens[0])
 posting, ok := idx.index[firstToken]
 if !ok {
  return []Document{}
 }

 var results []Document

 // æ£€æŸ¥æ¯ä¸ªæ–‡æ¡£
 for docIdx, docID := range posting.DocIDs {
  positions := posting.Positions[docIdx]

  // æ£€æŸ¥çŸ­è¯­æ˜¯å¦åŒ¹é…
  for _, startPos := range positions {
   match := true

   for i := 1; i < len(tokens); i++ {
    token := strings.ToLower(tokens[i])
    nextPosting, ok := idx.index[token]
    if !ok {
     match = false
     break
    }

    // æŸ¥æ‰¾docID
    found := false
    for nextDocIdx, nextDocID := range nextPosting.DocIDs {
     if nextDocID == docID {
      // æ£€æŸ¥ä½ç½®æ˜¯å¦è¿ç»­
      nextPositions := nextPosting.Positions[nextDocIdx]
      expectedPos := startPos + i

      hasPosition := false
      for _, pos := range nextPositions {
       if pos == expectedPos {
        hasPosition = true
        break
       }
      }

      if !hasPosition {
       match = false
      }

      found = true
      break
     }
    }

    if !found {
     match = false
     break
    }
   }

   if match {
    if doc, ok := idx.docs[docID]; ok {
     results = append(results, *doc)
    }
    break // å·²æ‰¾åˆ°è¯¥æ–‡æ¡£ï¼Œè·³è¿‡å…¶ä»–ä½ç½®
   }
  }
 }

 return results
}

// tokenize ç®€å•åˆ†è¯ (ç©ºæ ¼åˆ†éš”)
func tokenize(text string) []string {
 return strings.Fields(text)
}

// intersection å–äº¤é›†
func intersection(a, b []string) []string {
 set := make(map[string]bool)
 for _, item := range a {
  set[item] = true
 }

 var result []string
 for _, item := range b {
  if set[item] {
   result = append(result, item)
  }
 }

 return result
}
```

---

## 3. Bleveæœ¬åœ°æœç´¢

### 3.1 Bleveå…¨æ–‡æœç´¢

```go
package bleve

import (
 "fmt"
 "log"

 "github.com/blevesearch/bleve/v2"
 "github.com/blevesearch/bleve/v2/analysis/analyzer/custom"
 "github.com/blevesearch/bleve/v2/analysis/token/lowercase"
 "github.com/blevesearch/bleve/v2/analysis/tokenizer/unicode"
 "github.com/blevesearch/bleve/v2/mapping"
)

// Bleve - Pure Goå…¨æ–‡æœç´¢
/*
Bleveç‰¹ç‚¹:
  âœ… çº¯Goå®ç° (æ— Cä¾èµ–)
  âœ… æ”¯æŒä¸­æ–‡åˆ†è¯
  âœ… æ”¯æŒæ¨¡ç³ŠæŸ¥è¯¢
  âœ… æ”¯æŒåœ°ç†ä½ç½®æœç´¢
  âœ… åµŒå…¥å¼ (æ— éœ€å¤–éƒ¨æœåŠ¡)

é€‚ç”¨åœºæ™¯:
  â€¢ å°å‹åº”ç”¨ (<100ä¸‡æ–‡æ¡£)
  â€¢ åµŒå…¥å¼æœç´¢
  â€¢ ç¦»çº¿æœç´¢
*/

// BleveEngine Bleveæœç´¢å¼•æ“
type BleveEngine struct {
 index bleve.Index
 path  string
}

// NewBleveEngine åˆ›å»ºBleveå¼•æ“
func NewBleveEngine(indexPath string) (*BleveEngine, error) {
 // åˆ›å»ºç´¢å¼•æ˜ å°„
 indexMapping := buildIndexMapping()

 // æ‰“å¼€æˆ–åˆ›å»ºç´¢å¼•
 var index bleve.Index
 var err error

 index, err = bleve.Open(indexPath)
 if err == bleve.ErrorIndexPathDoesNotExist {
  // åˆ›å»ºæ–°ç´¢å¼•
  index, err = bleve.New(indexPath, indexMapping)
  if err != nil {
   return nil, fmt.Errorf("åˆ›å»ºç´¢å¼•å¤±è´¥: %w", err)
  }
  log.Printf("âœ… åˆ›å»ºæ–°ç´¢å¼•: %s", indexPath)
 } else if err != nil {
  return nil, fmt.Errorf("æ‰“å¼€ç´¢å¼•å¤±è´¥: %w", err)
 } else {
  log.Printf("âœ… æ‰“å¼€å·²æœ‰ç´¢å¼•: %s", indexPath)
 }

 return &BleveEngine{
  index: index,
  path:  indexPath,
 }, nil
}

// buildIndexMapping æ„å»ºç´¢å¼•æ˜ å°„
func buildIndexMapping() mapping.IndexMapping {
 // æ–‡æ¡£æ˜ å°„
 docMapping := bleve.NewDocumentMapping()

 // æ ‡é¢˜å­—æ®µ (æƒé‡æ›´é«˜)
 titleFieldMapping := bleve.NewTextFieldMapping()
 titleFieldMapping.Analyzer = "standard"
 titleFieldMapping.IncludeInAll = true
 docMapping.AddFieldMappingsAt("title", titleFieldMapping)

 // å†…å®¹å­—æ®µ
 contentFieldMapping := bleve.NewTextFieldMapping()
 contentFieldMapping.Analyzer = "standard"
 contentFieldMapping.IncludeInAll = true
 docMapping.AddFieldMappingsAt("content", contentFieldMapping)

 // ç´¢å¼•æ˜ å°„
 indexMapping := bleve.NewIndexMapping()
 indexMapping.AddDocumentMapping("_default", docMapping)

 return indexMapping
}

// IndexDocument ç´¢å¼•æ–‡æ¡£
func (be *BleveEngine) IndexDocument(doc Document) error {
 err := be.index.Index(doc.ID, doc)
 if err != nil {
  return fmt.Errorf("ç´¢å¼•å¤±è´¥: %w", err)
 }

 return nil
}

// BatchIndex æ‰¹é‡ç´¢å¼•
func (be *BleveEngine) BatchIndex(docs []Document) error {
 batch := be.index.NewBatch()

 for _, doc := range docs {
  if err := batch.Index(doc.ID, doc); err != nil {
   return fmt.Errorf("æ‰¹é‡ç´¢å¼•å¤±è´¥: %w", err)
  }
 }

 return be.index.Batch(batch)
}

// Search æœç´¢
func (be *BleveEngine) Search(queryStr string, opts SearchOptions) (*SearchResult, error) {
 // æ„é€ æŸ¥è¯¢
 query := bleve.NewMatchQuery(queryStr)

 // æœç´¢è¯·æ±‚
 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.From = opts.From
 searchRequest.Size = opts.Size

 // é«˜äº®
 if opts.Highlight {
  searchRequest.Highlight = bleve.NewHighlight()
 }

 // æ‰§è¡Œæœç´¢
 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, fmt.Errorf("æœç´¢å¤±è´¥: %w", err)
 }

 // è½¬æ¢ç»“æœ
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Took:  int64(searchResult.Took.Milliseconds()),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  doc := Document{
   ID:    hit.ID,
   Score: hit.Score,
  }

  // è·å–å®Œæ•´æ–‡æ¡£
  if err := be.index.Document(hit.ID).(*Document); err == nil {
   result.Hits = append(result.Hits, doc)
  }
 }

 return result, nil
}

// FuzzySearch æ¨¡ç³Šæœç´¢
func (be *BleveEngine) FuzzySearch(queryStr string, fuzziness int) (*SearchResult, error) {
 // æ¨¡ç³ŠæŸ¥è¯¢
 query := bleve.NewFuzzyQuery(queryStr)
 query.Fuzziness = fuzziness // ç¼–è¾‘è·ç¦»

 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.Size = 10

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // è½¬æ¢ç»“æœ
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Took:  int64(searchResult.Took.Milliseconds()),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}

// Delete åˆ é™¤æ–‡æ¡£
func (be *BleveEngine) Delete(id string) error {
 return be.index.Delete(id)
}

// Close å…³é—­ç´¢å¼•
func (be *BleveEngine) Close() error {
 return be.index.Close()
}
```

### 3.2 Bleveé«˜çº§æŸ¥è¯¢

```go
package bleve

import (
 "github.com/blevesearch/bleve/v2"
 "github.com/blevesearch/bleve/v2/search/query"
)

// BooleanSearch å¸ƒå°”æŸ¥è¯¢
func (be *BleveEngine) BooleanSearch(must, should, mustNot []string) (*SearchResult, error) {
 // æ„é€ å¸ƒå°”æŸ¥è¯¢
 boolQuery := bleve.NewBooleanQuery()

 // MUST (AND)
 for _, term := range must {
  q := bleve.NewMatchQuery(term)
  boolQuery.AddMust(q)
 }

 // SHOULD (OR)
 for _, term := range should {
  q := bleve.NewMatchQuery(term)
  boolQuery.AddShould(q)
 }

 // MUST NOT
 for _, term := range mustNot {
  q := bleve.NewMatchQuery(term)
  boolQuery.AddMustNot(q)
 }

 searchRequest := bleve.NewSearchRequest(boolQuery)
 searchRequest.Size = 20

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // è½¬æ¢ç»“æœ
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Took:  int64(searchResult.Took.Milliseconds()),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}

// PrefixSearch å‰ç¼€æœç´¢
func (be *BleveEngine) PrefixSearch(prefix string) (*SearchResult, error) {
 query := bleve.NewPrefixQuery(prefix)

 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.Size = 10

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // è½¬æ¢ç»“æœ
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}

// WildcardSearch é€šé…ç¬¦æœç´¢
func (be *BleveEngine) WildcardSearch(pattern string) (*SearchResult, error) {
 // *: åŒ¹é…ä»»æ„å­—ç¬¦
 // ?: åŒ¹é…å•ä¸ªå­—ç¬¦
 query := bleve.NewWildcardQuery(pattern)

 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.Size = 10

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // è½¬æ¢ç»“æœ
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}

// RangeSearch èŒƒå›´æœç´¢ (æ•°å€¼/æ—¥æœŸ)
func (be *BleveEngine) RangeSearch(field string, min, max interface{}) (*SearchResult, error) {
 var query query.Query

 // æ ¹æ®ç±»å‹æ„é€ æŸ¥è¯¢
 switch min.(type) {
 case int, int64, float64:
  query = bleve.NewNumericRangeQuery(&min, &max)
 case string:
  minStr := min.(string)
  maxStr := max.(string)
  query = bleve.NewTermRangeQuery(minStr, maxStr)
 default:
  return nil, fmt.Errorf("ä¸æ”¯æŒçš„ç±»å‹")
 }

 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.Size = 10

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // è½¬æ¢ç»“æœ
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}
```

---

## 4. Elasticsearché›†æˆ

### 4.1 Elasticsearchå®¢æˆ·ç«¯

```go
package elasticsearch

import (
 "bytes"
 "context"
 "encoding/json"
 "fmt"
 "io"
 "log"

 "github.com/elastic/go-elasticsearch/v8"
 "github.com/elastic/go-elasticsearch/v8/esapi"
)

// Elasticsearch - åˆ†å¸ƒå¼æœç´¢å¼•æ“
/*
Elasticsearchç‰¹ç‚¹:
  âœ… åˆ†å¸ƒå¼æ¶æ„
  âœ… æ°´å¹³æ‰©å±•
  âœ… å®æ—¶æœç´¢
  âœ… RESTful API
  âœ… ä¸°å¯Œçš„èšåˆåŠŸèƒ½

é€‚ç”¨åœºæ™¯:
  â€¢ å¤§è§„æ¨¡æ•°æ® (>100ä¸‡æ–‡æ¡£)
  â€¢ å¤æ‚æŸ¥è¯¢
  â€¢ æ—¥å¿—åˆ†æ
  â€¢ å®æ—¶åˆ†æ
*/

// ElasticsearchEngine Elasticsearchå¼•æ“
type ElasticsearchEngine struct {
 client *elasticsearch.Client
 index  string
}

// NewElasticsearchEngine åˆ›å»ºESå¼•æ“
func NewElasticsearchEngine(addresses []string, indexName string) (*ElasticsearchEngine, error) {
 cfg := elasticsearch.Config{
  Addresses: addresses,
 }

 client, err := elasticsearch.NewClient(cfg)
 if err != nil {
  return nil, fmt.Errorf("åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: %w", err)
 }

 // æ£€æŸ¥è¿æ¥
 res, err := client.Info()
 if err != nil {
  return nil, fmt.Errorf("è¿æ¥å¤±è´¥: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return nil, fmt.Errorf("è¿æ¥é”™è¯¯: %s", res.String())
 }

 log.Printf("âœ… è¿æ¥åˆ°Elasticsearch")

 engine := &ElasticsearchEngine{
  client: client,
  index:  indexName,
 }

 // åˆ›å»ºç´¢å¼•
 if err := engine.createIndex(); err != nil {
  log.Printf("âš ï¸  åˆ›å»ºç´¢å¼•è­¦å‘Š: %v", err)
 }

 return engine, nil
}

// createIndex åˆ›å»ºç´¢å¼•
func (es *ElasticsearchEngine) createIndex() error {
 // ç´¢å¼•æ˜ å°„
 mapping := `{
  "mappings": {
   "properties": {
    "title": {
     "type": "text",
     "analyzer": "standard",
     "fields": {
      "keyword": {
       "type": "keyword"
      }
     }
    },
    "content": {
     "type": "text",
     "analyzer": "standard"
    },
    "timestamp": {
     "type": "date"
    }
   }
  },
  "settings": {
   "number_of_shards": 1,
   "number_of_replicas": 0
  }
 }`

 res, err := es.client.Indices.Create(
  es.index,
  es.client.Indices.Create.WithBody(bytes.NewReader([]byte(mapping))),
 )
 if err != nil {
  return err
 }
 defer res.Body.Close()

 if res.IsError() {
  // ç´¢å¼•å·²å­˜åœ¨æ˜¯æ­£å¸¸æƒ…å†µ
  if res.StatusCode != 400 {
   return fmt.Errorf("åˆ›å»ºç´¢å¼•å¤±è´¥: %s", res.String())
  }
 }

 return nil
}

// IndexDocument ç´¢å¼•æ–‡æ¡£
func (es *ElasticsearchEngine) IndexDocument(doc Document) error {
 // åºåˆ—åŒ–æ–‡æ¡£
 docJSON, err := json.Marshal(doc)
 if err != nil {
  return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
 }

 // ç´¢å¼•æ–‡æ¡£
 req := esapi.IndexRequest{
  Index:      es.index,
  DocumentID: doc.ID,
  Body:       bytes.NewReader(docJSON),
  Refresh:    "true", // ç«‹å³åˆ·æ–° (ç”Ÿäº§ç¯å¢ƒåº”è®¾ä¸ºfalse)
 }

 res, err := req.Do(context.Background(), es.client)
 if err != nil {
  return fmt.Errorf("ç´¢å¼•å¤±è´¥: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return fmt.Errorf("ç´¢å¼•é”™è¯¯: %s", res.String())
 }

 return nil
}

// BatchIndex æ‰¹é‡ç´¢å¼•
func (es *ElasticsearchEngine) BatchIndex(docs []Document) error {
 var buf bytes.Buffer

 for _, doc := range docs {
  // æ‰¹é‡æ“ä½œå…ƒæ•°æ®
  meta := map[string]interface{}{
   "index": map[string]interface{}{
    "_id": doc.ID,
   },
  }
  metaJSON, _ := json.Marshal(meta)
  buf.Write(metaJSON)
  buf.WriteByte('\n')

  // æ–‡æ¡£æ•°æ®
  docJSON, _ := json.Marshal(doc)
  buf.Write(docJSON)
  buf.WriteByte('\n')
 }

 // æ‰¹é‡è¯·æ±‚
 res, err := es.client.Bulk(
  bytes.NewReader(buf.Bytes()),
  es.client.Bulk.WithIndex(es.index),
  es.client.Bulk.WithRefresh("true"),
 )
 if err != nil {
  return fmt.Errorf("æ‰¹é‡ç´¢å¼•å¤±è´¥: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return fmt.Errorf("æ‰¹é‡ç´¢å¼•é”™è¯¯: %s", res.String())
 }

 return nil
}

// Search æœç´¢
func (es *ElasticsearchEngine) Search(queryStr string, opts SearchOptions) (*SearchResult, error) {
 // æ„é€ æŸ¥è¯¢
 query := map[string]interface{}{
  "query": map[string]interface{}{
   "multi_match": map[string]interface{}{
    "query":  queryStr,
    "fields": []string{"title^2", "content"}, // titleæƒé‡ä¸º2
   },
  },
  "from": opts.From,
  "size": opts.Size,
 }

 // é«˜äº®
 if opts.Highlight {
  query["highlight"] = map[string]interface{}{
   "fields": map[string]interface{}{
    "title":   map[string]interface{}{},
    "content": map[string]interface{}{},
   },
  }
 }

 queryJSON, _ := json.Marshal(query)

 // æ‰§è¡Œæœç´¢
 res, err := es.client.Search(
  es.client.Search.WithIndex(es.index),
  es.client.Search.WithBody(bytes.NewReader(queryJSON)),
 )
 if err != nil {
  return nil, fmt.Errorf("æœç´¢å¤±è´¥: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return nil, fmt.Errorf("æœç´¢é”™è¯¯: %s", res.String())
 }

 // è§£æå“åº”
 var esResult struct {
  Took int64 `json:"took"`
  Hits struct {
   Total struct {
    Value int64 `json:"value"`
   } `json:"total"`
   Hits []struct {
    ID     string                 `json:"_id"`
    Score  float64                `json:"_score"`
    Source Document               `json:"_source"`
    Highlight map[string][]string `json:"highlight,omitempty"`
   } `json:"hits"`
  } `json:"hits"`
 }

 body, _ := io.ReadAll(res.Body)
 if err := json.Unmarshal(body, &esResult); err != nil {
  return nil, fmt.Errorf("è§£æå“åº”å¤±è´¥: %w", err)
 }

 // è½¬æ¢ç»“æœ
 result := &SearchResult{
  Total: esResult.Hits.Total.Value,
  Took:  esResult.Took,
  Hits:  make([]Document, 0, len(esResult.Hits.Hits)),
 }

 for _, hit := range esResult.Hits.Hits {
  doc := hit.Source
  doc.ID = hit.ID
  doc.Score = hit.Score

  result.Hits = append(result.Hits, doc)
 }

 return result, nil
}

// Delete åˆ é™¤æ–‡æ¡£
func (es *ElasticsearchEngine) Delete(id string) error {
 req := esapi.DeleteRequest{
  Index:      es.index,
  DocumentID: id,
  Refresh:    "true",
 }

 res, err := req.Do(context.Background(), es.client)
 if err != nil {
  return fmt.Errorf("åˆ é™¤å¤±è´¥: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return fmt.Errorf("åˆ é™¤é”™è¯¯: %s", res.String())
 }

 return nil
}

// Close å…³é—­å®¢æˆ·ç«¯
func (es *ElasticsearchEngine) Close() error {
 // Elasticsearchå®¢æˆ·ç«¯æ— éœ€æ˜¾å¼å…³é—­
 return nil
}
```

### 4.2 Elasticsearché«˜çº§æŸ¥è¯¢

```go
package elasticsearch

import (
 "bytes"
 "context"
 "encoding/json"
 "io"
)

// BoolQuery å¸ƒå°”æŸ¥è¯¢
func (es *ElasticsearchEngine) BoolQuery(must, should, mustNot []string) (*SearchResult, error) {
 // æ„é€ å¸ƒå°”æŸ¥è¯¢
 mustQueries := make([]interface{}, 0, len(must))
 for _, term := range must {
  mustQueries = append(mustQueries, map[string]interface{}{
   "match": map[string]interface{}{
    "content": term,
   },
  })
 }

 shouldQueries := make([]interface{}, 0, len(should))
 for _, term := range should {
  shouldQueries = append(shouldQueries, map[string]interface{}{
   "match": map[string]interface{}{
    "content": term,
   },
  })
 }

 mustNotQueries := make([]interface{}, 0, len(mustNot))
 for _, term := range mustNot {
  mustNotQueries = append(mustNotQueries, map[string]interface{}{
   "match": map[string]interface{}{
    "content": term,
   },
  })
 }

 query := map[string]interface{}{
  "query": map[string]interface{}{
   "bool": map[string]interface{}{
    "must":     mustQueries,
    "should":   shouldQueries,
    "must_not": mustNotQueries,
   },
  },
 }

 queryJSON, _ := json.Marshal(query)

 res, err := es.client.Search(
  es.client.Search.WithIndex(es.index),
  es.client.Search.WithBody(bytes.NewReader(queryJSON)),
 )
 if err != nil {
  return nil, err
 }
 defer res.Body.Close()

 // è§£æå“åº”
 var esResult struct {
  Took int64 `json:"took"`
  Hits struct {
   Total struct {
    Value int64 `json:"value"`
   } `json:"total"`
   Hits []struct {
    ID     string   `json:"_id"`
    Score  float64  `json:"_score"`
    Source Document `json:"_source"`
   } `json:"hits"`
  } `json:"hits"`
 }

 body, _ := io.ReadAll(res.Body)
 json.Unmarshal(body, &esResult)

 result := &SearchResult{
  Total: esResult.Hits.Total.Value,
  Took:  esResult.Took,
  Hits:  make([]Document, 0, len(esResult.Hits.Hits)),
 }

 for _, hit := range esResult.Hits.Hits {
  doc := hit.Source
  doc.ID = hit.ID
  doc.Score = hit.Score
  result.Hits = append(result.Hits, doc)
 }

 return result, nil
}

// Aggregation èšåˆæŸ¥è¯¢
func (es *ElasticsearchEngine) Aggregation(field string) (map[string]int64, error) {
 query := map[string]interface{}{
  "aggs": map[string]interface{}{
   "field_agg": map[string]interface{}{
    "terms": map[string]interface{}{
     "field": field,
     "size":  10,
    },
   },
  },
  "size": 0, // ä¸è¿”å›æ–‡æ¡£ï¼Œåªè¿”å›èšåˆç»“æœ
 }

 queryJSON, _ := json.Marshal(query)

 res, err := es.client.Search(
  es.client.Search.WithIndex(es.index),
  es.client.Search.WithBody(bytes.NewReader(queryJSON)),
 )
 if err != nil {
  return nil, err
 }
 defer res.Body.Close()

 // è§£æèšåˆç»“æœ
 var esResult struct {
  Aggregations struct {
   FieldAgg struct {
    Buckets []struct {
     Key      string `json:"key"`
     DocCount int64  `json:"doc_count"`
    } `json:"buckets"`
   } `json:"field_agg"`
  } `json:"aggregations"`
 }

 body, _ := io.ReadAll(res.Body)
 json.Unmarshal(body, &esResult)

 result := make(map[string]int64)
 for _, bucket := range esResult.Aggregations.FieldAgg.Buckets {
  result[bucket.Key] = bucket.DocCount
 }

 return result, nil
}
```

---

## 5. ä¸­æ–‡åˆ†è¯

### 5.1 jiebaä¸­æ–‡åˆ†è¯

```go
package tokenizer

import (
 "github.com/yanyiwu/gojieba"
)

// ä¸­æ–‡åˆ†è¯
/*
ä¸­æ–‡åˆ†è¯æŒ‘æˆ˜:
  â€¢ æ— æ˜æ˜¾è¯ç•Œ (æ— ç©ºæ ¼)
  â€¢ æ­§ä¹‰é—®é¢˜ ("å—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥")
  â€¢ æ–°è¯è¯†åˆ«

ä¸»æµä¸­æ–‡åˆ†è¯å™¨:
  â€¢ jieba (ç»“å·´åˆ†è¯) âœ…
  â€¢ sego
  â€¢ gse
  â€¢ mmseg

åˆ†è¯ç®—æ³•:
  â€¢ åŸºäºè¯å…¸: æœ€å¤§åŒ¹é…
  â€¢ åŸºäºç»Ÿè®¡: HMM, CRF
  â€¢ åŸºäºæ·±åº¦å­¦ä¹ : LSTM, BERT
*/

// JiebaTokenizer jiebaåˆ†è¯å™¨
type JiebaTokenizer struct {
 jieba *gojieba.Jieba
}

// NewJiebaTokenizer åˆ›å»ºjiebaåˆ†è¯å™¨
func NewJiebaTokenizer(dictPath string) *JiebaTokenizer {
 // åŠ è½½è¯å…¸
 jieba := gojieba.NewJieba(
  dictPath+"/jieba.dict.utf8",
  dictPath+"/hmm_model.utf8",
  dictPath+"/user.dict.utf8",
 )

 return &JiebaTokenizer{
  jieba: jieba,
 }
}

// Tokenize åˆ†è¯
func (jt *JiebaTokenizer) Tokenize(text string) []string {
 // ç²¾ç¡®æ¨¡å¼
 return jt.jieba.Cut(text, false)
}

// TokenizeWithMode æŒ‡å®šæ¨¡å¼åˆ†è¯
func (jt *JiebaTokenizer) TokenizeWithMode(text string, mode TokenizeMode) []string {
 switch mode {
 case ModeSearch:
  // æœç´¢å¼•æ“æ¨¡å¼ (ç»†ç²’åº¦)
  return jt.jieba.CutForSearch(text, true)
 case ModeFull:
  // å…¨æ¨¡å¼ (æ‰¾å‡ºæ‰€æœ‰å¯èƒ½çš„è¯)
  return jt.jieba.CutAll(text)
 default:
  // ç²¾ç¡®æ¨¡å¼
  return jt.jieba.Cut(text, false)
 }
}

// ExtractKeywords æå–å…³é”®è¯
func (jt *JiebaTokenizer) ExtractKeywords(text string, topN int) []string {
 // TF-IDFæå–å…³é”®è¯
 keywords := jt.jieba.ExtractWithWeight(text, topN)

 result := make([]string, 0, len(keywords))
 for _, kw := range keywords {
  result = append(result, kw.Word)
 }

 return result
}

// AddWord æ·»åŠ è‡ªå®šä¹‰è¯
func (jt *JiebaTokenizer) AddWord(word string, freq int, tag string) {
 jt.jieba.AddWord(word)
}

// Close å…³é—­åˆ†è¯å™¨
func (jt *JiebaTokenizer) Close() {
 jt.jieba.Free()
}

// TokenizeMode åˆ†è¯æ¨¡å¼
type TokenizeMode int

const (
 ModeExact  TokenizeMode = iota // ç²¾ç¡®æ¨¡å¼
 ModeFull                       // å…¨æ¨¡å¼
 ModeSearch                     // æœç´¢å¼•æ“æ¨¡å¼
)
```

---

## 6. ç›¸å…³æ€§æ’åº

### 6.1 TF-IDFç®—æ³•

```go
package ranking

import (
 "math"
)

// TF-IDFç›¸å…³æ€§ç®—æ³•
/*
TF-IDF (Term Frequency - Inverse Document Frequency)

TF (è¯é¢‘):
  TF(t, d) = (è¯tåœ¨æ–‡æ¡£dä¸­å‡ºç°çš„æ¬¡æ•°) / (æ–‡æ¡£dçš„æ€»è¯æ•°)

IDF (é€†æ–‡æ¡£é¢‘ç‡):
  IDF(t) = log(æ–‡æ¡£æ€»æ•° / åŒ…å«è¯tçš„æ–‡æ¡£æ•°)

TF-IDF:
  TF-IDF(t, d) = TF(t, d) Ã— IDF(t)

æ„ä¹‰:
  â€¢ è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°è¶Šå¤šï¼Œæƒé‡è¶Šé«˜ (TF)
  â€¢ è¯åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­è¶Šç½•è§ï¼Œæƒé‡è¶Šé«˜ (IDF)
*/

// TFIDFRanker TF-IDFæ’åºå™¨
type TFIDFRanker struct {
 docCount int                        // æ–‡æ¡£æ€»æ•°
 termDF   map[string]int             // è¯çš„æ–‡æ¡£é¢‘ç‡
 termTF   map[string]map[string]int  // è¯åœ¨å„æ–‡æ¡£ä¸­çš„è¯é¢‘
}

// NewTFIDFRanker åˆ›å»ºTF-IDFæ’åºå™¨
func NewTFIDFRanker() *TFIDFRanker {
 return &TFIDFRanker{
  termDF: make(map[string]int),
  termTF: make(map[string]map[string]int),
 }
}

// AddDocument æ·»åŠ æ–‡æ¡£
func (tr *TFIDFRanker) AddDocument(docID string, tokens []string) {
 tr.docCount++

 // ç»Ÿè®¡è¯é¢‘
 termFreq := make(map[string]int)
 for _, token := range tokens {
  termFreq[token]++
 }

 // æ›´æ–°TF
 for term, freq := range termFreq {
  if tr.termTF[term] == nil {
   tr.termTF[term] = make(map[string]int)
  }
  tr.termTF[term][docID] = freq
 }

 // æ›´æ–°DF (æ¯ä¸ªæ–‡æ¡£åªè®¡æ•°ä¸€æ¬¡)
 seen := make(map[string]bool)
 for _, token := range tokens {
  if !seen[token] {
   tr.termDF[token]++
   seen[token] = true
  }
 }
}

// CalculateTFIDF è®¡ç®—TF-IDF
func (tr *TFIDFRanker) CalculateTFIDF(term, docID string, docLength int) float64 {
 // TF
 tf := float64(tr.termTF[term][docID]) / float64(docLength)

 // IDF
 idf := math.Log(float64(tr.docCount) / float64(tr.termDF[term]+1))

 return tf * idf
}

// ScoreDocument è®¡ç®—æ–‡æ¡£å¾—åˆ†
func (tr *TFIDFRanker) ScoreDocument(queryTerms []string, docID string, docLength int) float64 {
 score := 0.0

 for _, term := range queryTerms {
  score += tr.CalculateTFIDF(term, docID, docLength)
 }

 return score
}
```

### 6.2 BM25ç®—æ³•

```go
package ranking

import (
 "math"
)

// BM25ç›¸å…³æ€§ç®—æ³•
/*
BM25 (Best Matching 25) - Elasticsearché»˜è®¤ç®—æ³•

BM25 = IDF(q) Ã— (TF(q,d) Ã— (k1 + 1)) / (TF(q,d) + k1 Ã— (1 - b + b Ã— (|d| / avgdl)))

å‚æ•°:
  â€¢ k1: è¯é¢‘é¥±å’Œåº¦å‚æ•° (1.2-2.0)
  â€¢ b:  é•¿åº¦å½’ä¸€åŒ–å‚æ•° (0.75)
  â€¢ |d|: æ–‡æ¡£é•¿åº¦
  â€¢ avgdl: å¹³å‡æ–‡æ¡£é•¿åº¦

ç‰¹ç‚¹:
  âœ… è¯é¢‘é¥±å’Œ (é«˜é¢‘è¯å½±å“é€’å‡)
  âœ… é•¿åº¦å½’ä¸€åŒ– (é¿å…é•¿æ–‡æ¡£ä¼˜åŠ¿)
  âœ… ç›¸æ¯”TF-IDFæ›´å‡†ç¡®
*/

// BM25Ranker BM25æ’åºå™¨
type BM25Ranker struct {
 docCount    int                       // æ–‡æ¡£æ€»æ•°
 avgDocLen   float64                   // å¹³å‡æ–‡æ¡£é•¿åº¦
 docLengths  map[string]int            // æ–‡æ¡£é•¿åº¦
 termDF      map[string]int            // è¯çš„æ–‡æ¡£é¢‘ç‡
 termTF      map[string]map[string]int // è¯åœ¨å„æ–‡æ¡£ä¸­çš„è¯é¢‘
 k1          float64                   // è¯é¢‘é¥±å’Œå‚æ•°
 b           float64                   // é•¿åº¦å½’ä¸€åŒ–å‚æ•°
}

// NewBM25Ranker åˆ›å»ºBM25æ’åºå™¨
func NewBM25Ranker(k1, b float64) *BM25Ranker {
 return &BM25Ranker{
  docLengths: make(map[string]int),
  termDF:     make(map[string]int),
  termTF:     make(map[string]map[string]int),
  k1:         k1,
  b:          b,
 }
}

// AddDocument æ·»åŠ æ–‡æ¡£
func (br *BM25Ranker) AddDocument(docID string, tokens []string) {
 br.docCount++
 br.docLengths[docID] = len(tokens)

 // æ›´æ–°å¹³å‡æ–‡æ¡£é•¿åº¦
 totalLen := 0
 for _, length := range br.docLengths {
  totalLen += length
 }
 br.avgDocLen = float64(totalLen) / float64(br.docCount)

 // ç»Ÿè®¡è¯é¢‘
 termFreq := make(map[string]int)
 for _, token := range tokens {
  termFreq[token]++
 }

 // æ›´æ–°TF
 for term, freq := range termFreq {
  if br.termTF[term] == nil {
   br.termTF[term] = make(map[string]int)
  }
  br.termTF[term][docID] = freq
 }

 // æ›´æ–°DF
 seen := make(map[string]bool)
 for _, token := range tokens {
  if !seen[token] {
   br.termDF[token]++
   seen[token] = true
  }
 }
}

// CalculateBM25 è®¡ç®—BM25å¾—åˆ†
func (br *BM25Ranker) CalculateBM25(term, docID string) float64 {
 // IDF
 idf := math.Log((float64(br.docCount) - float64(br.termDF[term]) + 0.5) / (float64(br.termDF[term]) + 0.5) + 1.0)

 // TF
 tf := float64(br.termTF[term][docID])

 // æ–‡æ¡£é•¿åº¦
 docLen := float64(br.docLengths[docID])

 // BM25å…¬å¼
 numerator := tf * (br.k1 + 1)
 denominator := tf + br.k1*(1-br.b+br.b*(docLen/br.avgDocLen))

 return idf * (numerator / denominator)
}

// ScoreDocument è®¡ç®—æ–‡æ¡£å¾—åˆ†
func (br *BM25Ranker) ScoreDocument(queryTerms []string, docID string) float64 {
 score := 0.0

 for _, term := range queryTerms {
  score += br.CalculateBM25(term, docID)
 }

 return score
}
```

---

## 7. å‘é‡æ£€ç´¢

### 7.1 å‘é‡æœç´¢

```go
package vector

import (
 "math"
)

// å‘é‡æ£€ç´¢ (Vector Search)
/*
å‘é‡æ£€ç´¢ = è¯­ä¹‰æœç´¢

ä¼ ç»Ÿå…³é”®è¯æœç´¢:
  æŸ¥è¯¢: "Go programming"
  åŒ¹é…: åŒ…å«"Go"å’Œ"programming"çš„æ–‡æ¡£

å‘é‡æœç´¢:
  æŸ¥è¯¢: "Go programming"
  å‘é‡åŒ–: [0.1, 0.3, 0.5, ...]
  åŒ¹é…: å‘é‡ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æ¡£

åº”ç”¨:
  â€¢ è¯­ä¹‰æœç´¢
  â€¢ æ¨èç³»ç»Ÿ
  â€¢ å›¾ç‰‡æœç´¢
  â€¢ é—®ç­”ç³»ç»Ÿ

ç›¸ä¼¼åº¦ç®—æ³•:
  â€¢ ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity) âœ…
  â€¢ æ¬§æ°è·ç¦» (Euclidean Distance)
  â€¢ æ›¼å“ˆé¡¿è·ç¦» (Manhattan Distance)
*/

// VectorIndex å‘é‡ç´¢å¼•
type VectorIndex struct {
 vectors map[string][]float64 // docID -> vector
}

// NewVectorIndex åˆ›å»ºå‘é‡ç´¢å¼•
func NewVectorIndex() *VectorIndex {
 return &VectorIndex{
  vectors: make(map[string][]float64),
 }
}

// AddVector æ·»åŠ å‘é‡
func (vi *VectorIndex) AddVector(docID string, vector []float64) {
 vi.vectors[docID] = vector
}

// Search å‘é‡æœç´¢
func (vi *VectorIndex) Search(queryVector []float64, topK int) []SearchResultItem {
 // è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
 similarities := make([]SearchResultItem, 0, len(vi.vectors))

 for docID, docVector := range vi.vectors {
  similarity := cosineSimilarity(queryVector, docVector)
  similarities = append(similarities, SearchResultItem{
   DocID: docID,
   Score: similarity,
  })
 }

 // æ’åº (ç›¸ä¼¼åº¦é™åº)
 sort.Slice(similarities, func(i, j int) bool {
  return similarities[i].Score > similarities[j].Score
 })

 // è¿”å›Top K
 if topK > len(similarities) {
  topK = len(similarities)
 }

 return similarities[:topK]
}

// SearchResultItem æœç´¢ç»“æœé¡¹
type SearchResultItem struct {
 DocID string
 Score float64
}

// cosineSimilarity ä½™å¼¦ç›¸ä¼¼åº¦
func cosineSimilarity(a, b []float64) float64 {
 if len(a) != len(b) {
  return 0
 }

 dotProduct := 0.0
 normA := 0.0
 normB := 0.0

 for i := 0; i < len(a); i++ {
  dotProduct += a[i] * b[i]
  normA += a[i] * a[i]
  normB += b[i] * b[i]
 }

 if normA == 0 || normB == 0 {
  return 0
 }

 return dotProduct / (math.Sqrt(normA) * math.Sqrt(normB))
}

// euclideanDistance æ¬§æ°è·ç¦»
func euclideanDistance(a, b []float64) float64 {
 if len(a) != len(b) {
  return math.Inf(1)
 }

 sum := 0.0
 for i := 0; i < len(a); i++ {
  diff := a[i] - b[i]
  sum += diff * diff
 }

 return math.Sqrt(sum)
}
```

---

## 8. æœç´¢å»ºè®®

### 8.1 è‡ªåŠ¨è¡¥å…¨

```go
package suggest

import (
 "strings"
)

// Trieæ ‘è‡ªåŠ¨è¡¥å…¨
/*
Trieæ ‘ (å‰ç¼€æ ‘):
  â€¢ æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªå­—ç¬¦
  â€¢ ä»æ ¹åˆ°å¶çš„è·¯å¾„ä»£è¡¨ä¸€ä¸ªè¯
  â€¢ æ”¯æŒå‰ç¼€æŸ¥è¯¢

ç¤ºä¾‹:
       root
       /  \
      c    g
     / \    \
    a   o    o
   /     \
  t      d
  (cat) (code) (go)

åº”ç”¨:
  â€¢ è‡ªåŠ¨è¡¥å…¨
  â€¢ æ‹¼å†™æ£€æŸ¥
  â€¢ IPè·¯ç”±
*/

// Trie Trieæ ‘
type Trie struct {
 root *TrieNode
}

// TrieNode TrieèŠ‚ç‚¹
type TrieNode struct {
 children map[rune]*TrieNode
 isEnd    bool
 word     string
 freq     int // è¯é¢‘ (ç”¨äºæ’åº)
}

// NewTrie åˆ›å»ºTrieæ ‘
func NewTrie() *Trie {
 return &Trie{
  root: &TrieNode{
   children: make(map[rune]*TrieNode),
  },
 }
}

// Insert æ’å…¥è¯
func (t *Trie) Insert(word string, freq int) {
 node := t.root

 for _, ch := range word {
  if node.children[ch] == nil {
   node.children[ch] = &TrieNode{
    children: make(map[rune]*TrieNode),
   }
  }
  node = node.children[ch]
 }

 node.isEnd = true
 node.word = word
 node.freq = freq
}

// SearchPrefix å‰ç¼€æœç´¢
func (t *Trie) SearchPrefix(prefix string) []string {
 node := t.root

 // æ‰¾åˆ°å‰ç¼€èŠ‚ç‚¹
 for _, ch := range prefix {
  if node.children[ch] == nil {
   return []string{} // å‰ç¼€ä¸å­˜åœ¨
  }
  node = node.children[ch]
 }

 // æ”¶é›†æ‰€æœ‰ä»¥è¯¥å‰ç¼€å¼€å§‹çš„è¯
 var results []string
 t.collectWords(node, &results)

 return results
}

// collectWords æ”¶é›†æ‰€æœ‰è¯
func (t *Trie) collectWords(node *TrieNode, results *[]string) {
 if node.isEnd {
  *results = append(*results, node.word)
 }

 for _, child := range node.children {
  t.collectWords(child, results)
 }
}

// AutoComplete è‡ªåŠ¨è¡¥å…¨ (Top K)
func (t *Trie) AutoComplete(prefix string, topK int) []Suggestion {
 node := t.root

 // æ‰¾åˆ°å‰ç¼€èŠ‚ç‚¹
 for _, ch := range prefix {
  if node.children[ch] == nil {
   return []Suggestion{}
  }
  node = node.children[ch]
 }

 // æ”¶é›†æ‰€æœ‰è¯åŠå…¶é¢‘ç‡
 var suggestions []Suggestion
 t.collectSuggestions(node, &suggestions)

 // æŒ‰é¢‘ç‡æ’åº
 sort.Slice(suggestions, func(i, j int) bool {
  return suggestions[i].Freq > suggestions[j].Freq
 })

 // è¿”å›Top K
 if topK > len(suggestions) {
  topK = len(suggestions)
 }

 return suggestions[:topK]
}

// collectSuggestions æ”¶é›†å»ºè®®
func (t *Trie) collectSuggestions(node *TrieNode, suggestions *[]Suggestion) {
 if node.isEnd {
  *suggestions = append(*suggestions, Suggestion{
   Word: node.word,
   Freq: node.freq,
  })
 }

 for _, child := range node.children {
  t.collectSuggestions(child, suggestions)
 }
}

// Suggestion æœç´¢å»ºè®®
type Suggestion struct {
 Word string
 Freq int
}
```

---

## 9. æ€§èƒ½ä¼˜åŒ–

### 9.1 ç´¢å¼•ä¼˜åŒ–

```go
package optimization

// ç´¢å¼•ä¼˜åŒ–æŠ€å·§

/*
1. æ‰¹é‡ç´¢å¼•
   âœ… ä½¿ç”¨Bulk API
   âœ… æ‰¹é‡å¤§å°: 1000-5000æ–‡æ¡£
   âŒ é¿å…å•æ¡ç´¢å¼•

2. åˆ·æ–°ç­–ç•¥
   âœ… ç”Ÿäº§: refresh_interval = 30s
   âŒ å¼€å‘: refresh = true (ç«‹å³åˆ·æ–°)

3. å‰¯æœ¬ç­–ç•¥
   âœ… ç´¢å¼•é˜¶æ®µ: 0å‰¯æœ¬
   âœ… æŸ¥è¯¢é˜¶æ®µ: 1+å‰¯æœ¬

4. åˆ†ç‰‡ç­–ç•¥
   â€¢ æ¯ä¸ªåˆ†ç‰‡: 20-50GB
   â€¢ åˆ†ç‰‡æ•° = ç´¢å¼•å¤§å° / 30GB

5. å­—æ®µä¼˜åŒ–
   âœ… ä¸éœ€è¦æœç´¢çš„å­—æ®µ: index: false
   âœ… ä¸éœ€è¦è¯„åˆ†çš„å­—æ®µ: norms: false
   âœ… ç²¾ç¡®åŒ¹é…å­—æ®µ: keywordç±»å‹

6. æŸ¥è¯¢ä¼˜åŒ–
   âœ… ä½¿ç”¨filter (ä¸è®¡ç®—è¯„åˆ†)
   âœ… ç¼“å­˜é¢‘ç¹æŸ¥è¯¢
   âœ… é™åˆ¶è¿”å›å­—æ®µ (_source: false)

7. ç¡¬ä»¶ä¼˜åŒ–
   âœ… SSDç¡¬ç›˜
   âœ… å……è¶³å†…å­˜ (ç´¢å¼•å¤§å°çš„2å€)
   âœ… å¤šæ ¸CPU
*/

// IndexOptimizer ç´¢å¼•ä¼˜åŒ–å™¨
type IndexOptimizer struct {
 batchSize     int
 refreshInterval string
 replicas      int
}

// OptimizeForBulkIndexing æ‰¹é‡ç´¢å¼•ä¼˜åŒ–
func (io *IndexOptimizer) OptimizeForBulkIndexing() map[string]interface{} {
 return map[string]interface{}{
  "index": map[string]interface{}{
   "refresh_interval": "-1",      // ç¦ç”¨è‡ªåŠ¨åˆ·æ–°
   "number_of_replicas": 0,       // 0å‰¯æœ¬
   "translog.durability": "async", // å¼‚æ­¥translog
  },
 }
}

// OptimizeForQuerying æŸ¥è¯¢ä¼˜åŒ–
func (io *IndexOptimizer) OptimizeForQuerying() map[string]interface{} {
 return map[string]interface{}{
  "index": map[string]interface{}{
   "refresh_interval": "30s", // 30ç§’åˆ·æ–°
   "number_of_replicas": 1,   // 1å‰¯æœ¬
  },
 }
}
```

---

## 10. ç”Ÿäº§éƒ¨ç½²

### 10.1 Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM golang:1.25-alpine AS builder

WORKDIR /app

COPY go.mod go.sum ./
RUN go mod download

COPY . .

RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o search-engine ./cmd/search

FROM alpine:latest

RUN apk --no-cache add ca-certificates

WORKDIR /root/

COPY --from=builder /app/search-engine .
COPY --from=builder /app/dict ./dict

EXPOSE 8080

CMD ["./search-engine"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  search-api:
    build: .
    ports:
      - "8080:8080"
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - xpack.security.enabled=false
    volumes:
      - es-data:/usr/share/elasticsearch/data
    restart: unless-stopped

  # Kibana (å¯è§†åŒ–)
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  es-data:
```

---

## æ€»ç»“

### âœ… æ ¸å¿ƒæˆæœ

1. **æœç´¢å¼•æ“åŸç†**
   - å€’æ’ç´¢å¼•
   - åˆ†è¯æŠ€æœ¯
   - ç›¸å…³æ€§æ’åº

2. **Bleveæœ¬åœ°æœç´¢**
   - çº¯Goå®ç°
   - åµŒå…¥å¼æœç´¢
   - æ¨¡ç³ŠæŸ¥è¯¢

3. **Elasticsearché›†æˆ**
   - åˆ†å¸ƒå¼æœç´¢
   - æ‰¹é‡ç´¢å¼•
   - é«˜çº§æŸ¥è¯¢

4. **ä¸­æ–‡åˆ†è¯**
   - jiebaåˆ†è¯
   - å…³é”®è¯æå–
   - è‡ªå®šä¹‰è¯å…¸

5. **ç›¸å…³æ€§ç®—æ³•**
   - TF-IDF
   - BM25
   - å‘é‡æ£€ç´¢

6. **æœç´¢å»ºè®®**
   - Trieæ ‘
   - è‡ªåŠ¨è¡¥å…¨
   - æ‹¼å†™çº é”™

### ğŸŒŸ Go 1.25.3åº”ç”¨

| ç‰¹æ€§ | åº”ç”¨ |
|------|------|
| Goroutine | å¹¶å‘ç´¢å¼• |
| Channel | æ‰¹é‡å¤„ç†ç®¡é“ |
| Map | å€’æ’ç´¢å¼• |
| Interface | æœç´¢å¼•æ“æŠ½è±¡ |
| Generic | ç±»å‹å®‰å…¨çš„æ•°æ®ç»“æ„ |

### ğŸ“Š æ¶æ„äº®ç‚¹

- **é«˜æ€§èƒ½**: å€’æ’ç´¢å¼•O(1)æŸ¥æ‰¾
- **å¯æ‰©å±•**: åˆ†å¸ƒå¼æ¶æ„
- **è¯­ä¹‰æœç´¢**: å‘é‡æ£€ç´¢
- **ä¸­æ–‡å‹å¥½**: jiebaåˆ†è¯

### ğŸ¯ é€‚ç”¨åœºæ™¯

- ç”µå•†æœç´¢
- æ—¥å¿—åˆ†æ
- çŸ¥è¯†åº“æ£€ç´¢
- ä¼ä¸šæœç´¢

---

## ä¸‹ä¸€æ­¥å­¦ä¹ 

- [ ] Luceneå†…æ ¸åŸç†
- [ ] å‘é‡æ•°æ®åº“ (Milvus, Weaviate)
- [ ] BERTè¯­ä¹‰æœç´¢
- [ ] åˆ†å¸ƒå¼æœç´¢æ¶æ„

---

> **å»¶ä¼¸é˜…è¯»**:  
> â€¢ [Elasticsearchå®˜æ–¹æ–‡æ¡£](https://www.elastic.co/guide/)  
> â€¢ [Bleveæ–‡æ¡£](http://blevesearch.com/)  
> â€¢ [ä¿¡æ¯æ£€ç´¢å¯¼è®º](https://nlp.stanford.edu/IR-book/)  
> â€¢ [æœç´¢å¼•æ“åŸç†](https://www.amazon.com/Introduction-Information-Retrieval-Christopher-Manning/dp/0521865719)
