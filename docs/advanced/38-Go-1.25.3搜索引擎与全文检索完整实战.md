# Go 1.25.3搜索引擎与全文检索完整实战

> **Elasticsearch·Bleve·倒排索引·分词器·相关性排序**  
> 📍 **难度**：⭐⭐⭐⭐⭐ (高级)  
> 🎯 **目标**：构建生产级搜索引擎  
> ⏱ **预计时间**：6-8小时  
> 🔧 **技术栈**：Elasticsearch·Bleve·TF-IDF·BM25·中文分词·向量检索

---


## 📋 目录


- [目录](#目录)
- [1. 搜索引擎原理](#1.-搜索引擎原理)
  - [1.1 搜索引擎架构](#11-搜索引擎架构)
- [2. 倒排索引](#2.-倒排索引)
  - [2.1 倒排索引原理](#21-倒排索引原理)
- [3. Bleve本地搜索](#3.-bleve本地搜索)
  - [3.1 Bleve全文搜索](#31-bleve全文搜索)
  - [3.2 Bleve高级查询](#32-bleve高级查询)
- [4. Elasticsearch集成](#4.-elasticsearch集成)
  - [4.1 Elasticsearch客户端](#41-elasticsearch客户端)
  - [4.2 Elasticsearch高级查询](#42-elasticsearch高级查询)
- [5. 中文分词](#5.-中文分词)
  - [5.1 jieba中文分词](#51-jieba中文分词)
- [6. 相关性排序](#6.-相关性排序)
  - [6.1 TF-IDF算法](#61-tf-idf算法)
  - [6.2 BM25算法](#62-bm25算法)
- [7. 向量检索](#7.-向量检索)
  - [7.1 向量搜索](#71-向量搜索)
- [8. 搜索建议](#8.-搜索建议)
  - [8.1 自动补全](#81-自动补全)
- [9. 性能优化](#9.-性能优化)
  - [9.1 索引优化](#91-索引优化)
- [10. 生产部署](#10.-生产部署)
  - [10.1 Docker部署](#101-docker部署)
- [总结](#总结)
  - [✅ 核心成果](#核心成果)
  - [🌟 Go 1.25.3应用](#go-1.25.3应用)
  - [📊 架构亮点](#架构亮点)
  - [🎯 适用场景](#适用场景)
- [下一步学习](#下一步学习)

## 目录

- [Go 1.25.3搜索引擎与全文检索完整实战](#go-1.25.3搜索引擎与全文检索完整实战)
  - [目录](#目录)
  - [1. 搜索引擎原理](#1.-搜索引擎原理)
    - [1.1 搜索引擎架构](#11-搜索引擎架构)
  - [2. 倒排索引](#2.-倒排索引)
    - [2.1 倒排索引原理](#21-倒排索引原理)
  - [3. Bleve本地搜索](#3.-bleve本地搜索)
    - [3.1 Bleve全文搜索](#31-bleve全文搜索)
    - [3.2 Bleve高级查询](#32-bleve高级查询)
  - [4. Elasticsearch集成](#4.-elasticsearch集成)
    - [4.1 Elasticsearch客户端](#41-elasticsearch客户端)
    - [4.2 Elasticsearch高级查询](#42-elasticsearch高级查询)
  - [5. 中文分词](#5.-中文分词)
    - [5.1 jieba中文分词](#51-jieba中文分词)
  - [6. 相关性排序](#6.-相关性排序)
    - [6.1 TF-IDF算法](#61-tf-idf算法)
    - [6.2 BM25算法](#62-bm25算法)
  - [7. 向量检索](#7.-向量检索)
    - [7.1 向量搜索](#71-向量搜索)
  - [8. 搜索建议](#8.-搜索建议)
    - [8.1 自动补全](#81-自动补全)
  - [9. 性能优化](#9.-性能优化)
    - [9.1 索引优化](#91-索引优化)
  - [10. 生产部署](#10.-生产部署)
    - [10.1 Docker部署](#101-docker部署)
  - [总结](#总结)
    - [✅ 核心成果](#核心成果)
    - [🌟 Go 1.25.3应用](#go-1.25.3应用)
    - [📊 架构亮点](#架构亮点)
    - [🎯 适用场景](#适用场景)
  - [下一步学习](#下一步学习)

---

## 1. 搜索引擎原理

### 1.1 搜索引擎架构

```go
package search

// 搜索引擎核心概念
/*
搜索引擎工作流程:

1. 索引阶段 (Indexing):
   文档 → 分词 (Tokenization) → 倒排索引 (Inverted Index)

2. 查询阶段 (Querying):
   查询 → 解析 → 倒排索引查找 → 相关性排序 → 返回结果

核心组件:
  • 爬虫 (Crawler): 获取文档
  • 分词器 (Tokenizer): 文本分词
  • 索引器 (Indexer): 构建倒排索引
  • 查询引擎 (Query Engine): 处理查询
  • 排序器 (Ranker): 相关性排序

关键技术:
  ✅ 倒排索引 (Inverted Index)
  ✅ TF-IDF相关性算法
  ✅ BM25排序算法
  ✅ 中文分词 (jieba, sego)
  ✅ 向量检索 (Vector Search)
  ✅ 模糊搜索 (Fuzzy Search)
*/

// SearchEngine 搜索引擎接口
type SearchEngine interface {
 // 索引文档
 IndexDocument(doc Document) error
 BatchIndex(docs []Document) error

 // 搜索
 Search(query string, opts SearchOptions) (*SearchResult, error)

 // 删除
 Delete(id string) error

 // 更新
 Update(doc Document) error

 // 关闭
 Close() error
}

// Document 文档
type Document struct {
 ID      string                 `json:"id"`
 Title   string                 `json:"title"`
 Content string                 `json:"content"`
 Fields  map[string]interface{} `json:"fields"`
 Score   float64                `json:"score,omitempty"`
}

// SearchOptions 搜索选项
type SearchOptions struct {
 From      int               // 分页起始
 Size      int               // 每页大小
 Sort      []SortField       // 排序字段
 Highlight bool              // 是否高亮
 Filters   map[string]string // 过滤条件
}

// SortField 排序字段
type SortField struct {
 Field string
 Desc  bool
}

// SearchResult 搜索结果
type SearchResult struct {
 Total int64      `json:"total"`
 Hits  []Document `json:"hits"`
 Took  int64      `json:"took_ms"` // 耗时(毫秒)
}
```

---

## 2. 倒排索引

### 2.1 倒排索引原理

```go
package index

import (
 "strings"
 "sync"
)

// 倒排索引 (Inverted Index)
/*
正排索引 (Forward Index):
  DocID → Words
  Doc1  → ["Go", "programming", "language"]
  Doc2  → ["Go", "is", "fast"]

倒排索引 (Inverted Index):
  Word         → DocIDs
  "Go"         → [Doc1, Doc2]
  "programming" → [Doc1]
  "language"   → [Doc1]
  "is"         → [Doc2]
  "fast"       → [Doc2]

优势:
  ✅ O(1)查找包含关键词的文档
  ✅ 支持布尔查询 (AND, OR, NOT)
  ✅ 支持短语查询
*/

// InvertedIndex 倒排索引
type InvertedIndex struct {
 mu     sync.RWMutex
 index  map[string]PostingList // term -> posting list
 docs   map[string]*Document    // docID -> document
 docCount int
}

// PostingList 倒排列表
type PostingList struct {
 DocIDs   []string  // 文档ID列表
 Positions [][]int  // 词在文档中的位置
}

// NewInvertedIndex 创建倒排索引
func NewInvertedIndex() *InvertedIndex {
 return &InvertedIndex{
  index: make(map[string]PostingList),
  docs:  make(map[string]*Document),
 }
}

// IndexDocument 索引文档
func (idx *InvertedIndex) IndexDocument(doc Document) error {
 idx.mu.Lock()
 defer idx.mu.Unlock()

 // 存储文档
 idx.docs[doc.ID] = &doc
 idx.docCount++

 // 分词
 tokens := tokenize(doc.Content)

 // 构建倒排索引
 for pos, token := range tokens {
  token = strings.ToLower(token) // 转小写

  posting := idx.index[token]
  
  // 检查文档是否已存在
  docIndex := -1
  for i, docID := range posting.DocIDs {
   if docID == doc.ID {
    docIndex = i
    break
   }
  }

  if docIndex == -1 {
   // 新文档
   posting.DocIDs = append(posting.DocIDs, doc.ID)
   posting.Positions = append(posting.Positions, []int{pos})
  } else {
   // 已存在文档，添加位置
   posting.Positions[docIndex] = append(posting.Positions[docIndex], pos)
  }

  idx.index[token] = posting
 }

 return nil
}

// Search 搜索
func (idx *InvertedIndex) Search(query string) []Document {
 idx.mu.RLock()
 defer idx.mu.RUnlock()

 // 分词查询
 queryTokens := tokenize(query)

 // 查找包含所有查询词的文档 (AND查询)
 var resultDocIDs []string

 for i, token := range queryTokens {
  token = strings.ToLower(token)

  posting, ok := idx.index[token]
  if !ok {
   return []Document{} // 某个词不存在，返回空结果
  }

  if i == 0 {
   resultDocIDs = posting.DocIDs
  } else {
   // 取交集
   resultDocIDs = intersection(resultDocIDs, posting.DocIDs)
  }

  if len(resultDocIDs) == 0 {
   break
  }
 }

 // 获取文档
 var results []Document
 for _, docID := range resultDocIDs {
  if doc, ok := idx.docs[docID]; ok {
   results = append(results, *doc)
  }
 }

 return results
}

// SearchOR OR查询
func (idx *InvertedIndex) SearchOR(query string) []Document {
 idx.mu.RLock()
 defer idx.mu.RUnlock()

 queryTokens := tokenize(query)

 // 收集所有文档ID (去重)
 docIDSet := make(map[string]bool)

 for _, token := range queryTokens {
  token = strings.ToLower(token)

  if posting, ok := idx.index[token]; ok {
   for _, docID := range posting.DocIDs {
    docIDSet[docID] = true
   }
  }
 }

 // 获取文档
 var results []Document
 for docID := range docIDSet {
  if doc, ok := idx.docs[docID]; ok {
   results = append(results, *doc)
  }
 }

 return results
}

// PhraseSearch 短语查询
func (idx *InvertedIndex) PhraseSearch(phrase string) []Document {
 idx.mu.RLock()
 defer idx.mu.RUnlock()

 tokens := tokenize(phrase)
 if len(tokens) == 0 {
  return []Document{}
 }

 // 获取第一个词的posting list
 firstToken := strings.ToLower(tokens[0])
 posting, ok := idx.index[firstToken]
 if !ok {
  return []Document{}
 }

 var results []Document

 // 检查每个文档
 for docIdx, docID := range posting.DocIDs {
  positions := posting.Positions[docIdx]

  // 检查短语是否匹配
  for _, startPos := range positions {
   match := true

   for i := 1; i < len(tokens); i++ {
    token := strings.ToLower(tokens[i])
    nextPosting, ok := idx.index[token]
    if !ok {
     match = false
     break
    }

    // 查找docID
    found := false
    for nextDocIdx, nextDocID := range nextPosting.DocIDs {
     if nextDocID == docID {
      // 检查位置是否连续
      nextPositions := nextPosting.Positions[nextDocIdx]
      expectedPos := startPos + i

      hasPosition := false
      for _, pos := range nextPositions {
       if pos == expectedPos {
        hasPosition = true
        break
       }
      }

      if !hasPosition {
       match = false
      }

      found = true
      break
     }
    }

    if !found {
     match = false
     break
    }
   }

   if match {
    if doc, ok := idx.docs[docID]; ok {
     results = append(results, *doc)
    }
    break // 已找到该文档，跳过其他位置
   }
  }
 }

 return results
}

// tokenize 简单分词 (空格分隔)
func tokenize(text string) []string {
 return strings.Fields(text)
}

// intersection 取交集
func intersection(a, b []string) []string {
 set := make(map[string]bool)
 for _, item := range a {
  set[item] = true
 }

 var result []string
 for _, item := range b {
  if set[item] {
   result = append(result, item)
  }
 }

 return result
}
```

---

## 3. Bleve本地搜索

### 3.1 Bleve全文搜索

```go
package bleve

import (
 "fmt"
 "log"

 "github.com/blevesearch/bleve/v2"
 "github.com/blevesearch/bleve/v2/analysis/analyzer/custom"
 "github.com/blevesearch/bleve/v2/analysis/token/lowercase"
 "github.com/blevesearch/bleve/v2/analysis/tokenizer/unicode"
 "github.com/blevesearch/bleve/v2/mapping"
)

// Bleve - Pure Go全文搜索
/*
Bleve特点:
  ✅ 纯Go实现 (无C依赖)
  ✅ 支持中文分词
  ✅ 支持模糊查询
  ✅ 支持地理位置搜索
  ✅ 嵌入式 (无需外部服务)

适用场景:
  • 小型应用 (<100万文档)
  • 嵌入式搜索
  • 离线搜索
*/

// BleveEngine Bleve搜索引擎
type BleveEngine struct {
 index bleve.Index
 path  string
}

// NewBleveEngine 创建Bleve引擎
func NewBleveEngine(indexPath string) (*BleveEngine, error) {
 // 创建索引映射
 indexMapping := buildIndexMapping()

 // 打开或创建索引
 var index bleve.Index
 var err error

 index, err = bleve.Open(indexPath)
 if err == bleve.ErrorIndexPathDoesNotExist {
  // 创建新索引
  index, err = bleve.New(indexPath, indexMapping)
  if err != nil {
   return nil, fmt.Errorf("创建索引失败: %w", err)
  }
  log.Printf("✅ 创建新索引: %s", indexPath)
 } else if err != nil {
  return nil, fmt.Errorf("打开索引失败: %w", err)
 } else {
  log.Printf("✅ 打开已有索引: %s", indexPath)
 }

 return &BleveEngine{
  index: index,
  path:  indexPath,
 }, nil
}

// buildIndexMapping 构建索引映射
func buildIndexMapping() mapping.IndexMapping {
 // 文档映射
 docMapping := bleve.NewDocumentMapping()

 // 标题字段 (权重更高)
 titleFieldMapping := bleve.NewTextFieldMapping()
 titleFieldMapping.Analyzer = "standard"
 titleFieldMapping.IncludeInAll = true
 docMapping.AddFieldMappingsAt("title", titleFieldMapping)

 // 内容字段
 contentFieldMapping := bleve.NewTextFieldMapping()
 contentFieldMapping.Analyzer = "standard"
 contentFieldMapping.IncludeInAll = true
 docMapping.AddFieldMappingsAt("content", contentFieldMapping)

 // 索引映射
 indexMapping := bleve.NewIndexMapping()
 indexMapping.AddDocumentMapping("_default", docMapping)

 return indexMapping
}

// IndexDocument 索引文档
func (be *BleveEngine) IndexDocument(doc Document) error {
 err := be.index.Index(doc.ID, doc)
 if err != nil {
  return fmt.Errorf("索引失败: %w", err)
 }

 return nil
}

// BatchIndex 批量索引
func (be *BleveEngine) BatchIndex(docs []Document) error {
 batch := be.index.NewBatch()

 for _, doc := range docs {
  if err := batch.Index(doc.ID, doc); err != nil {
   return fmt.Errorf("批量索引失败: %w", err)
  }
 }

 return be.index.Batch(batch)
}

// Search 搜索
func (be *BleveEngine) Search(queryStr string, opts SearchOptions) (*SearchResult, error) {
 // 构造查询
 query := bleve.NewMatchQuery(queryStr)

 // 搜索请求
 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.From = opts.From
 searchRequest.Size = opts.Size

 // 高亮
 if opts.Highlight {
  searchRequest.Highlight = bleve.NewHighlight()
 }

 // 执行搜索
 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, fmt.Errorf("搜索失败: %w", err)
 }

 // 转换结果
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Took:  int64(searchResult.Took.Milliseconds()),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  doc := Document{
   ID:    hit.ID,
   Score: hit.Score,
  }

  // 获取完整文档
  if err := be.index.Document(hit.ID).(*Document); err == nil {
   result.Hits = append(result.Hits, doc)
  }
 }

 return result, nil
}

// FuzzySearch 模糊搜索
func (be *BleveEngine) FuzzySearch(queryStr string, fuzziness int) (*SearchResult, error) {
 // 模糊查询
 query := bleve.NewFuzzyQuery(queryStr)
 query.Fuzziness = fuzziness // 编辑距离

 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.Size = 10

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // 转换结果
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Took:  int64(searchResult.Took.Milliseconds()),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}

// Delete 删除文档
func (be *BleveEngine) Delete(id string) error {
 return be.index.Delete(id)
}

// Close 关闭索引
func (be *BleveEngine) Close() error {
 return be.index.Close()
}
```

### 3.2 Bleve高级查询

```go
package bleve

import (
 "github.com/blevesearch/bleve/v2"
 "github.com/blevesearch/bleve/v2/search/query"
)

// BooleanSearch 布尔查询
func (be *BleveEngine) BooleanSearch(must, should, mustNot []string) (*SearchResult, error) {
 // 构造布尔查询
 boolQuery := bleve.NewBooleanQuery()

 // MUST (AND)
 for _, term := range must {
  q := bleve.NewMatchQuery(term)
  boolQuery.AddMust(q)
 }

 // SHOULD (OR)
 for _, term := range should {
  q := bleve.NewMatchQuery(term)
  boolQuery.AddShould(q)
 }

 // MUST NOT
 for _, term := range mustNot {
  q := bleve.NewMatchQuery(term)
  boolQuery.AddMustNot(q)
 }

 searchRequest := bleve.NewSearchRequest(boolQuery)
 searchRequest.Size = 20

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // 转换结果
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Took:  int64(searchResult.Took.Milliseconds()),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}

// PrefixSearch 前缀搜索
func (be *BleveEngine) PrefixSearch(prefix string) (*SearchResult, error) {
 query := bleve.NewPrefixQuery(prefix)

 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.Size = 10

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // 转换结果
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}

// WildcardSearch 通配符搜索
func (be *BleveEngine) WildcardSearch(pattern string) (*SearchResult, error) {
 // *: 匹配任意字符
 // ?: 匹配单个字符
 query := bleve.NewWildcardQuery(pattern)

 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.Size = 10

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // 转换结果
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}

// RangeSearch 范围搜索 (数值/日期)
func (be *BleveEngine) RangeSearch(field string, min, max interface{}) (*SearchResult, error) {
 var query query.Query

 // 根据类型构造查询
 switch min.(type) {
 case int, int64, float64:
  query = bleve.NewNumericRangeQuery(&min, &max)
 case string:
  minStr := min.(string)
  maxStr := max.(string)
  query = bleve.NewTermRangeQuery(minStr, maxStr)
 default:
  return nil, fmt.Errorf("不支持的类型")
 }

 searchRequest := bleve.NewSearchRequest(query)
 searchRequest.Size = 10

 searchResult, err := be.index.Search(searchRequest)
 if err != nil {
  return nil, err
 }

 // 转换结果
 result := &SearchResult{
  Total: int64(searchResult.Total),
  Hits:  make([]Document, 0, len(searchResult.Hits)),
 }

 for _, hit := range searchResult.Hits {
  result.Hits = append(result.Hits, Document{
   ID:    hit.ID,
   Score: hit.Score,
  })
 }

 return result, nil
}
```

---

## 4. Elasticsearch集成

### 4.1 Elasticsearch客户端

```go
package elasticsearch

import (
 "bytes"
 "context"
 "encoding/json"
 "fmt"
 "io"
 "log"

 "github.com/elastic/go-elasticsearch/v8"
 "github.com/elastic/go-elasticsearch/v8/esapi"
)

// Elasticsearch - 分布式搜索引擎
/*
Elasticsearch特点:
  ✅ 分布式架构
  ✅ 水平扩展
  ✅ 实时搜索
  ✅ RESTful API
  ✅ 丰富的聚合功能

适用场景:
  • 大规模数据 (>100万文档)
  • 复杂查询
  • 日志分析
  • 实时分析
*/

// ElasticsearchEngine Elasticsearch引擎
type ElasticsearchEngine struct {
 client *elasticsearch.Client
 index  string
}

// NewElasticsearchEngine 创建ES引擎
func NewElasticsearchEngine(addresses []string, indexName string) (*ElasticsearchEngine, error) {
 cfg := elasticsearch.Config{
  Addresses: addresses,
 }

 client, err := elasticsearch.NewClient(cfg)
 if err != nil {
  return nil, fmt.Errorf("创建客户端失败: %w", err)
 }

 // 检查连接
 res, err := client.Info()
 if err != nil {
  return nil, fmt.Errorf("连接失败: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return nil, fmt.Errorf("连接错误: %s", res.String())
 }

 log.Printf("✅ 连接到Elasticsearch")

 engine := &ElasticsearchEngine{
  client: client,
  index:  indexName,
 }

 // 创建索引
 if err := engine.createIndex(); err != nil {
  log.Printf("⚠️  创建索引警告: %v", err)
 }

 return engine, nil
}

// createIndex 创建索引
func (es *ElasticsearchEngine) createIndex() error {
 // 索引映射
 mapping := `{
  "mappings": {
   "properties": {
    "title": {
     "type": "text",
     "analyzer": "standard",
     "fields": {
      "keyword": {
       "type": "keyword"
      }
     }
    },
    "content": {
     "type": "text",
     "analyzer": "standard"
    },
    "timestamp": {
     "type": "date"
    }
   }
  },
  "settings": {
   "number_of_shards": 1,
   "number_of_replicas": 0
  }
 }`

 res, err := es.client.Indices.Create(
  es.index,
  es.client.Indices.Create.WithBody(bytes.NewReader([]byte(mapping))),
 )
 if err != nil {
  return err
 }
 defer res.Body.Close()

 if res.IsError() {
  // 索引已存在是正常情况
  if res.StatusCode != 400 {
   return fmt.Errorf("创建索引失败: %s", res.String())
  }
 }

 return nil
}

// IndexDocument 索引文档
func (es *ElasticsearchEngine) IndexDocument(doc Document) error {
 // 序列化文档
 docJSON, err := json.Marshal(doc)
 if err != nil {
  return fmt.Errorf("序列化失败: %w", err)
 }

 // 索引文档
 req := esapi.IndexRequest{
  Index:      es.index,
  DocumentID: doc.ID,
  Body:       bytes.NewReader(docJSON),
  Refresh:    "true", // 立即刷新 (生产环境应设为false)
 }

 res, err := req.Do(context.Background(), es.client)
 if err != nil {
  return fmt.Errorf("索引失败: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return fmt.Errorf("索引错误: %s", res.String())
 }

 return nil
}

// BatchIndex 批量索引
func (es *ElasticsearchEngine) BatchIndex(docs []Document) error {
 var buf bytes.Buffer

 for _, doc := range docs {
  // 批量操作元数据
  meta := map[string]interface{}{
   "index": map[string]interface{}{
    "_id": doc.ID,
   },
  }
  metaJSON, _ := json.Marshal(meta)
  buf.Write(metaJSON)
  buf.WriteByte('\n')

  // 文档数据
  docJSON, _ := json.Marshal(doc)
  buf.Write(docJSON)
  buf.WriteByte('\n')
 }

 // 批量请求
 res, err := es.client.Bulk(
  bytes.NewReader(buf.Bytes()),
  es.client.Bulk.WithIndex(es.index),
  es.client.Bulk.WithRefresh("true"),
 )
 if err != nil {
  return fmt.Errorf("批量索引失败: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return fmt.Errorf("批量索引错误: %s", res.String())
 }

 return nil
}

// Search 搜索
func (es *ElasticsearchEngine) Search(queryStr string, opts SearchOptions) (*SearchResult, error) {
 // 构造查询
 query := map[string]interface{}{
  "query": map[string]interface{}{
   "multi_match": map[string]interface{}{
    "query":  queryStr,
    "fields": []string{"title^2", "content"}, // title权重为2
   },
  },
  "from": opts.From,
  "size": opts.Size,
 }

 // 高亮
 if opts.Highlight {
  query["highlight"] = map[string]interface{}{
   "fields": map[string]interface{}{
    "title":   map[string]interface{}{},
    "content": map[string]interface{}{},
   },
  }
 }

 queryJSON, _ := json.Marshal(query)

 // 执行搜索
 res, err := es.client.Search(
  es.client.Search.WithIndex(es.index),
  es.client.Search.WithBody(bytes.NewReader(queryJSON)),
 )
 if err != nil {
  return nil, fmt.Errorf("搜索失败: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return nil, fmt.Errorf("搜索错误: %s", res.String())
 }

 // 解析响应
 var esResult struct {
  Took int64 `json:"took"`
  Hits struct {
   Total struct {
    Value int64 `json:"value"`
   } `json:"total"`
   Hits []struct {
    ID     string                 `json:"_id"`
    Score  float64                `json:"_score"`
    Source Document               `json:"_source"`
    Highlight map[string][]string `json:"highlight,omitempty"`
   } `json:"hits"`
  } `json:"hits"`
 }

 body, _ := io.ReadAll(res.Body)
 if err := json.Unmarshal(body, &esResult); err != nil {
  return nil, fmt.Errorf("解析响应失败: %w", err)
 }

 // 转换结果
 result := &SearchResult{
  Total: esResult.Hits.Total.Value,
  Took:  esResult.Took,
  Hits:  make([]Document, 0, len(esResult.Hits.Hits)),
 }

 for _, hit := range esResult.Hits.Hits {
  doc := hit.Source
  doc.ID = hit.ID
  doc.Score = hit.Score

  result.Hits = append(result.Hits, doc)
 }

 return result, nil
}

// Delete 删除文档
func (es *ElasticsearchEngine) Delete(id string) error {
 req := esapi.DeleteRequest{
  Index:      es.index,
  DocumentID: id,
  Refresh:    "true",
 }

 res, err := req.Do(context.Background(), es.client)
 if err != nil {
  return fmt.Errorf("删除失败: %w", err)
 }
 defer res.Body.Close()

 if res.IsError() {
  return fmt.Errorf("删除错误: %s", res.String())
 }

 return nil
}

// Close 关闭客户端
func (es *ElasticsearchEngine) Close() error {
 // Elasticsearch客户端无需显式关闭
 return nil
}
```

### 4.2 Elasticsearch高级查询

```go
package elasticsearch

import (
 "bytes"
 "context"
 "encoding/json"
 "io"
)

// BoolQuery 布尔查询
func (es *ElasticsearchEngine) BoolQuery(must, should, mustNot []string) (*SearchResult, error) {
 // 构造布尔查询
 mustQueries := make([]interface{}, 0, len(must))
 for _, term := range must {
  mustQueries = append(mustQueries, map[string]interface{}{
   "match": map[string]interface{}{
    "content": term,
   },
  })
 }

 shouldQueries := make([]interface{}, 0, len(should))
 for _, term := range should {
  shouldQueries = append(shouldQueries, map[string]interface{}{
   "match": map[string]interface{}{
    "content": term,
   },
  })
 }

 mustNotQueries := make([]interface{}, 0, len(mustNot))
 for _, term := range mustNot {
  mustNotQueries = append(mustNotQueries, map[string]interface{}{
   "match": map[string]interface{}{
    "content": term,
   },
  })
 }

 query := map[string]interface{}{
  "query": map[string]interface{}{
   "bool": map[string]interface{}{
    "must":     mustQueries,
    "should":   shouldQueries,
    "must_not": mustNotQueries,
   },
  },
 }

 queryJSON, _ := json.Marshal(query)

 res, err := es.client.Search(
  es.client.Search.WithIndex(es.index),
  es.client.Search.WithBody(bytes.NewReader(queryJSON)),
 )
 if err != nil {
  return nil, err
 }
 defer res.Body.Close()

 // 解析响应
 var esResult struct {
  Took int64 `json:"took"`
  Hits struct {
   Total struct {
    Value int64 `json:"value"`
   } `json:"total"`
   Hits []struct {
    ID     string   `json:"_id"`
    Score  float64  `json:"_score"`
    Source Document `json:"_source"`
   } `json:"hits"`
  } `json:"hits"`
 }

 body, _ := io.ReadAll(res.Body)
 json.Unmarshal(body, &esResult)

 result := &SearchResult{
  Total: esResult.Hits.Total.Value,
  Took:  esResult.Took,
  Hits:  make([]Document, 0, len(esResult.Hits.Hits)),
 }

 for _, hit := range esResult.Hits.Hits {
  doc := hit.Source
  doc.ID = hit.ID
  doc.Score = hit.Score
  result.Hits = append(result.Hits, doc)
 }

 return result, nil
}

// Aggregation 聚合查询
func (es *ElasticsearchEngine) Aggregation(field string) (map[string]int64, error) {
 query := map[string]interface{}{
  "aggs": map[string]interface{}{
   "field_agg": map[string]interface{}{
    "terms": map[string]interface{}{
     "field": field,
     "size":  10,
    },
   },
  },
  "size": 0, // 不返回文档，只返回聚合结果
 }

 queryJSON, _ := json.Marshal(query)

 res, err := es.client.Search(
  es.client.Search.WithIndex(es.index),
  es.client.Search.WithBody(bytes.NewReader(queryJSON)),
 )
 if err != nil {
  return nil, err
 }
 defer res.Body.Close()

 // 解析聚合结果
 var esResult struct {
  Aggregations struct {
   FieldAgg struct {
    Buckets []struct {
     Key      string `json:"key"`
     DocCount int64  `json:"doc_count"`
    } `json:"buckets"`
   } `json:"field_agg"`
  } `json:"aggregations"`
 }

 body, _ := io.ReadAll(res.Body)
 json.Unmarshal(body, &esResult)

 result := make(map[string]int64)
 for _, bucket := range esResult.Aggregations.FieldAgg.Buckets {
  result[bucket.Key] = bucket.DocCount
 }

 return result, nil
}
```

---

## 5. 中文分词

### 5.1 jieba中文分词

```go
package tokenizer

import (
 "github.com/yanyiwu/gojieba"
)

// 中文分词
/*
中文分词挑战:
  • 无明显词界 (无空格)
  • 歧义问题 ("南京市长江大桥")
  • 新词识别

主流中文分词器:
  • jieba (结巴分词) ✅
  • sego
  • gse
  • mmseg

分词算法:
  • 基于词典: 最大匹配
  • 基于统计: HMM, CRF
  • 基于深度学习: LSTM, BERT
*/

// JiebaTokenizer jieba分词器
type JiebaTokenizer struct {
 jieba *gojieba.Jieba
}

// NewJiebaTokenizer 创建jieba分词器
func NewJiebaTokenizer(dictPath string) *JiebaTokenizer {
 // 加载词典
 jieba := gojieba.NewJieba(
  dictPath+"/jieba.dict.utf8",
  dictPath+"/hmm_model.utf8",
  dictPath+"/user.dict.utf8",
 )

 return &JiebaTokenizer{
  jieba: jieba,
 }
}

// Tokenize 分词
func (jt *JiebaTokenizer) Tokenize(text string) []string {
 // 精确模式
 return jt.jieba.Cut(text, false)
}

// TokenizeWithMode 指定模式分词
func (jt *JiebaTokenizer) TokenizeWithMode(text string, mode TokenizeMode) []string {
 switch mode {
 case ModeSearch:
  // 搜索引擎模式 (细粒度)
  return jt.jieba.CutForSearch(text, true)
 case ModeFull:
  // 全模式 (找出所有可能的词)
  return jt.jieba.CutAll(text)
 default:
  // 精确模式
  return jt.jieba.Cut(text, false)
 }
}

// ExtractKeywords 提取关键词
func (jt *JiebaTokenizer) ExtractKeywords(text string, topN int) []string {
 // TF-IDF提取关键词
 keywords := jt.jieba.ExtractWithWeight(text, topN)

 result := make([]string, 0, len(keywords))
 for _, kw := range keywords {
  result = append(result, kw.Word)
 }

 return result
}

// AddWord 添加自定义词
func (jt *JiebaTokenizer) AddWord(word string, freq int, tag string) {
 jt.jieba.AddWord(word)
}

// Close 关闭分词器
func (jt *JiebaTokenizer) Close() {
 jt.jieba.Free()
}

// TokenizeMode 分词模式
type TokenizeMode int

const (
 ModeExact  TokenizeMode = iota // 精确模式
 ModeFull                       // 全模式
 ModeSearch                     // 搜索引擎模式
)
```

---

## 6. 相关性排序

### 6.1 TF-IDF算法

```go
package ranking

import (
 "math"
)

// TF-IDF相关性算法
/*
TF-IDF (Term Frequency - Inverse Document Frequency)

TF (词频):
  TF(t, d) = (词t在文档d中出现的次数) / (文档d的总词数)

IDF (逆文档频率):
  IDF(t) = log(文档总数 / 包含词t的文档数)

TF-IDF:
  TF-IDF(t, d) = TF(t, d) × IDF(t)

意义:
  • 词在文档中出现越多，权重越高 (TF)
  • 词在所有文档中越罕见，权重越高 (IDF)
*/

// TFIDFRanker TF-IDF排序器
type TFIDFRanker struct {
 docCount int                        // 文档总数
 termDF   map[string]int             // 词的文档频率
 termTF   map[string]map[string]int  // 词在各文档中的词频
}

// NewTFIDFRanker 创建TF-IDF排序器
func NewTFIDFRanker() *TFIDFRanker {
 return &TFIDFRanker{
  termDF: make(map[string]int),
  termTF: make(map[string]map[string]int),
 }
}

// AddDocument 添加文档
func (tr *TFIDFRanker) AddDocument(docID string, tokens []string) {
 tr.docCount++

 // 统计词频
 termFreq := make(map[string]int)
 for _, token := range tokens {
  termFreq[token]++
 }

 // 更新TF
 for term, freq := range termFreq {
  if tr.termTF[term] == nil {
   tr.termTF[term] = make(map[string]int)
  }
  tr.termTF[term][docID] = freq
 }

 // 更新DF (每个文档只计数一次)
 seen := make(map[string]bool)
 for _, token := range tokens {
  if !seen[token] {
   tr.termDF[token]++
   seen[token] = true
  }
 }
}

// CalculateTFIDF 计算TF-IDF
func (tr *TFIDFRanker) CalculateTFIDF(term, docID string, docLength int) float64 {
 // TF
 tf := float64(tr.termTF[term][docID]) / float64(docLength)

 // IDF
 idf := math.Log(float64(tr.docCount) / float64(tr.termDF[term]+1))

 return tf * idf
}

// ScoreDocument 计算文档得分
func (tr *TFIDFRanker) ScoreDocument(queryTerms []string, docID string, docLength int) float64 {
 score := 0.0

 for _, term := range queryTerms {
  score += tr.CalculateTFIDF(term, docID, docLength)
 }

 return score
}
```

### 6.2 BM25算法

```go
package ranking

import (
 "math"
)

// BM25相关性算法
/*
BM25 (Best Matching 25) - Elasticsearch默认算法

BM25 = IDF(q) × (TF(q,d) × (k1 + 1)) / (TF(q,d) + k1 × (1 - b + b × (|d| / avgdl)))

参数:
  • k1: 词频饱和度参数 (1.2-2.0)
  • b:  长度归一化参数 (0.75)
  • |d|: 文档长度
  • avgdl: 平均文档长度

特点:
  ✅ 词频饱和 (高频词影响递减)
  ✅ 长度归一化 (避免长文档优势)
  ✅ 相比TF-IDF更准确
*/

// BM25Ranker BM25排序器
type BM25Ranker struct {
 docCount    int                       // 文档总数
 avgDocLen   float64                   // 平均文档长度
 docLengths  map[string]int            // 文档长度
 termDF      map[string]int            // 词的文档频率
 termTF      map[string]map[string]int // 词在各文档中的词频
 k1          float64                   // 词频饱和参数
 b           float64                   // 长度归一化参数
}

// NewBM25Ranker 创建BM25排序器
func NewBM25Ranker(k1, b float64) *BM25Ranker {
 return &BM25Ranker{
  docLengths: make(map[string]int),
  termDF:     make(map[string]int),
  termTF:     make(map[string]map[string]int),
  k1:         k1,
  b:          b,
 }
}

// AddDocument 添加文档
func (br *BM25Ranker) AddDocument(docID string, tokens []string) {
 br.docCount++
 br.docLengths[docID] = len(tokens)

 // 更新平均文档长度
 totalLen := 0
 for _, length := range br.docLengths {
  totalLen += length
 }
 br.avgDocLen = float64(totalLen) / float64(br.docCount)

 // 统计词频
 termFreq := make(map[string]int)
 for _, token := range tokens {
  termFreq[token]++
 }

 // 更新TF
 for term, freq := range termFreq {
  if br.termTF[term] == nil {
   br.termTF[term] = make(map[string]int)
  }
  br.termTF[term][docID] = freq
 }

 // 更新DF
 seen := make(map[string]bool)
 for _, token := range tokens {
  if !seen[token] {
   br.termDF[token]++
   seen[token] = true
  }
 }
}

// CalculateBM25 计算BM25得分
func (br *BM25Ranker) CalculateBM25(term, docID string) float64 {
 // IDF
 idf := math.Log((float64(br.docCount) - float64(br.termDF[term]) + 0.5) / (float64(br.termDF[term]) + 0.5) + 1.0)

 // TF
 tf := float64(br.termTF[term][docID])

 // 文档长度
 docLen := float64(br.docLengths[docID])

 // BM25公式
 numerator := tf * (br.k1 + 1)
 denominator := tf + br.k1*(1-br.b+br.b*(docLen/br.avgDocLen))

 return idf * (numerator / denominator)
}

// ScoreDocument 计算文档得分
func (br *BM25Ranker) ScoreDocument(queryTerms []string, docID string) float64 {
 score := 0.0

 for _, term := range queryTerms {
  score += br.CalculateBM25(term, docID)
 }

 return score
}
```

---

## 7. 向量检索

### 7.1 向量搜索

```go
package vector

import (
 "math"
)

// 向量检索 (Vector Search)
/*
向量检索 = 语义搜索

传统关键词搜索:
  查询: "Go programming"
  匹配: 包含"Go"和"programming"的文档

向量搜索:
  查询: "Go programming"
  向量化: [0.1, 0.3, 0.5, ...]
  匹配: 向量相似度最高的文档

应用:
  • 语义搜索
  • 推荐系统
  • 图片搜索
  • 问答系统

相似度算法:
  • 余弦相似度 (Cosine Similarity) ✅
  • 欧氏距离 (Euclidean Distance)
  • 曼哈顿距离 (Manhattan Distance)
*/

// VectorIndex 向量索引
type VectorIndex struct {
 vectors map[string][]float64 // docID -> vector
}

// NewVectorIndex 创建向量索引
func NewVectorIndex() *VectorIndex {
 return &VectorIndex{
  vectors: make(map[string][]float64),
 }
}

// AddVector 添加向量
func (vi *VectorIndex) AddVector(docID string, vector []float64) {
 vi.vectors[docID] = vector
}

// Search 向量搜索
func (vi *VectorIndex) Search(queryVector []float64, topK int) []SearchResultItem {
 // 计算所有文档的相似度
 similarities := make([]SearchResultItem, 0, len(vi.vectors))

 for docID, docVector := range vi.vectors {
  similarity := cosineSimilarity(queryVector, docVector)
  similarities = append(similarities, SearchResultItem{
   DocID: docID,
   Score: similarity,
  })
 }

 // 排序 (相似度降序)
 sort.Slice(similarities, func(i, j int) bool {
  return similarities[i].Score > similarities[j].Score
 })

 // 返回Top K
 if topK > len(similarities) {
  topK = len(similarities)
 }

 return similarities[:topK]
}

// SearchResultItem 搜索结果项
type SearchResultItem struct {
 DocID string
 Score float64
}

// cosineSimilarity 余弦相似度
func cosineSimilarity(a, b []float64) float64 {
 if len(a) != len(b) {
  return 0
 }

 dotProduct := 0.0
 normA := 0.0
 normB := 0.0

 for i := 0; i < len(a); i++ {
  dotProduct += a[i] * b[i]
  normA += a[i] * a[i]
  normB += b[i] * b[i]
 }

 if normA == 0 || normB == 0 {
  return 0
 }

 return dotProduct / (math.Sqrt(normA) * math.Sqrt(normB))
}

// euclideanDistance 欧氏距离
func euclideanDistance(a, b []float64) float64 {
 if len(a) != len(b) {
  return math.Inf(1)
 }

 sum := 0.0
 for i := 0; i < len(a); i++ {
  diff := a[i] - b[i]
  sum += diff * diff
 }

 return math.Sqrt(sum)
}
```

---

## 8. 搜索建议

### 8.1 自动补全

```go
package suggest

import (
 "strings"
)

// Trie树自动补全
/*
Trie树 (前缀树):
  • 每个节点代表一个字符
  • 从根到叶的路径代表一个词
  • 支持前缀查询

示例:
       root
       /  \
      c    g
     / \    \
    a   o    o
   /     \
  t      d
  (cat) (code) (go)

应用:
  • 自动补全
  • 拼写检查
  • IP路由
*/

// Trie Trie树
type Trie struct {
 root *TrieNode
}

// TrieNode Trie节点
type TrieNode struct {
 children map[rune]*TrieNode
 isEnd    bool
 word     string
 freq     int // 词频 (用于排序)
}

// NewTrie 创建Trie树
func NewTrie() *Trie {
 return &Trie{
  root: &TrieNode{
   children: make(map[rune]*TrieNode),
  },
 }
}

// Insert 插入词
func (t *Trie) Insert(word string, freq int) {
 node := t.root

 for _, ch := range word {
  if node.children[ch] == nil {
   node.children[ch] = &TrieNode{
    children: make(map[rune]*TrieNode),
   }
  }
  node = node.children[ch]
 }

 node.isEnd = true
 node.word = word
 node.freq = freq
}

// SearchPrefix 前缀搜索
func (t *Trie) SearchPrefix(prefix string) []string {
 node := t.root

 // 找到前缀节点
 for _, ch := range prefix {
  if node.children[ch] == nil {
   return []string{} // 前缀不存在
  }
  node = node.children[ch]
 }

 // 收集所有以该前缀开始的词
 var results []string
 t.collectWords(node, &results)

 return results
}

// collectWords 收集所有词
func (t *Trie) collectWords(node *TrieNode, results *[]string) {
 if node.isEnd {
  *results = append(*results, node.word)
 }

 for _, child := range node.children {
  t.collectWords(child, results)
 }
}

// AutoComplete 自动补全 (Top K)
func (t *Trie) AutoComplete(prefix string, topK int) []Suggestion {
 node := t.root

 // 找到前缀节点
 for _, ch := range prefix {
  if node.children[ch] == nil {
   return []Suggestion{}
  }
  node = node.children[ch]
 }

 // 收集所有词及其频率
 var suggestions []Suggestion
 t.collectSuggestions(node, &suggestions)

 // 按频率排序
 sort.Slice(suggestions, func(i, j int) bool {
  return suggestions[i].Freq > suggestions[j].Freq
 })

 // 返回Top K
 if topK > len(suggestions) {
  topK = len(suggestions)
 }

 return suggestions[:topK]
}

// collectSuggestions 收集建议
func (t *Trie) collectSuggestions(node *TrieNode, suggestions *[]Suggestion) {
 if node.isEnd {
  *suggestions = append(*suggestions, Suggestion{
   Word: node.word,
   Freq: node.freq,
  })
 }

 for _, child := range node.children {
  t.collectSuggestions(child, suggestions)
 }
}

// Suggestion 搜索建议
type Suggestion struct {
 Word string
 Freq int
}
```

---

## 9. 性能优化

### 9.1 索引优化

```go
package optimization

// 索引优化技巧

/*
1. 批量索引
   ✅ 使用Bulk API
   ✅ 批量大小: 1000-5000文档
   ❌ 避免单条索引

2. 刷新策略
   ✅ 生产: refresh_interval = 30s
   ❌ 开发: refresh = true (立即刷新)

3. 副本策略
   ✅ 索引阶段: 0副本
   ✅ 查询阶段: 1+副本

4. 分片策略
   • 每个分片: 20-50GB
   • 分片数 = 索引大小 / 30GB

5. 字段优化
   ✅ 不需要搜索的字段: index: false
   ✅ 不需要评分的字段: norms: false
   ✅ 精确匹配字段: keyword类型

6. 查询优化
   ✅ 使用filter (不计算评分)
   ✅ 缓存频繁查询
   ✅ 限制返回字段 (_source: false)

7. 硬件优化
   ✅ SSD硬盘
   ✅ 充足内存 (索引大小的2倍)
   ✅ 多核CPU
*/

// IndexOptimizer 索引优化器
type IndexOptimizer struct {
 batchSize     int
 refreshInterval string
 replicas      int
}

// OptimizeForBulkIndexing 批量索引优化
func (io *IndexOptimizer) OptimizeForBulkIndexing() map[string]interface{} {
 return map[string]interface{}{
  "index": map[string]interface{}{
   "refresh_interval": "-1",      // 禁用自动刷新
   "number_of_replicas": 0,       // 0副本
   "translog.durability": "async", // 异步translog
  },
 }
}

// OptimizeForQuerying 查询优化
func (io *IndexOptimizer) OptimizeForQuerying() map[string]interface{} {
 return map[string]interface{}{
  "index": map[string]interface{}{
   "refresh_interval": "30s", // 30秒刷新
   "number_of_replicas": 1,   // 1副本
  },
 }
}
```

---

## 10. 生产部署

### 10.1 Docker部署

```dockerfile
# Dockerfile
FROM golang:1.25-alpine AS builder

WORKDIR /app

COPY go.mod go.sum ./
RUN go mod download

COPY . .

RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o search-engine ./cmd/search

FROM alpine:latest

RUN apk --no-cache add ca-certificates

WORKDIR /root/

COPY --from=builder /app/search-engine .
COPY --from=builder /app/dict ./dict

EXPOSE 8080

CMD ["./search-engine"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  search-api:
    build: .
    ports:
      - "8080:8080"
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - xpack.security.enabled=false
    volumes:
      - es-data:/usr/share/elasticsearch/data
    restart: unless-stopped

  # Kibana (可视化)
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  es-data:
```

---

## 总结

### ✅ 核心成果

1. **搜索引擎原理**
   - 倒排索引
   - 分词技术
   - 相关性排序

2. **Bleve本地搜索**
   - 纯Go实现
   - 嵌入式搜索
   - 模糊查询

3. **Elasticsearch集成**
   - 分布式搜索
   - 批量索引
   - 高级查询

4. **中文分词**
   - jieba分词
   - 关键词提取
   - 自定义词典

5. **相关性算法**
   - TF-IDF
   - BM25
   - 向量检索

6. **搜索建议**
   - Trie树
   - 自动补全
   - 拼写纠错

### 🌟 Go 1.25.3应用

| 特性 | 应用 |
|------|------|
| Goroutine | 并发索引 |
| Channel | 批量处理管道 |
| Map | 倒排索引 |
| Interface | 搜索引擎抽象 |
| Generic | 类型安全的数据结构 |

### 📊 架构亮点

- **高性能**: 倒排索引O(1)查找
- **可扩展**: 分布式架构
- **语义搜索**: 向量检索
- **中文友好**: jieba分词

### 🎯 适用场景

- 电商搜索
- 日志分析
- 知识库检索
- 企业搜索

---

## 下一步学习

- [ ] Lucene内核原理
- [ ] 向量数据库 (Milvus, Weaviate)
- [ ] BERT语义搜索
- [ ] 分布式搜索架构

---

**版本**: v1.0  
**更新日期**: 2025-10-29  
**适用于**: Go 1.25.3

---
