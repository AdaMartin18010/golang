# Go 1.25.3 AIä¸æœºå™¨å­¦ä¹ é›†æˆå®Œæ•´å®æˆ˜

## ğŸ“‹ ç›®å½•

- [Go 1.25.3 AIä¸æœºå™¨å­¦ä¹ é›†æˆå®Œæ•´å®æˆ˜](#go-1253-aiä¸æœºå™¨å­¦ä¹ é›†æˆå®Œæ•´å®æˆ˜)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. AI/MLæ¦‚è¿°](#1-aimlæ¦‚è¿°)
    - [1.1 Goåœ¨AI/MLä¸­çš„è§’è‰²](#11-goåœ¨aimlä¸­çš„è§’è‰²)
    - [1.2 æ¨¡å‹äº¤æ¢æ ¼å¼](#12-æ¨¡å‹äº¤æ¢æ ¼å¼)
  - [2. ONNX Runtimeé›†æˆ](#2-onnx-runtimeé›†æˆ)
    - [2.1 ONNX RuntimeåŸºç¡€](#21-onnx-runtimeåŸºç¡€)
    - [2.2 æ‰¹é‡æ¨ç†ä¼˜åŒ–](#22-æ‰¹é‡æ¨ç†ä¼˜åŒ–)
    - [2.3 GPUåŠ é€Ÿ](#23-gpuåŠ é€Ÿ)
  - [3. TensorFlow Go](#3-tensorflow-go)
    - [3.1 TensorFlow C APIé›†æˆ](#31-tensorflow-c-apié›†æˆ)
    - [3.2 TensorFlow Lite](#32-tensorflow-lite)
  - [4. PyTorchæ¨¡å‹æœåŠ¡](#4-pytorchæ¨¡å‹æœåŠ¡)
    - [4.1 TorchScripté›†æˆ](#41-torchscripté›†æˆ)
    - [4.2 gRPCæ¨¡å‹æœåŠ¡](#42-grpcæ¨¡å‹æœåŠ¡)
  - [5. è‡ªç„¶è¯­è¨€å¤„ç†](#5-è‡ªç„¶è¯­è¨€å¤„ç†)
    - [5.1 æ–‡æœ¬åˆ†ç±»](#51-æ–‡æœ¬åˆ†ç±»)
    - [5.2 å‘½åå®ä½“è¯†åˆ« (NER)](#52-å‘½åå®ä½“è¯†åˆ«-ner)
  - [6. è®¡ç®—æœºè§†è§‰](#6-è®¡ç®—æœºè§†è§‰)
    - [6.1 å›¾åƒåˆ†ç±»](#61-å›¾åƒåˆ†ç±»)
    - [6.2 ç›®æ ‡æ£€æµ‹ (YOLO)](#62-ç›®æ ‡æ£€æµ‹-yolo)
  - [7. æ¨èç³»ç»Ÿ](#7-æ¨èç³»ç»Ÿ)
    - [7.1 ååŒè¿‡æ»¤](#71-ååŒè¿‡æ»¤)
    - [7.2 æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹](#72-æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹)
  - [8. æ¨¡å‹æœåŠ¡åŒ–](#8-æ¨¡å‹æœåŠ¡åŒ–)
    - [8.1 RESTful API](#81-restful-api)
    - [8.2 æ¨¡å‹ç‰ˆæœ¬ç®¡ç†](#82-æ¨¡å‹ç‰ˆæœ¬ç®¡ç†)
  - [9. æ€§èƒ½ä¼˜åŒ–](#9-æ€§èƒ½ä¼˜åŒ–)
    - [9.1 æ¨¡å‹é‡åŒ–](#91-æ¨¡å‹é‡åŒ–)
    - [9.2 æ‰¹å¤„ç†ä¼˜åŒ–](#92-æ‰¹å¤„ç†ä¼˜åŒ–)
  - [10. å®Œæ•´é¡¹ç›®ç¤ºä¾‹](#10-å®Œæ•´é¡¹ç›®ç¤ºä¾‹)
    - [10.1 AIé©±åŠ¨çš„å†…å®¹å®¡æ ¸ç³»ç»Ÿ](#101-aié©±åŠ¨çš„å†…å®¹å®¡æ ¸ç³»ç»Ÿ)
    - [10.2 ç”Ÿäº§ç¯å¢ƒé…ç½®](#102-ç”Ÿäº§ç¯å¢ƒé…ç½®)
  - [æ€»ç»“](#æ€»ç»“)
    - [Go AI/MLæŠ€æœ¯æ ˆ](#go-aimlæŠ€æœ¯æ ˆ)
    - [æ€§èƒ½ä¼˜åŠ¿](#æ€§èƒ½ä¼˜åŠ¿)
    - [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## 1. AI/MLæ¦‚è¿°

### 1.1 Goåœ¨AI/MLä¸­çš„è§’è‰²

```go
// Goåœ¨AI/MLç”Ÿæ€ä¸­çš„å®šä½
/*
è®­ç»ƒ (Training) â†’ Python (PyTorch, TensorFlow, scikit-learn)
    â€¢ å¤§é‡æ•°æ®å¤„ç†
    â€¢ å¤æ‚æ¨¡å‹è®­ç»ƒ
    â€¢ è¶…å‚æ•°è°ƒä¼˜

æ¨ç† (Inference) â†’ Go â­
    â€¢ é«˜æ€§èƒ½æœåŠ¡
    â€¢ ä½å»¶è¿Ÿå“åº”
    â€¢ å¹¶å‘å¤„ç†
    â€¢ æ˜“éƒ¨ç½²

Goçš„AI/MLä¼˜åŠ¿:
âœ… 1. æ¨ç†æ€§èƒ½: æ¯”Pythonå¿«5-10å€
âœ… 2. å¹¶å‘èƒ½åŠ›: Goroutineå¤„ç†å¤šè¯·æ±‚
âœ… 3. å†…å­˜æ•ˆç‡: èµ„æºå ç”¨å°
âœ… 4. éƒ¨ç½²ç®€å•: å•ä¸€äºŒè¿›åˆ¶æ–‡ä»¶
âœ… 5. ç”Ÿäº§å°±ç»ª: ç¨³å®šå¯é 

å…¸å‹æ¶æ„:
  Python (è®­ç»ƒ) â†’ å¯¼å‡ºæ¨¡å‹ (ONNX/SavedModel) â†’ Go (æ¨ç†æœåŠ¡)
*/

// AI/MLæŠ€æœ¯æ ˆ
type AIMLStack struct {
 ModelFormat     string // "ONNX, TensorFlow SavedModel, PyTorch TorchScript"
 Runtime         string // "ONNX Runtime, TensorFlow C API, LibTorch"
 Preprocessing   string // "å›¾åƒ/æ–‡æœ¬é¢„å¤„ç†"
 Postprocessing  string // "ç»“æœè§£æä¸æ ¼å¼åŒ–"
 Serving         string // "gRPC, REST API"
 Monitoring      string // "å»¶è¿Ÿ, ååé‡, æ¨¡å‹æ€§èƒ½"
}

// åº”ç”¨åœºæ™¯
var useCases = []string{
 "å›¾åƒåˆ†ç±»ä¸æ£€æµ‹",
 "è‡ªç„¶è¯­è¨€å¤„ç† (NLP)",
 "æ¨èç³»ç»Ÿ",
 "å¼‚å¸¸æ£€æµ‹",
 "æ—¶é—´åºåˆ—é¢„æµ‹",
 "è¯­éŸ³è¯†åˆ«",
 "æœç´¢æ’åº",
}
```

### 1.2 æ¨¡å‹äº¤æ¢æ ¼å¼

```go
// ONNX (Open Neural Network Exchange)
/*
ONNXæ˜¯è·¨æ¡†æ¶çš„æ¨¡å‹äº¤æ¢æ ‡å‡†

æ”¯æŒçš„æ¡†æ¶:
  â€¢ PyTorch â†’ ONNX
  â€¢ TensorFlow â†’ ONNX
  â€¢ scikit-learn â†’ ONNX
  â€¢ Keras â†’ ONNX

ä¼˜åŠ¿:
  âœ… è·¨å¹³å°: Windows, Linux, macOS
  âœ… è·¨æ¡†æ¶: ç»Ÿä¸€æ ¼å¼
  âœ… ä¼˜åŒ–: ç®—å­èåˆ, é‡åŒ–
  âœ… ç¡¬ä»¶åŠ é€Ÿ: CPU, GPU, NPU

ç¤ºä¾‹: PyTorchå¯¼å‡ºONNX
```

```python
# Python (è®­ç»ƒä¸å¯¼å‡º)
import torch
import torch.nn as nn

# å®šä¹‰æ¨¡å‹
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 2)

    def forward(self, x):
        return self.fc(x)

model = SimpleModel()
model.eval()

# å¯¼å‡ºONNX
dummy_input = torch.randn(1, 10)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}}
)
```

---

## 2. ONNX Runtimeé›†æˆ

### 2.1 ONNX RuntimeåŸºç¡€

```go
package main

import (
 "fmt"
 "log"

 ort "github.com/yalue/onnxruntime_go"
)

// ONNXInferenceEngineå°è£…ONNX Runtime
type ONNXInferenceEngine struct {
 session *ort.Session
 inputs  []string
 outputs []string
}

func NewONNXInferenceEngine(modelPath string) (*ONNXInferenceEngine, error) {
 // åˆå§‹åŒ–ONNX Runtimeç¯å¢ƒ
 ort.SetSharedLibraryPath("onnxruntime.so") // Linux
 // ort.SetSharedLibraryPath("onnxruntime.dll") // Windows

 if err := ort.InitializeEnvironment(); err != nil {
  return nil, fmt.Errorf("init environment: %w", err)
 }

 // åŠ è½½æ¨¡å‹
 session, err := ort.NewSession(modelPath, ort.NewSessionOptions())
 if err != nil {
  return nil, fmt.Errorf("load model: %w", err)
 }

 // è·å–è¾“å…¥è¾“å‡ºåç§°
 inputs, err := session.GetInputNames()
 if err != nil {
  return nil, err
 }

 outputs, err := session.GetOutputNames()
 if err != nil {
  return nil, err
 }

 return &ONNXInferenceEngine{
  session: session,
  inputs:  inputs,
  outputs: outputs,
 }, nil
}

// Predictæ‰§è¡Œæ¨ç†
func (e *ONNXInferenceEngine) Predict(input []float32, shape []int64) ([]float32, error) {
 // åˆ›å»ºè¾“å…¥tensor
 inputTensor, err := ort.NewTensor(shape, input)
 if err != nil {
  return nil, fmt.Errorf("create input tensor: %w", err)
 }
 defer inputTensor.Destroy()

 // è¿è¡Œæ¨ç†
 outputs, err := e.session.Run(
  []ort.Value{inputTensor},
  e.inputs,
  e.outputs,
 )
 if err != nil {
  return nil, fmt.Errorf("run inference: %w", err)
 }
 defer outputs[0].Destroy()

 // æå–ç»“æœ
 outputData := outputs[0].GetData()
 return outputData.([]float32), nil
}

func (e *ONNXInferenceEngine) Close() {
 if e.session != nil {
  e.session.Destroy()
 }
 ort.DestroyEnvironment()
}

// ä½¿ç”¨ç¤ºä¾‹
func ExampleONNXInference() {
 engine, err := NewONNXInferenceEngine("model.onnx")
 if err != nil {
  log.Fatal(err)
 }
 defer engine.Close()

 // å‡†å¤‡è¾“å…¥æ•°æ® (batch=1, features=10)
 input := make([]float32, 10)
 for i := range input {
  input[i] = float32(i) * 0.1
 }

 // æ‰§è¡Œæ¨ç†
 output, err := engine.Predict(input, []int64{1, 10})
 if err != nil {
  log.Fatal(err)
 }

 fmt.Printf("Output: %v\n", output)
}
```

### 2.2 æ‰¹é‡æ¨ç†ä¼˜åŒ–

```go
package inference

import (
 "context"
 "fmt"
 "sync"
 "time"

 ort "github.com/yalue/onnxruntime_go"
)

// BatchInferenceEngineæ”¯æŒæ‰¹é‡æ¨ç†
type BatchInferenceEngine struct {
 engine    *ONNXInferenceEngine
 batchSize int
 timeout   time.Duration

 mu      sync.Mutex
 batch   []InferenceRequest
 results chan InferenceResult
}

type InferenceRequest struct {
 ID    string
 Data  []float32
 Shape []int64
}

type InferenceResult struct {
 ID     string
 Output []float32
 Error  error
}

func NewBatchInferenceEngine(modelPath string, batchSize int, timeout time.Duration) (*BatchInferenceEngine, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 be := &BatchInferenceEngine{
  engine:    engine,
  batchSize: batchSize,
  timeout:   timeout,
  batch:     make([]InferenceRequest, 0, batchSize),
  results:   make(chan InferenceResult, 100),
 }

 // å¯åŠ¨æ‰¹å¤„ç†worker
 go be.batchWorker()

 return be, nil
}

// Inferæäº¤æ¨ç†è¯·æ±‚
func (be *BatchInferenceEngine) Infer(ctx context.Context, req InferenceRequest) ([]float32, error) {
 be.mu.Lock()
 be.batch = append(be.batch, req)
 batchFull := len(be.batch) >= be.batchSize
 be.mu.Unlock()

 // å¦‚æœæ‰¹æ¬¡æ»¡äº†,ç«‹å³è§¦å‘æ¨ç†
 if batchFull {
  be.processBatch()
 }

 // ç­‰å¾…ç»“æœ
 select {
 case result := <-be.results:
  if result.ID == req.ID {
   return result.Output, result.Error
  }
 case <-ctx.Done():
  return nil, ctx.Err()
 case <-time.After(be.timeout):
  return nil, fmt.Errorf("inference timeout")
 }

 return nil, fmt.Errorf("unexpected error")
}

func (be *BatchInferenceEngine) batchWorker() {
 ticker := time.NewTicker(be.timeout)
 defer ticker.Stop()

 for {
  <-ticker.C
  be.processBatch()
 }
}

func (be *BatchInferenceEngine) processBatch() {
 be.mu.Lock()
 if len(be.batch) == 0 {
  be.mu.Unlock()
  return
 }

 batch := be.batch
 be.batch = make([]InferenceRequest, 0, be.batchSize)
 be.mu.Unlock()

 // åˆå¹¶è¾“å…¥
 batchData := make([]float32, 0)
 for _, req := range batch {
  batchData = append(batchData, req.Data...)
 }

 // æ‰¹é‡æ¨ç†
 batchShape := []int64{int64(len(batch)), int64(len(batch[0].Data))}
 outputs, err := be.engine.Predict(batchData, batchShape)

 // åˆ†å‘ç»“æœ
 outputSize := len(outputs) / len(batch)
 for i, req := range batch {
  result := InferenceResult{
   ID:    req.ID,
   Error: err,
  }
  if err == nil {
   start := i * outputSize
   end := start + outputSize
   result.Output = outputs[start:end]
  }
  be.results <- result
 }
}
```

### 2.3 GPUåŠ é€Ÿ

```go
package gpu

import (
 "fmt"

 ort "github.com/yalue/onnxruntime_go"
)

// GPUInferenceEngineä½¿ç”¨GPUåŠ é€Ÿ
type GPUInferenceEngine struct {
 session *ort.Session
}

func NewGPUInferenceEngine(modelPath string, deviceID int) (*GPUInferenceEngine, error) {
 ort.SetSharedLibraryPath("onnxruntime.so")

 if err := ort.InitializeEnvironment(); err != nil {
  return nil, err
 }

 // é…ç½®CUDAæ‰§è¡Œæä¾›å™¨
 options := ort.NewSessionOptions()

 // æ·»åŠ CUDAæ‰§è¡Œæä¾›å™¨
 cudaOptions := map[string]string{
  "device_id": fmt.Sprintf("%d", deviceID),
 }

 if err := options.AppendExecutionProviderCUDA(cudaOptions); err != nil {
  return nil, fmt.Errorf("add CUDA provider: %w", err)
 }

 // ä¼˜åŒ–é€‰é¡¹
 options.SetIntraOpNumThreads(4)
 options.SetInterOpNumThreads(4)
 options.SetGraphOptimizationLevel(ort.AllOptimizations)

 session, err := ort.NewSession(modelPath, options)
 if err != nil {
  return nil, err
 }

 return &GPUInferenceEngine{session: session}, nil
}

// æ€§èƒ½å¯¹æ¯”
/*
æ¨¡å‹: ResNet-50
è¾“å…¥: 224x224 å›¾åƒ
æ‰¹å¤§å°: 32

CPU (Intel Xeon):
  â€¢ å»¶è¿Ÿ: ~500ms/batch
  â€¢ ååé‡: ~64 images/sec

GPU (NVIDIA V100):
  â€¢ å»¶è¿Ÿ: ~50ms/batch
  â€¢ ååé‡: ~640 images/sec
  â€¢ åŠ é€Ÿæ¯”: 10x âœ…

GPUé€‚ç”¨åœºæ™¯:
  âœ… å¤§æ¨¡å‹ (ResNet, BERT, GPT)
  âœ… æ‰¹é‡å¤„ç†
  âœ… å®æ—¶è§†é¢‘å¤„ç†
  âœ… é«˜ååé‡éœ€æ±‚
*/
```

---

## 3. TensorFlow Go

### 3.1 TensorFlow C APIé›†æˆ

```go
package tensorflow

import (
 "fmt"
 "io/ioutil"

 tf "github.com/tensorflow/tensorflow/tensorflow/go"
 "github.com/tensorflow/tensorflow/tensorflow/go/op"
)

// TFInferenceEngineå°è£…TensorFlowæ¨ç†
type TFInferenceEngine struct {
 model   *tf.SavedModel
 session *tf.Session
}

func NewTFInferenceEngine(modelDir string) (*TFInferenceEngine, error) {
 // åŠ è½½SavedModel
 model, err := tf.LoadSavedModel(modelDir, []string{"serve"}, nil)
 if err != nil {
  return nil, fmt.Errorf("load model: %w", err)
 }

 return &TFInferenceEngine{
  model:   model,
  session: model.Session,
 }, nil
}

// Predictæ‰§è¡Œæ¨ç†
func (e *TFInferenceEngine) Predict(input [][]float32) ([][]float32, error) {
 // åˆ›å»ºè¾“å…¥tensor
 inputTensor, err := tf.NewTensor(input)
 if err != nil {
  return nil, fmt.Errorf("create tensor: %w", err)
 }

 // è¿è¡Œæ¨ç†
 feeds := map[tf.Output]*tf.Tensor{
  e.model.Graph.Operation("serving_default_input").Output(0): inputTensor,
 }

 fetches := []tf.Output{
  e.model.Graph.Operation("StatefulPartitionedCall").Output(0),
 }

 results, err := e.session.Run(feeds, fetches, nil)
 if err != nil {
  return nil, fmt.Errorf("run inference: %w", err)
 }

 // æå–ç»“æœ
 output := results[0].Value().([][]float32)
 return output, nil
}

func (e *TFInferenceEngine) Close() {
 if e.session != nil {
  e.session.Close()
 }
}
```

### 3.2 TensorFlow Lite

```go
package tflite

import (
 "fmt"

 "github.com/mattn/go-tflite"
)

// TFLiteEngineç”¨äºç§»åŠ¨/åµŒå…¥å¼è®¾å¤‡
type TFLiteEngine struct {
 model      *tflite.Model
 interpreter *tflite.Interpreter
}

func NewTFLiteEngine(modelPath string) (*TFLiteEngine, error) {
 // åŠ è½½TFLiteæ¨¡å‹
 model := tflite.NewModelFromFile(modelPath)
 if model == nil {
  return nil, fmt.Errorf("failed to load model")
 }

 // åˆ›å»ºè§£é‡Šå™¨
 options := tflite.NewInterpreterOptions()
 options.SetNumThread(4)

 interpreter := tflite.NewInterpreter(model, options)
 if interpreter == nil {
  return nil, fmt.Errorf("failed to create interpreter")
 }

 // åˆ†é…tensors
 if status := interpreter.AllocateTensors(); status != tflite.OK {
  return nil, fmt.Errorf("allocate tensors failed")
 }

 return &TFLiteEngine{
  model:      model,
  interpreter: interpreter,
 }, nil
}

// Inferæ‰§è¡Œæ¨ç†
func (e *TFLiteEngine) Infer(input []float32) ([]float32, error) {
 // è®¾ç½®è¾“å…¥
 inputTensor := e.interpreter.GetInputTensor(0)
 copy(inputTensor.Float32s(), input)

 // æ‰§è¡Œæ¨ç†
 if status := e.interpreter.Invoke(); status != tflite.OK {
  return nil, fmt.Errorf("invoke failed")
 }

 // è·å–è¾“å‡º
 outputTensor := e.interpreter.GetOutputTensor(0)
 output := make([]float32, outputTensor.Dim(1))
 copy(output, outputTensor.Float32s())

 return output, nil
}

// TFLiteä¼˜åŠ¿
/*
æ¨¡å‹å¤§å°å¯¹æ¯”:
  TensorFlow SavedModel: 100MB
  TFLite: 25MB (4x smaller) âœ…

å»¶è¿Ÿå¯¹æ¯” (ç§»åŠ¨CPU):
  TensorFlow: 200ms
  TFLite: 50ms (4x faster) âœ…

é€‚ç”¨åœºæ™¯:
  âœ… ç§»åŠ¨åº”ç”¨ (Android, iOS)
  âœ… åµŒå…¥å¼è®¾å¤‡ (æ ‘è“æ´¾, Jetson Nano)
  âœ… è¾¹ç¼˜è®¡ç®—
  âœ… ä½åŠŸè€—åœºæ™¯
*/
```

---

## 4. PyTorchæ¨¡å‹æœåŠ¡

### 4.1 TorchScripté›†æˆ

```go
package pytorch

import (
 "fmt"

 torch "github.com/wangkuiyi/gotorch"
)

// PyTorchEngineåŠ è½½TorchScriptæ¨¡å‹
type PyTorchEngine struct {
 module torch.Module
}

func NewPyTorchEngine(modelPath string) (*PyTorchEngine, error) {
 // åŠ è½½TorchScriptæ¨¡å‹
 module, err := torch.Load(modelPath)
 if err != nil {
  return nil, fmt.Errorf("load model: %w", err)
 }

 // è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
 module.Eval()

 return &PyTorchEngine{module: module}, nil
}

// Forwardæ‰§è¡Œå‰å‘ä¼ æ’­
func (e *PyTorchEngine) Forward(input []float32, shape []int64) ([]float32, error) {
 // åˆ›å»ºtensor
 inputTensor := torch.NewTensor(input, shape)

 // æ‰§è¡Œæ¨ç† (no_gradæ¨¡å¼)
 var output torch.Tensor
 torch.NoGrad(func() {
  output = e.module.Forward(inputTensor)
 })

 // æå–æ•°æ®
 outputData := output.Float32Value()
 return outputData, nil
}
```

```python
# Python: å¯¼å‡ºTorchScript
import torch

# å®šä¹‰æ¨¡å‹
class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(10, 5)

    def forward(self, x):
        return self.linear(x)

model = MyModel()
model.eval()

# æ–¹æ³•1: Tracing
example_input = torch.rand(1, 10)
traced_model = torch.jit.trace(model, example_input)
traced_model.save("model_traced.pt")

# æ–¹æ³•2: Scripting
scripted_model = torch.jit.script(model)
scripted_model.save("model_scripted.pt")
```

### 4.2 gRPCæ¨¡å‹æœåŠ¡

```go
package serving

import (
 "context"
 "fmt"

 pb "your_project/proto"
)

// ModelServerå®ç°gRPCæœåŠ¡
type ModelServer struct {
 pb.UnimplementedModelServiceServer
 engine *PyTorchEngine
}

func NewModelServer(modelPath string) (*ModelServer, error) {
 engine, err := NewPyTorchEngine(modelPath)
 if err != nil {
  return nil, err
 }

 return &ModelServer{engine: engine}, nil
}

// Predictå¤„ç†æ¨ç†è¯·æ±‚
func (s *ModelServer) Predict(ctx context.Context, req *pb.PredictRequest) (*pb.PredictResponse, error) {
 // æ‰§è¡Œæ¨ç†
 output, err := s.engine.Forward(req.Input, req.Shape)
 if err != nil {
  return nil, fmt.Errorf("inference failed: %w", err)
 }

 return &pb.PredictResponse{
  Output: output,
 }, nil
}
```

```protobuf
// proto/model_service.proto
syntax = "proto3";

package model;

option go_package = "your_project/proto";

service ModelService {
  rpc Predict(PredictRequest) returns (PredictResponse);
  rpc BatchPredict(BatchPredictRequest) returns (BatchPredictResponse);
}

message PredictRequest {
  repeated float input = 1;
  repeated int64 shape = 2;
}

message PredictResponse {
  repeated float output = 1;
}

message BatchPredictRequest {
  repeated PredictRequest requests = 1;
}

message BatchPredictResponse {
  repeated PredictResponse responses = 1;
}
```

---

## 5. è‡ªç„¶è¯­è¨€å¤„ç†

### 5.1 æ–‡æœ¬åˆ†ç±»

```go
package nlp

import (
 "fmt"
 "strings"

 ort "github.com/yalue/onnxruntime_go"
)

// TextClassifierå®ç°æ–‡æœ¬åˆ†ç±»
type TextClassifier struct {
 engine    *ONNXInferenceEngine
 tokenizer *Tokenizer
 labels    []string
}

type Tokenizer struct {
 vocab     map[string]int
 maxLength int
}

func NewTokenizer(vocabPath string, maxLength int) (*Tokenizer, error) {
 // åŠ è½½è¯è¡¨
 vocab := make(map[string]int)
 // ... ä»æ–‡ä»¶åŠ è½½vocab

 return &Tokenizer{
  vocab:     vocab,
  maxLength: maxLength,
 }, nil
}

// Tokenizeå°†æ–‡æœ¬è½¬æ¢ä¸ºtoken IDs
func (t *Tokenizer) Tokenize(text string) []int32 {
 words := strings.Fields(strings.ToLower(text))
 tokens := make([]int32, t.maxLength)

 for i, word := range words {
  if i >= t.maxLength {
   break
  }
  if id, ok := t.vocab[word]; ok {
   tokens[i] = int32(id)
  } else {
   tokens[i] = int32(t.vocab["[UNK]"]) // Unknown token
  }
 }

 return tokens
}

func NewTextClassifier(modelPath, vocabPath string, labels []string) (*TextClassifier, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 tokenizer, err := NewTokenizer(vocabPath, 128)
 if err != nil {
  return nil, err
 }

 return &TextClassifier{
  engine:    engine,
  tokenizer: tokenizer,
  labels:    labels,
 }, nil
}

// Classifyåˆ†ç±»æ–‡æœ¬
func (c *TextClassifier) Classify(text string) (string, float32, error) {
 // 1. Tokenization
 tokens := c.tokenizer.Tokenize(text)

 // 2. è½¬æ¢ä¸ºfloat32
 input := make([]float32, len(tokens))
 for i, token := range tokens {
  input[i] = float32(token)
 }

 // 3. æ¨ç†
 output, err := c.engine.Predict(input, []int64{1, int64(len(tokens))})
 if err != nil {
  return "", 0, err
 }

 // 4. Softmax + Argmax
 maxIdx := 0
 maxScore := output[0]
 for i, score := range output {
  if score > maxScore {
   maxScore = score
   maxIdx = i
  }
 }

 return c.labels[maxIdx], maxScore, nil
}

// ä½¿ç”¨ç¤ºä¾‹
func ExampleTextClassification() {
 classifier, _ := NewTextClassifier(
  "sentiment_model.onnx",
  "vocab.txt",
  []string{"negative", "neutral", "positive"},
 )

 text := "This product is amazing!"
 label, score, _ := classifier.Classify(text)

 fmt.Printf("Text: %s\n", text)
 fmt.Printf("Label: %s (%.2f)\n", label, score)
 // Output: Label: positive (0.95)
}
```

### 5.2 å‘½åå®ä½“è¯†åˆ« (NER)

```go
package nlp

// NERModelæ‰§è¡Œå‘½åå®ä½“è¯†åˆ«
type NERModel struct {
 engine    *ONNXInferenceEngine
 tokenizer *Tokenizer
 labels    []string // ["O", "B-PER", "I-PER", "B-ORG", "I-ORG", "B-LOC", "I-LOC"]
}

type Entity struct {
 Text  string
 Type  string
 Start int
 End   int
}

func (m *NERModel) ExtractEntities(text string) ([]Entity, error) {
 // 1. Tokenize
 tokens := m.tokenizer.Tokenize(text)
 input := make([]float32, len(tokens))
 for i, t := range tokens {
  input[i] = float32(t)
 }

 // 2. æ¨ç†
 output, err := m.engine.Predict(input, []int64{1, int64(len(tokens))})
 if err != nil {
  return nil, err
 }

 // 3. è§£æå®ä½“
 entities := make([]Entity, 0)
 var currentEntity *Entity

 words := strings.Fields(text)
 for i, word := range words {
  labelIdx := argmax(output[i*len(m.labels) : (i+1)*len(m.labels)])
  label := m.labels[labelIdx]

  if strings.HasPrefix(label, "B-") {
   // å¼€å§‹æ–°å®ä½“
   if currentEntity != nil {
    entities = append(entities, *currentEntity)
   }
   currentEntity = &Entity{
    Text:  word,
    Type:  label[2:],
    Start: i,
    End:   i + 1,
   }
  } else if strings.HasPrefix(label, "I-") && currentEntity != nil {
   // ç»§ç»­å½“å‰å®ä½“
   currentEntity.Text += " " + word
   currentEntity.End = i + 1
  } else {
   // "O" æ ‡ç­¾,ç»“æŸå½“å‰å®ä½“
   if currentEntity != nil {
    entities = append(entities, *currentEntity)
    currentEntity = nil
   }
  }
 }

 if currentEntity != nil {
  entities = append(entities, *currentEntity)
 }

 return entities, nil
}

func argmax(values []float32) int {
 maxIdx := 0
 maxVal := values[0]
 for i, v := range values {
  if v > maxVal {
   maxVal = v
   maxIdx = i
  }
 }
 return maxIdx
}

// ä½¿ç”¨ç¤ºä¾‹
func ExampleNER() {
 ner, _ := NewNERModel("ner_model.onnx", "vocab.txt")

 text := "Apple CEO Tim Cook announced the new iPhone in Cupertino."
 entities, _ := ner.ExtractEntities(text)

 for _, entity := range entities {
  fmt.Printf("%s (%s)\n", entity.Text, entity.Type)
 }
 // Output:
 // Apple (ORG)
 // Tim Cook (PER)
 // iPhone (PRODUCT)
 // Cupertino (LOC)
}
```

---

## 6. è®¡ç®—æœºè§†è§‰

### 6.1 å›¾åƒåˆ†ç±»

```go
package vision

import (
 "fmt"
 "image"
 "image/color"
 "os"

 "gocv.io/x/gocv"
 ort "github.com/yalue/onnxruntime_go"
)

// ImageClassifierå®ç°å›¾åƒåˆ†ç±»
type ImageClassifier struct {
 engine *ONNXInferenceEngine
 labels []string
 mean   []float32
 std    []float32
}

func NewImageClassifier(modelPath, labelsPath string) (*ImageClassifier, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 // åŠ è½½ç±»åˆ«æ ‡ç­¾
 labels, err := loadLabels(labelsPath)
 if err != nil {
  return nil, err
 }

 // ImageNet normalization
 return &ImageClassifier{
  engine: engine,
  labels: labels,
  mean:   []float32{0.485, 0.456, 0.406},
  std:    []float32{0.229, 0.224, 0.225},
 }, nil
}

// Classifyåˆ†ç±»å›¾åƒ
func (c *ImageClassifier) Classify(imagePath string) (string, float32, error) {
 // 1. åŠ è½½å›¾åƒ
 img := gocv.IMRead(imagePath, gocv.IMReadColor)
 if img.Empty() {
  return "", 0, fmt.Errorf("failed to load image")
 }
 defer img.Close()

 // 2. é¢„å¤„ç†
 input := c.preprocess(img)

 // 3. æ¨ç†
 output, err := c.engine.Predict(input, []int64{1, 3, 224, 224})
 if err != nil {
  return "", 0, err
 }

 // 4. Softmax + Top-1
 probs := softmax(output)
 maxIdx := argmax(probs)

 return c.labels[maxIdx], probs[maxIdx], nil
}

// preprocessé¢„å¤„ç†å›¾åƒ
func (c *ImageClassifier) preprocess(img gocv.Mat) []float32 {
 // 1. Resize to 224x224
 resized := gocv.NewMat()
 defer resized.Close()
 gocv.Resize(img, &resized, image.Point{X: 224, Y: 224}, 0, 0, gocv.InterpolationLinear)

 // 2. Convert BGR to RGB
 rgb := gocv.NewMat()
 defer rgb.Close()
 gocv.CvtColor(resized, &rgb, gocv.ColorBGRToRGB)

 // 3. Normalize to [0, 1]
 normalized := gocv.NewMat()
 defer normalized.Close()
 rgb.ConvertTo(&normalized, gocv.MatTypeCV32F)
 normalized.DivideFloat(255.0)

 // 4. Apply mean and std
 data := normalized.ToBytes()
 input := make([]float32, 3*224*224)

 for c := 0; c < 3; c++ {
  for h := 0; h < 224; h++ {
   for w := 0; w < 224; w++ {
    idx := c*224*224 + h*224 + w
    pixelIdx := h*224*3 + w*3 + c
    val := float32(data[pixelIdx]) / 255.0
    input[idx] = (val - c.mean[c]) / c.std[c]
   }
  }
 }

 return input
}

func softmax(logits []float32) []float32 {
 // è®¡ç®—exp(x - max(x))
 maxLogit := logits[0]
 for _, l := range logits {
  if l > maxLogit {
   maxLogit = l
  }
 }

 expSum := float32(0)
 probs := make([]float32, len(logits))
 for i, l := range logits {
  probs[i] = float32(math.Exp(float64(l - maxLogit)))
  expSum += probs[i]
 }

 // å½’ä¸€åŒ–
 for i := range probs {
  probs[i] /= expSum
 }

 return probs
}

func loadLabels(path string) ([]string, error) {
 data, err := os.ReadFile(path)
 if err != nil {
  return nil, err
 }
 return strings.Split(string(data), "\n"), nil
}
```

### 6.2 ç›®æ ‡æ£€æµ‹ (YOLO)

```go
package vision

import (
 "fmt"
 "image"

 "gocv.io/x/gocv"
)

// YOLODetectorå®ç°ç›®æ ‡æ£€æµ‹
type YOLODetector struct {
 engine     *ONNXInferenceEngine
 labels     []string
 inputSize  int
 confThresh float32
 iouThresh  float32
}

type Detection struct {
 Label      string
 Confidence float32
 Box        image.Rectangle
}

func NewYOLODetector(modelPath string) (*YOLODetector, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 return &YOLODetector{
  engine:     engine,
  inputSize:  640,
  confThresh: 0.25,
  iouThresh:  0.45,
 }, nil
}

// Detectæ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡
func (d *YOLODetector) Detect(imagePath string) ([]Detection, error) {
 // 1. åŠ è½½å›¾åƒ
 img := gocv.IMRead(imagePath, gocv.IMReadColor)
 if img.Empty() {
  return nil, fmt.Errorf("failed to load image")
 }
 defer img.Close()

 origHeight := img.Rows()
 origWidth := img.Cols()

 // 2. é¢„å¤„ç†
 input := d.preprocessYOLO(img)

 // 3. æ¨ç†
 output, err := d.engine.Predict(input, []int64{1, 3, int64(d.inputSize), int64(d.inputSize)})
 if err != nil {
  return nil, err
 }

 // 4. åå¤„ç†
 detections := d.postprocess(output, origWidth, origHeight)

 // 5. NMS (Non-Maximum Suppression)
 detections = nms(detections, d.iouThresh)

 return detections, nil
}

func (d *YOLODetector) preprocessYOLO(img gocv.Mat) []float32 {
 // Resize with padding
 resized := gocv.NewMat()
 defer resized.Close()
 gocv.Resize(img, &resized, image.Point{X: d.inputSize, Y: d.inputSize}, 0, 0, gocv.InterpolationLinear)

 // Normalize to [0, 1]
 normalized := gocv.NewMat()
 defer normalized.Close()
 resized.ConvertTo(&normalized, gocv.MatTypeCV32F)
 normalized.DivideFloat(255.0)

 // Convert to CHW format
 data := normalized.ToBytes()
 input := make([]float32, 3*d.inputSize*d.inputSize)

 for c := 0; c < 3; c++ {
  for h := 0; h < d.inputSize; h++ {
   for w := 0; w < d.inputSize; w++ {
    idx := c*d.inputSize*d.inputSize + h*d.inputSize + w
    pixelIdx := h*d.inputSize*3 + w*3 + (2 - c) // BGR to RGB
    input[idx] = float32(data[pixelIdx]) / 255.0
   }
  }
 }

 return input
}

func (d *YOLODetector) postprocess(output []float32, origWidth, origHeight int) []Detection {
 detections := make([]Detection, 0)

 // YOLO output format: [batch, num_boxes, 85]
 // 85 = 4 (bbox) + 1 (objectness) + 80 (classes)
 numBoxes := len(output) / 85

 scaleX := float32(origWidth) / float32(d.inputSize)
 scaleY := float32(origHeight) / float32(d.inputSize)

 for i := 0; i < numBoxes; i++ {
  base := i * 85

  // æå–boxå’Œconfidence
  cx := output[base+0] * scaleX
  cy := output[base+1] * scaleY
  w := output[base+2] * scaleX
  h := output[base+3] * scaleY
  objectness := output[base+4]

  if objectness < d.confThresh {
   continue
  }

  // æ‰¾åˆ°æœ€å¤§ç±»åˆ«æ¦‚ç‡
  maxClassIdx := 0
  maxClassProb := output[base+5]
  for j := 1; j < 80; j++ {
   classProb := output[base+5+j]
   if classProb > maxClassProb {
    maxClassProb = classProb
    maxClassIdx = j
   }
  }

  confidence := objectness * maxClassProb
  if confidence < d.confThresh {
   continue
  }

  // è½¬æ¢ä¸ºxyxyæ ¼å¼
  x1 := int(cx - w/2)
  y1 := int(cy - h/2)
  x2 := int(cx + w/2)
  y2 := int(cy + h/2)

  detections = append(detections, Detection{
   Label:      d.labels[maxClassIdx],
   Confidence: confidence,
   Box:        image.Rect(x1, y1, x2, y2),
  })
 }

 return detections
}

// NMS (Non-Maximum Suppression)
func nms(detections []Detection, iouThresh float32) []Detection {
 // æŒ‰confidenceé™åºæ’åº
 sort.Slice(detections, func(i, j int) bool {
  return detections[i].Confidence > detections[j].Confidence
 })

 result := make([]Detection, 0)
 suppressed := make([]bool, len(detections))

 for i := range detections {
  if suppressed[i] {
   continue
  }

  result = append(result, detections[i])

  // æŠ‘åˆ¶é‡å çš„æ£€æµ‹
  for j := i + 1; j < len(detections); j++ {
   if suppressed[j] {
    continue
   }

   iou := calculateIoU(detections[i].Box, detections[j].Box)
   if iou > iouThresh {
    suppressed[j] = true
   }
  }
 }

 return result
}

func calculateIoU(box1, box2 image.Rectangle) float32 {
 // è®¡ç®—äº¤é›†
 intersect := box1.Intersect(box2)
 intersectArea := float32(intersect.Dx() * intersect.Dy())

 // è®¡ç®—å¹¶é›†
 box1Area := float32(box1.Dx() * box1.Dy())
 box2Area := float32(box2.Dx() * box2.Dy())
 unionArea := box1Area + box2Area - intersectArea

 if unionArea == 0 {
  return 0
 }

 return intersectArea / unionArea
}
```

---

## 7. æ¨èç³»ç»Ÿ

### 7.1 ååŒè¿‡æ»¤

```go
package recommendation

import (
 "fmt"
 "sort"
)

// CollaborativeFilteringModelå®ç°ååŒè¿‡æ»¤
type CollaborativeFilteringModel struct {
 userEmbeddings map[int][]float32
 itemEmbeddings map[int][]float32
 embeddingDim   int
}

func NewCollaborativeFilteringModel(userEmb, itemEmb map[int][]float32, dim int) *CollaborativeFilteringModel {
 return &CollaborativeFilteringModel{
  userEmbeddings: userEmb,
  itemEmbeddings: itemEmb,
  embeddingDim:   dim,
 }
}

// Recommendæ¨èTopK items
func (m *CollaborativeFilteringModel) Recommend(userID int, k int, excludeItems []int) ([]int, []float32, error) {
 userEmb, ok := m.userEmbeddings[userID]
 if !ok {
  return nil, nil, fmt.Errorf("user %d not found", userID)
 }

 excludeSet := make(map[int]bool)
 for _, id := range excludeItems {
  excludeSet[id] = true
 }

 // è®¡ç®—æ‰€æœ‰itemçš„åˆ†æ•°
 type itemScore struct {
  itemID int
  score  float32
 }
 scores := make([]itemScore, 0)

 for itemID, itemEmb := range m.itemEmbeddings {
  if excludeSet[itemID] {
   continue
  }

  score := dotProduct(userEmb, itemEmb)
  scores = append(scores, itemScore{itemID: itemID, score: score})
 }

 // æ’åºå¹¶å–TopK
 sort.Slice(scores, func(i, j int) bool {
  return scores[i].score > scores[j].score
 })

 if k > len(scores) {
  k = len(scores)
 }

 itemIDs := make([]int, k)
 itemScores := make([]float32, k)
 for i := 0; i < k; i++ {
  itemIDs[i] = scores[i].itemID
  itemScores[i] = scores[i].score
 }

 return itemIDs, itemScores, nil
}

func dotProduct(a, b []float32) float32 {
 var sum float32
 for i := range a {
  sum += a[i] * b[i]
 }
 return sum
}
```

### 7.2 æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹

```go
package recommendation

// DeepFMModelå®ç°DeepFMæ¨èæ¨¡å‹
type DeepFMModel struct {
 engine *ONNXInferenceEngine
}

func NewDeepFMModel(modelPath string) (*DeepFMModel, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 return &DeepFMModel{engine: engine}, nil
}

// PredictCTRé¢„æµ‹ç‚¹å‡»ç‡
func (m *DeepFMModel) PredictCTR(features []float32) (float32, error) {
 // featuresåŒ…å«: [user_id, item_id, user_features..., item_features..., context_features...]

 output, err := m.engine.Predict(features, []int64{1, int64(len(features))})
 if err != nil {
  return 0, err
 }

 // Sigmoidæ¿€æ´»
 ctr := sigmoid(output[0])
 return ctr, nil
}

// RankItemså¯¹å€™é€‰itemsæ’åº
func (m *DeepFMModel) RankItems(userFeatures []float32, candidateItems [][]float32) ([]int, []float32, error) {
 type itemScore struct {
  idx   int
  score float32
 }

 scores := make([]itemScore, len(candidateItems))

 for i, itemFeatures := range candidateItems {
  // åˆå¹¶userå’Œitem features
  features := append(userFeatures, itemFeatures...)

  ctr, err := m.PredictCTR(features)
  if err != nil {
   return nil, nil, err
  }

  scores[i] = itemScore{idx: i, score: ctr}
 }

 // æŒ‰CTRé™åºæ’åº
 sort.Slice(scores, func(i, j int) bool {
  return scores[i].score > scores[j].score
 })

 indices := make([]int, len(scores))
 ctrs := make([]float32, len(scores))
 for i, s := range scores {
  indices[i] = s.idx
  ctrs[i] = s.score
 }

 return indices, ctrs, nil
}

func sigmoid(x float32) float32 {
 return 1.0 / (1.0 + float32(math.Exp(-float64(x))))
}
```

---

## 8. æ¨¡å‹æœåŠ¡åŒ–

### 8.1 RESTful API

```go
package serving

import (
 "encoding/json"
 "fmt"
 "net/http"

 "github.com/gin-gonic/gin"
)

// ModelServiceæä¾›REST API
type ModelService struct {
 classifier *ImageClassifier
 detector   *YOLODetector
}

func NewModelService(classifierPath, detectorPath string) (*ModelService, error) {
 classifier, err := NewImageClassifier(classifierPath, "labels.txt")
 if err != nil {
  return nil, err
 }

 detector, err := NewYOLODetector(detectorPath)
 if err != nil {
  return nil, err
 }

 return &ModelService{
  classifier: classifier,
  detector:   detector,
 }, nil
}

func (s *ModelService) SetupRoutes(r *gin.Engine) {
 api := r.Group("/api/v1")
 {
  api.POST("/classify", s.handleClassify)
  api.POST("/detect", s.handleDetect)
  api.GET("/health", s.handleHealth)
 }
}

type ClassifyRequest struct {
 ImageURL string `json:"image_url"`
}

type ClassifyResponse struct {
 Label      string  `json:"label"`
 Confidence float32 `json:"confidence"`
}

func (s *ModelService) handleClassify(c *gin.Context) {
 var req ClassifyRequest
 if err := c.ShouldBindJSON(&req); err != nil {
  c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request"})
  return
 }

 // ä¸‹è½½å›¾åƒ
 imagePath, err := downloadImage(req.ImageURL)
 if err != nil {
  c.JSON(http.StatusBadRequest, gin.H{"error": "Failed to download image"})
  return
 }
 defer os.Remove(imagePath)

 // åˆ†ç±»
 label, conf, err := s.classifier.Classify(imagePath)
 if err != nil {
  c.JSON(http.StatusInternalServerError, gin.H{"error": "Classification failed"})
  return
 }

 c.JSON(http.StatusOK, ClassifyResponse{
  Label:      label,
  Confidence: conf,
 })
}

func (s *ModelService) handleHealth(c *gin.Context) {
 c.JSON(http.StatusOK, gin.H{
  "status": "healthy",
 })
}

func main() {
 service, err := NewModelService("resnet50.onnx", "yolov8.onnx")
 if err != nil {
  log.Fatal(err)
 }

 r := gin.Default()
 service.SetupRoutes(r)

 r.Run(":8080")
}
```

### 8.2 æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

```go
package serving

import (
 "fmt"
 "sync"
)

// ModelRegistryç®¡ç†å¤šç‰ˆæœ¬æ¨¡å‹
type ModelRegistry struct {
 mu     sync.RWMutex
 models map[string]map[string]*ONNXInferenceEngine // modelName -> version -> engine
}

func NewModelRegistry() *ModelRegistry {
 return &ModelRegistry{
  models: make(map[string]map[string]*ONNXInferenceEngine),
 }
}

// RegisterModelæ³¨å†Œæ¨¡å‹ç‰ˆæœ¬
func (r *ModelRegistry) RegisterModel(name, version, path string) error {
 engine, err := NewONNXInferenceEngine(path)
 if err != nil {
  return err
 }

 r.mu.Lock()
 defer r.mu.Unlock()

 if r.models[name] == nil {
  r.models[name] = make(map[string]*ONNXInferenceEngine)
 }

 r.models[name][version] = engine
 return nil
}

// GetModelè·å–æŒ‡å®šç‰ˆæœ¬çš„æ¨¡å‹
func (r *ModelRegistry) GetModel(name, version string) (*ONNXInferenceEngine, error) {
 r.mu.RLock()
 defer r.mu.RUnlock()

 versions, ok := r.models[name]
 if !ok {
  return nil, fmt.Errorf("model %s not found", name)
 }

 engine, ok := versions[version]
 if !ok {
  return nil, fmt.Errorf("model %s version %s not found", name, version)
 }

 return engine, nil
}

// ListModelsåˆ—å‡ºæ‰€æœ‰æ¨¡å‹
func (r *ModelRegistry) ListModels() map[string][]string {
 r.mu.RLock()
 defer r.mu.RUnlock()

 result := make(map[string][]string)
 for name, versions := range r.models {
  versionList := make([]string, 0, len(versions))
  for v := range versions {
   versionList = append(versionList, v)
  }
  result[name] = versionList
 }

 return result
}

// A/B Testing
func (r *ModelRegistry) ABTest(name, versionA, versionB string, trafficSplit float32) (*ONNXInferenceEngine, error) {
 if rand.Float32() < trafficSplit {
  return r.GetModel(name, versionA)
 }
 return r.GetModel(name, versionB)
}
```

---

(ç»§ç»­ç¬¬9-10ç« ...)

## 9. æ€§èƒ½ä¼˜åŒ–

### 9.1 æ¨¡å‹é‡åŒ–

```go
// æ¨¡å‹é‡åŒ– (Quantization)
/*
ç²¾åº¦å¯¹æ¯”:
  FP32 (æµ®ç‚¹32ä½): æ ‡å‡†ç²¾åº¦
  FP16 (æµ®ç‚¹16ä½): 2x faster, 2x smaller
  INT8 (æ•´æ•°8ä½): 4x faster, 4x smaller

é‡åŒ–æ–¹æ³•:
  1. è®­ç»ƒåé‡åŒ– (Post-Training Quantization)
     â€¢ ç®€å•,æ— éœ€é‡æ–°è®­ç»ƒ
     â€¢ ç²¾åº¦å¯èƒ½ä¸‹é™0.5-2%

  2. é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (Quantization-Aware Training)
     â€¢ è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ–
     â€¢ ç²¾åº¦ä¸‹é™ <0.5%

æ€§èƒ½æå‡:
  ResNet-50 INT8 vs FP32:
    â€¢ å»¶è¿Ÿ: 50ms â†’ 15ms (3.3x faster) âœ…
    â€¢ æ¨¡å‹å¤§å°: 98MB â†’ 25MB (4x smaller) âœ…
    â€¢ ç²¾åº¦: 76.1% â†’ 75.8% (-0.3%)
*/

// ONNX Runtimeé‡åŒ–ç¤ºä¾‹
/*
Python: å¯¼å‡ºé‡åŒ–æ¨¡å‹
import onnxruntime as ort
from onnxruntime.quantization import quantize_dynamic

model_fp32 = "model.onnx"
model_int8 = "model_quantized.onnx"

quantize_dynamic(
    model_input=model_fp32,
    model_output=model_int8,
    weight_type=QuantType.QUInt8
)
*/

// Go: åŠ è½½é‡åŒ–æ¨¡å‹
func NewQuantizedEngine(modelPath string) (*ONNXInferenceEngine, error) {
 ort.SetSharedLibraryPath("onnxruntime.so")
 ort.InitializeEnvironment()

 options := ort.NewSessionOptions()
 options.SetIntraOpNumThreads(4)
 options.SetGraphOptimizationLevel(ort.AllOptimizations)

 // é‡åŒ–æ¨¡å‹æ¨ç†ä¸FP32æ¨¡å‹ç›¸åŒ
 session, err := ort.NewSession(modelPath, options)
 if err != nil {
  return nil, err
 }

 return &ONNXInferenceEngine{session: session}, nil
}
```

### 9.2 æ‰¹å¤„ç†ä¼˜åŒ–

```go
package optimization

import (
 "context"
 "sync"
 "time"
)

// AdaptiveBatcheråŠ¨æ€è°ƒæ•´æ‰¹å¤§å°
type AdaptiveBatcher struct {
 engine       *ONNXInferenceEngine
 minBatchSize int
 maxBatchSize int
 maxLatency   time.Duration

 mu        sync.Mutex
 buffer    []InferenceRequest
 results   map[string]chan InferenceResult
 batchSize int
}

func NewAdaptiveBatcher(engine *ONNXInferenceEngine, min, max int, latency time.Duration) *AdaptiveBatcher {
 ab := &AdaptiveBatcher{
  engine:       engine,
  minBatchSize: min,
  maxBatchSize: max,
  maxLatency:   latency,
  buffer:       make([]InferenceRequest, 0, max),
  results:      make(map[string]chan InferenceResult),
  batchSize:    min,
 }

 go ab.batchWorker()
 go ab.adaptBatchSize()

 return ab
}

func (ab *AdaptiveBatcher) Infer(ctx context.Context, req InferenceRequest) ([]float32, error) {
 resultChan := make(chan InferenceResult, 1)

 ab.mu.Lock()
 ab.buffer = append(ab.buffer, req)
 ab.results[req.ID] = resultChan
 shouldProcess := len(ab.buffer) >= ab.batchSize
 ab.mu.Unlock()

 if shouldProcess {
  ab.processBatch()
 }

 select {
 case result := <-resultChan:
  return result.Output, result.Error
 case <-ctx.Done():
  return nil, ctx.Err()
 }
}

func (ab *AdaptiveBatcher) adaptBatchSize() {
 ticker := time.NewTicker(10 * time.Second)
 defer ticker.Stop()

 var prevLatency time.Duration

 for range ticker.C {
  // æµ‹é‡å½“å‰å»¶è¿Ÿ
  start := time.Now()
  ab.processBatch()
  latency := time.Since(start)

  // è‡ªé€‚åº”è°ƒæ•´
  if latency < ab.maxLatency && ab.batchSize < ab.maxBatchSize {
   // å»¶è¿Ÿä½,å¢åŠ æ‰¹å¤§å°
   ab.batchSize = min(ab.batchSize+4, ab.maxBatchSize)
  } else if latency > ab.maxLatency && ab.batchSize > ab.minBatchSize {
   // å»¶è¿Ÿé«˜,å‡å°æ‰¹å¤§å°
   ab.batchSize = max(ab.batchSize-4, ab.minBatchSize)
  }

  prevLatency = latency
 }
}

func min(a, b int) int {
 if a < b {
  return a
 }
 return b
}

func max(a, b int) int {
 if a > b {
  return a
 }
 return b
}
```

---

## 10. å®Œæ•´é¡¹ç›®ç¤ºä¾‹

### 10.1 AIé©±åŠ¨çš„å†…å®¹å®¡æ ¸ç³»ç»Ÿ

```go
package main

import (
 "context"
 "fmt"
 "log"
 "net/http"

 "github.com/gin-gonic/gin"
)

// ContentModerationSystemå®ç°å¤šæ¨¡æ€å†…å®¹å®¡æ ¸
type ContentModerationSystem struct {
 textClassifier  *TextClassifier
 imageClassifier *ImageClassifier
 ner             *NERModel
 yolo            *YOLODetector
}

func NewContentModerationSystem() (*ContentModerationSystem, error) {
 textClassifier, err := NewTextClassifier("toxicity_model.onnx", "vocab.txt", []string{"safe", "toxic"})
 if err != nil {
  return nil, err
 }

 imageClassifier, err := NewImageClassifier("nsfw_model.onnx", "labels.txt")
 if err != nil {
  return nil, err
 }

 ner, err := NewNERModel("ner_model.onnx", "vocab.txt")
 if err != nil {
  return nil, err
 }

 yolo, err := NewYOLODetector("yolov8.onnx")
 if err != nil {
  return nil, err
 }

 return &ContentModerationSystem{
  textClassifier:  textClassifier,
  imageClassifier: imageClassifier,
  ner:             ner,
  yolo:            yolo,
 }, nil
}

type ModerationRequest struct {
 Text     string `json:"text"`
 ImageURL string `json:"image_url"`
}

type ModerationResponse struct {
 Safe         bool     `json:"safe"`
 TextScore    float32  `json:"text_score"`
 ImageScore   float32  `json:"image_score"`
 Violations   []string `json:"violations"`
 SensitiveInfo []string `json:"sensitive_info"`
}

func (cms *ContentModerationSystem) Moderate(ctx context.Context, req ModerationRequest) (*ModerationResponse, error) {
 response := &ModerationResponse{
  Safe:       true,
  Violations: make([]string, 0),
  SensitiveInfo: make([]string, 0),
 }

 // 1. æ–‡æœ¬å®¡æ ¸
 if req.Text != "" {
  label, score, err := cms.textClassifier.Classify(req.Text)
  if err != nil {
   return nil, err
  }

  response.TextScore = score
  if label == "toxic" && score > 0.7 {
   response.Safe = false
   response.Violations = append(response.Violations, "toxic_language")
  }

  // NER - æ£€æµ‹æ•æ„Ÿä¿¡æ¯
  entities, _ := cms.ner.ExtractEntities(req.Text)
  for _, entity := range entities {
   if entity.Type == "PER" || entity.Type == "ID" {
    response.SensitiveInfo = append(response.SensitiveInfo, entity.Text)
   }
  }
 }

 // 2. å›¾åƒå®¡æ ¸
 if req.ImageURL != "" {
  imagePath, err := downloadImage(req.ImageURL)
  if err != nil {
   return nil, err
  }
  defer os.Remove(imagePath)

  // NSFWæ£€æµ‹
  label, score, err := cms.imageClassifier.Classify(imagePath)
  if err != nil {
   return nil, err
  }

  response.ImageScore = score
  if label == "nsfw" && score > 0.8 {
   response.Safe = false
   response.Violations = append(response.Violations, "nsfw_content")
  }

  // ç›®æ ‡æ£€æµ‹ - æ£€æµ‹è¿ç¦ç‰©å“
  detections, _ := cms.yolo.Detect(imagePath)
  for _, det := range detections {
   if det.Label == "weapon" || det.Label == "drugs" {
    response.Safe = false
    response.Violations = append(response.Violations, fmt.Sprintf("detected_%s", det.Label))
   }
  }
 }

 return response, nil
}

func main() {
 system, err := NewContentModerationSystem()
 if err != nil {
  log.Fatal(err)
 }

 r := gin.Default()

 r.POST("/moderate", func(c *gin.Context) {
  var req ModerationRequest
  if err := c.ShouldBindJSON(&req); err != nil {
   c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request"})
   return
  }

  result, err := system.Moderate(c.Request.Context(), req)
  if err != nil {
   c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
   return
  }

  c.JSON(http.StatusOK, result)
 })

 log.Println("Content Moderation System running on :8080")
 r.Run(":8080")
}
```

### 10.2 ç”Ÿäº§ç¯å¢ƒé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  model-server:
    build: .
    ports:
      - "8080:8080"
    environment:
      - MODEL_PATH=/models
      - LOG_LEVEL=info
      - MAX_BATCH_SIZE=32
      - GPU_ENABLED=true
    volumes:
      - ./models:/models
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

```dockerfile
# Dockerfile
FROM golang:1.23-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=1 GOOS=linux go build -o /model-server ./cmd/server

FROM nvidia/cuda:12.0-base

# å®‰è£…ONNX Runtime
RUN apt-get update && apt-get install -y \
    libgomp1 \
    wget \
    && wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.0/onnxruntime-linux-x64-gpu-1.16.0.tgz \
    && tar -xzf onnxruntime-linux-x64-gpu-1.16.0.tgz \
    && cp onnxruntime-linux-x64-gpu-1.16.0/lib/* /usr/local/lib/ \
    && ldconfig \
    && rm -rf onnxruntime*

COPY --from=builder /model-server /usr/local/bin/

EXPOSE 8080

CMD ["model-server"]
```

---

## æ€»ç»“

### Go AI/MLæŠ€æœ¯æ ˆ

```text
1. æ¨¡å‹æ ¼å¼:
   âœ… ONNX (æ¨è)
   âœ… TensorFlow SavedModel
   âœ… PyTorch TorchScript
   âœ… TensorFlow Lite

2. æ¨ç†å¼•æ“:
   âœ… ONNX Runtime (æœ€ä½³æ€§èƒ½)
   âœ… TensorFlow C API
   âœ… LibTorch

3. åº”ç”¨åœºæ™¯:
   âœ… å›¾åƒåˆ†ç±»ä¸æ£€æµ‹
   âœ… è‡ªç„¶è¯­è¨€å¤„ç†
   âœ… æ¨èç³»ç»Ÿ
   âœ… å†…å®¹å®¡æ ¸
   âœ… æœç´¢æ’åº
```

### æ€§èƒ½ä¼˜åŠ¿

```text
Go vs Pythonæ¨ç†æ€§èƒ½:
  â€¢ å»¶è¿Ÿ: 5-10x faster
  â€¢ ååé‡: 3-5x higher
  â€¢ å†…å­˜: 2-3x smaller
  â€¢ å¹¶å‘: 10x better

é€‚åˆGoçš„åœºæ™¯:
  âœ… é«˜å¹¶å‘æ¨ç†æœåŠ¡
  âœ… ä½å»¶è¿Ÿè¦æ±‚ (<50ms)
  âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
  âœ… å¾®æœåŠ¡æ¶æ„
```

### æœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„æ¨¡å‹æ ¼å¼**: ONNXé€šç”¨æ€§æœ€ä½³
2. **æ‰¹å¤„ç†ä¼˜åŒ–**: æå‡ååé‡3-5å€
3. **æ¨¡å‹é‡åŒ–**: INT8å¯å‡å°‘75%æ¨¡å‹å¤§å°
4. **GPUåŠ é€Ÿ**: å¤§æ¨¡å‹æ¨èä½¿ç”¨GPU
5. **ç›‘æ§æŒ‡æ ‡**: å»¶è¿Ÿã€ååé‡ã€é”™è¯¯ç‡

**ç›¸å…³æ–‡æ¡£:**

- [Serverlessä¸FaaS](32-Go-1.25.3Serverlessä¸FaaSå®Œæ•´å®æˆ˜.md)
- [æ€§èƒ½ä¼˜åŒ–](../07-æ€§èƒ½ä¼˜åŒ–/10-Go-1.25.3æ€§èƒ½ä¼˜åŒ–å®Œæ•´å®æˆ˜.md)
- [å¾®æœåŠ¡æ¶æ„](09-Go-1.25.3å¾®æœåŠ¡æ¶æ„å®Œæ•´å®æˆ˜.md)
