# Go 1.25.3 AI与机器学习集成完整实战

## 📋 目录

- [Go 1.25.3 AI与机器学习集成完整实战](#go-1253-ai与机器学习集成完整实战)
  - [📋 目录](#-目录)
  - [1. AI/ML概述](#1-aiml概述)
    - [1.1 Go在AI/ML中的角色](#11-go在aiml中的角色)
    - [1.2 模型交换格式](#12-模型交换格式)
  - [2. ONNX Runtime集成](#2-onnx-runtime集成)
    - [2.1 ONNX Runtime基础](#21-onnx-runtime基础)
    - [2.2 批量推理优化](#22-批量推理优化)
    - [2.3 GPU加速](#23-gpu加速)
  - [3. TensorFlow Go](#3-tensorflow-go)
    - [3.1 TensorFlow C API集成](#31-tensorflow-c-api集成)
    - [3.2 TensorFlow Lite](#32-tensorflow-lite)
  - [4. PyTorch模型服务](#4-pytorch模型服务)
    - [4.1 TorchScript集成](#41-torchscript集成)
    - [4.2 gRPC模型服务](#42-grpc模型服务)
  - [5. 自然语言处理](#5-自然语言处理)
    - [5.1 文本分类](#51-文本分类)
    - [5.2 命名实体识别 (NER)](#52-命名实体识别-ner)
  - [6. 计算机视觉](#6-计算机视觉)
    - [6.1 图像分类](#61-图像分类)
    - [6.2 目标检测 (YOLO)](#62-目标检测-yolo)
  - [7. 推荐系统](#7-推荐系统)
    - [7.1 协同过滤](#71-协同过滤)
    - [7.2 深度学习推荐模型](#72-深度学习推荐模型)
  - [8. 模型服务化](#8-模型服务化)
    - [8.1 RESTful API](#81-restful-api)
    - [8.2 模型版本管理](#82-模型版本管理)
  - [9. 性能优化](#9-性能优化)
    - [9.1 模型量化](#91-模型量化)
    - [9.2 批处理优化](#92-批处理优化)
  - [10. 完整项目示例](#10-完整项目示例)
    - [10.1 AI驱动的内容审核系统](#101-ai驱动的内容审核系统)
    - [10.2 生产环境配置](#102-生产环境配置)
  - [总结](#总结)
    - [Go AI/ML技术栈](#go-aiml技术栈)
    - [性能优势](#性能优势)
    - [最佳实践](#最佳实践)

## 1. AI/ML概述

### 1.1 Go在AI/ML中的角色

```go
// Go在AI/ML生态中的定位
/*
训练 (Training) → Python (PyTorch, TensorFlow, scikit-learn)
    • 大量数据处理
    • 复杂模型训练
    • 超参数调优

推理 (Inference) → Go ⭐
    • 高性能服务
    • 低延迟响应
    • 并发处理
    • 易部署

Go的AI/ML优势:
✅ 1. 推理性能: 比Python快5-10倍
✅ 2. 并发能力: Goroutine处理多请求
✅ 3. 内存效率: 资源占用小
✅ 4. 部署简单: 单一二进制文件
✅ 5. 生产就绪: 稳定可靠

典型架构:
  Python (训练) → 导出模型 (ONNX/SavedModel) → Go (推理服务)
*/

// AI/ML技术栈
type AIMLStack struct {
 ModelFormat     string // "ONNX, TensorFlow SavedModel, PyTorch TorchScript"
 Runtime         string // "ONNX Runtime, TensorFlow C API, LibTorch"
 Preprocessing   string // "图像/文本预处理"
 Postprocessing  string // "结果解析与格式化"
 Serving         string // "gRPC, REST API"
 Monitoring      string // "延迟, 吞吐量, 模型性能"
}

// 应用场景
var useCases = []string{
 "图像分类与检测",
 "自然语言处理 (NLP)",
 "推荐系统",
 "异常检测",
 "时间序列预测",
 "语音识别",
 "搜索排序",
}
```

### 1.2 模型交换格式

```go
// ONNX (Open Neural Network Exchange)
/*
ONNX是跨框架的模型交换标准

支持的框架:
  • PyTorch → ONNX
  • TensorFlow → ONNX
  • scikit-learn → ONNX
  • Keras → ONNX

优势:
  ✅ 跨平台: Windows, Linux, macOS
  ✅ 跨框架: 统一格式
  ✅ 优化: 算子融合, 量化
  ✅ 硬件加速: CPU, GPU, NPU

示例: PyTorch导出ONNX
```

```python
# Python (训练与导出)
import torch
import torch.nn as nn

# 定义模型
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 2)

    def forward(self, x):
        return self.fc(x)

model = SimpleModel()
model.eval()

# 导出ONNX
dummy_input = torch.randn(1, 10)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}}
)
```

---

## 2. ONNX Runtime集成

### 2.1 ONNX Runtime基础

```go
package main

import (
 "fmt"
 "log"

 ort "github.com/yalue/onnxruntime_go"
)

// ONNXInferenceEngine封装ONNX Runtime
type ONNXInferenceEngine struct {
 session *ort.Session
 inputs  []string
 outputs []string
}

func NewONNXInferenceEngine(modelPath string) (*ONNXInferenceEngine, error) {
 // 初始化ONNX Runtime环境
 ort.SetSharedLibraryPath("onnxruntime.so") // Linux
 // ort.SetSharedLibraryPath("onnxruntime.dll") // Windows

 if err := ort.InitializeEnvironment(); err != nil {
  return nil, fmt.Errorf("init environment: %w", err)
 }

 // 加载模型
 session, err := ort.NewSession(modelPath, ort.NewSessionOptions())
 if err != nil {
  return nil, fmt.Errorf("load model: %w", err)
 }

 // 获取输入输出名称
 inputs, err := session.GetInputNames()
 if err != nil {
  return nil, err
 }

 outputs, err := session.GetOutputNames()
 if err != nil {
  return nil, err
 }

 return &ONNXInferenceEngine{
  session: session,
  inputs:  inputs,
  outputs: outputs,
 }, nil
}

// Predict执行推理
func (e *ONNXInferenceEngine) Predict(input []float32, shape []int64) ([]float32, error) {
 // 创建输入tensor
 inputTensor, err := ort.NewTensor(shape, input)
 if err != nil {
  return nil, fmt.Errorf("create input tensor: %w", err)
 }
 defer inputTensor.Destroy()

 // 运行推理
 outputs, err := e.session.Run(
  []ort.Value{inputTensor},
  e.inputs,
  e.outputs,
 )
 if err != nil {
  return nil, fmt.Errorf("run inference: %w", err)
 }
 defer outputs[0].Destroy()

 // 提取结果
 outputData := outputs[0].GetData()
 return outputData.([]float32), nil
}

func (e *ONNXInferenceEngine) Close() {
 if e.session != nil {
  e.session.Destroy()
 }
 ort.DestroyEnvironment()
}

// 使用示例
func ExampleONNXInference() {
 engine, err := NewONNXInferenceEngine("model.onnx")
 if err != nil {
  log.Fatal(err)
 }
 defer engine.Close()

 // 准备输入数据 (batch=1, features=10)
 input := make([]float32, 10)
 for i := range input {
  input[i] = float32(i) * 0.1
 }

 // 执行推理
 output, err := engine.Predict(input, []int64{1, 10})
 if err != nil {
  log.Fatal(err)
 }

 fmt.Printf("Output: %v\n", output)
}
```

### 2.2 批量推理优化

```go
package inference

import (
 "context"
 "fmt"
 "sync"
 "time"

 ort "github.com/yalue/onnxruntime_go"
)

// BatchInferenceEngine支持批量推理
type BatchInferenceEngine struct {
 engine    *ONNXInferenceEngine
 batchSize int
 timeout   time.Duration

 mu      sync.Mutex
 batch   []InferenceRequest
 results chan InferenceResult
}

type InferenceRequest struct {
 ID    string
 Data  []float32
 Shape []int64
}

type InferenceResult struct {
 ID     string
 Output []float32
 Error  error
}

func NewBatchInferenceEngine(modelPath string, batchSize int, timeout time.Duration) (*BatchInferenceEngine, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 be := &BatchInferenceEngine{
  engine:    engine,
  batchSize: batchSize,
  timeout:   timeout,
  batch:     make([]InferenceRequest, 0, batchSize),
  results:   make(chan InferenceResult, 100),
 }

 // 启动批处理worker
 go be.batchWorker()

 return be, nil
}

// Infer提交推理请求
func (be *BatchInferenceEngine) Infer(ctx context.Context, req InferenceRequest) ([]float32, error) {
 be.mu.Lock()
 be.batch = append(be.batch, req)
 batchFull := len(be.batch) >= be.batchSize
 be.mu.Unlock()

 // 如果批次满了,立即触发推理
 if batchFull {
  be.processBatch()
 }

 // 等待结果
 select {
 case result := <-be.results:
  if result.ID == req.ID {
   return result.Output, result.Error
  }
 case <-ctx.Done():
  return nil, ctx.Err()
 case <-time.After(be.timeout):
  return nil, fmt.Errorf("inference timeout")
 }

 return nil, fmt.Errorf("unexpected error")
}

func (be *BatchInferenceEngine) batchWorker() {
 ticker := time.NewTicker(be.timeout)
 defer ticker.Stop()

 for {
  <-ticker.C
  be.processBatch()
 }
}

func (be *BatchInferenceEngine) processBatch() {
 be.mu.Lock()
 if len(be.batch) == 0 {
  be.mu.Unlock()
  return
 }

 batch := be.batch
 be.batch = make([]InferenceRequest, 0, be.batchSize)
 be.mu.Unlock()

 // 合并输入
 batchData := make([]float32, 0)
 for _, req := range batch {
  batchData = append(batchData, req.Data...)
 }

 // 批量推理
 batchShape := []int64{int64(len(batch)), int64(len(batch[0].Data))}
 outputs, err := be.engine.Predict(batchData, batchShape)

 // 分发结果
 outputSize := len(outputs) / len(batch)
 for i, req := range batch {
  result := InferenceResult{
   ID:    req.ID,
   Error: err,
  }
  if err == nil {
   start := i * outputSize
   end := start + outputSize
   result.Output = outputs[start:end]
  }
  be.results <- result
 }
}
```

### 2.3 GPU加速

```go
package gpu

import (
 "fmt"

 ort "github.com/yalue/onnxruntime_go"
)

// GPUInferenceEngine使用GPU加速
type GPUInferenceEngine struct {
 session *ort.Session
}

func NewGPUInferenceEngine(modelPath string, deviceID int) (*GPUInferenceEngine, error) {
 ort.SetSharedLibraryPath("onnxruntime.so")

 if err := ort.InitializeEnvironment(); err != nil {
  return nil, err
 }

 // 配置CUDA执行提供器
 options := ort.NewSessionOptions()

 // 添加CUDA执行提供器
 cudaOptions := map[string]string{
  "device_id": fmt.Sprintf("%d", deviceID),
 }

 if err := options.AppendExecutionProviderCUDA(cudaOptions); err != nil {
  return nil, fmt.Errorf("add CUDA provider: %w", err)
 }

 // 优化选项
 options.SetIntraOpNumThreads(4)
 options.SetInterOpNumThreads(4)
 options.SetGraphOptimizationLevel(ort.AllOptimizations)

 session, err := ort.NewSession(modelPath, options)
 if err != nil {
  return nil, err
 }

 return &GPUInferenceEngine{session: session}, nil
}

// 性能对比
/*
模型: ResNet-50
输入: 224x224 图像
批大小: 32

CPU (Intel Xeon):
  • 延迟: ~500ms/batch
  • 吞吐量: ~64 images/sec

GPU (NVIDIA V100):
  • 延迟: ~50ms/batch
  • 吞吐量: ~640 images/sec
  • 加速比: 10x ✅

GPU适用场景:
  ✅ 大模型 (ResNet, BERT, GPT)
  ✅ 批量处理
  ✅ 实时视频处理
  ✅ 高吞吐量需求
*/
```

---

## 3. TensorFlow Go

### 3.1 TensorFlow C API集成

```go
package tensorflow

import (
 "fmt"
 "io/ioutil"

 tf "github.com/tensorflow/tensorflow/tensorflow/go"
 "github.com/tensorflow/tensorflow/tensorflow/go/op"
)

// TFInferenceEngine封装TensorFlow推理
type TFInferenceEngine struct {
 model   *tf.SavedModel
 session *tf.Session
}

func NewTFInferenceEngine(modelDir string) (*TFInferenceEngine, error) {
 // 加载SavedModel
 model, err := tf.LoadSavedModel(modelDir, []string{"serve"}, nil)
 if err != nil {
  return nil, fmt.Errorf("load model: %w", err)
 }

 return &TFInferenceEngine{
  model:   model,
  session: model.Session,
 }, nil
}

// Predict执行推理
func (e *TFInferenceEngine) Predict(input [][]float32) ([][]float32, error) {
 // 创建输入tensor
 inputTensor, err := tf.NewTensor(input)
 if err != nil {
  return nil, fmt.Errorf("create tensor: %w", err)
 }

 // 运行推理
 feeds := map[tf.Output]*tf.Tensor{
  e.model.Graph.Operation("serving_default_input").Output(0): inputTensor,
 }

 fetches := []tf.Output{
  e.model.Graph.Operation("StatefulPartitionedCall").Output(0),
 }

 results, err := e.session.Run(feeds, fetches, nil)
 if err != nil {
  return nil, fmt.Errorf("run inference: %w", err)
 }

 // 提取结果
 output := results[0].Value().([][]float32)
 return output, nil
}

func (e *TFInferenceEngine) Close() {
 if e.session != nil {
  e.session.Close()
 }
}
```

### 3.2 TensorFlow Lite

```go
package tflite

import (
 "fmt"

 "github.com/mattn/go-tflite"
)

// TFLiteEngine用于移动/嵌入式设备
type TFLiteEngine struct {
 model      *tflite.Model
 interpreter *tflite.Interpreter
}

func NewTFLiteEngine(modelPath string) (*TFLiteEngine, error) {
 // 加载TFLite模型
 model := tflite.NewModelFromFile(modelPath)
 if model == nil {
  return nil, fmt.Errorf("failed to load model")
 }

 // 创建解释器
 options := tflite.NewInterpreterOptions()
 options.SetNumThread(4)

 interpreter := tflite.NewInterpreter(model, options)
 if interpreter == nil {
  return nil, fmt.Errorf("failed to create interpreter")
 }

 // 分配tensors
 if status := interpreter.AllocateTensors(); status != tflite.OK {
  return nil, fmt.Errorf("allocate tensors failed")
 }

 return &TFLiteEngine{
  model:      model,
  interpreter: interpreter,
 }, nil
}

// Infer执行推理
func (e *TFLiteEngine) Infer(input []float32) ([]float32, error) {
 // 设置输入
 inputTensor := e.interpreter.GetInputTensor(0)
 copy(inputTensor.Float32s(), input)

 // 执行推理
 if status := e.interpreter.Invoke(); status != tflite.OK {
  return nil, fmt.Errorf("invoke failed")
 }

 // 获取输出
 outputTensor := e.interpreter.GetOutputTensor(0)
 output := make([]float32, outputTensor.Dim(1))
 copy(output, outputTensor.Float32s())

 return output, nil
}

// TFLite优势
/*
模型大小对比:
  TensorFlow SavedModel: 100MB
  TFLite: 25MB (4x smaller) ✅

延迟对比 (移动CPU):
  TensorFlow: 200ms
  TFLite: 50ms (4x faster) ✅

适用场景:
  ✅ 移动应用 (Android, iOS)
  ✅ 嵌入式设备 (树莓派, Jetson Nano)
  ✅ 边缘计算
  ✅ 低功耗场景
*/
```

---

## 4. PyTorch模型服务

### 4.1 TorchScript集成

```go
package pytorch

import (
 "fmt"

 torch "github.com/wangkuiyi/gotorch"
)

// PyTorchEngine加载TorchScript模型
type PyTorchEngine struct {
 module torch.Module
}

func NewPyTorchEngine(modelPath string) (*PyTorchEngine, error) {
 // 加载TorchScript模型
 module, err := torch.Load(modelPath)
 if err != nil {
  return nil, fmt.Errorf("load model: %w", err)
 }

 // 设置为评估模式
 module.Eval()

 return &PyTorchEngine{module: module}, nil
}

// Forward执行前向传播
func (e *PyTorchEngine) Forward(input []float32, shape []int64) ([]float32, error) {
 // 创建tensor
 inputTensor := torch.NewTensor(input, shape)

 // 执行推理 (no_grad模式)
 var output torch.Tensor
 torch.NoGrad(func() {
  output = e.module.Forward(inputTensor)
 })

 // 提取数据
 outputData := output.Float32Value()
 return outputData, nil
}
```

```python
# Python: 导出TorchScript
import torch

# 定义模型
class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(10, 5)

    def forward(self, x):
        return self.linear(x)

model = MyModel()
model.eval()

# 方法1: Tracing
example_input = torch.rand(1, 10)
traced_model = torch.jit.trace(model, example_input)
traced_model.save("model_traced.pt")

# 方法2: Scripting
scripted_model = torch.jit.script(model)
scripted_model.save("model_scripted.pt")
```

### 4.2 gRPC模型服务

```go
package serving

import (
 "context"
 "fmt"

 pb "your_project/proto"
)

// ModelServer实现gRPC服务
type ModelServer struct {
 pb.UnimplementedModelServiceServer
 engine *PyTorchEngine
}

func NewModelServer(modelPath string) (*ModelServer, error) {
 engine, err := NewPyTorchEngine(modelPath)
 if err != nil {
  return nil, err
 }

 return &ModelServer{engine: engine}, nil
}

// Predict处理推理请求
func (s *ModelServer) Predict(ctx context.Context, req *pb.PredictRequest) (*pb.PredictResponse, error) {
 // 执行推理
 output, err := s.engine.Forward(req.Input, req.Shape)
 if err != nil {
  return nil, fmt.Errorf("inference failed: %w", err)
 }

 return &pb.PredictResponse{
  Output: output,
 }, nil
}
```

```protobuf
// proto/model_service.proto
syntax = "proto3";

package model;

option go_package = "your_project/proto";

service ModelService {
  rpc Predict(PredictRequest) returns (PredictResponse);
  rpc BatchPredict(BatchPredictRequest) returns (BatchPredictResponse);
}

message PredictRequest {
  repeated float input = 1;
  repeated int64 shape = 2;
}

message PredictResponse {
  repeated float output = 1;
}

message BatchPredictRequest {
  repeated PredictRequest requests = 1;
}

message BatchPredictResponse {
  repeated PredictResponse responses = 1;
}
```

---

## 5. 自然语言处理

### 5.1 文本分类

```go
package nlp

import (
 "fmt"
 "strings"

 ort "github.com/yalue/onnxruntime_go"
)

// TextClassifier实现文本分类
type TextClassifier struct {
 engine    *ONNXInferenceEngine
 tokenizer *Tokenizer
 labels    []string
}

type Tokenizer struct {
 vocab     map[string]int
 maxLength int
}

func NewTokenizer(vocabPath string, maxLength int) (*Tokenizer, error) {
 // 加载词表
 vocab := make(map[string]int)
 // ... 从文件加载vocab

 return &Tokenizer{
  vocab:     vocab,
  maxLength: maxLength,
 }, nil
}

// Tokenize将文本转换为token IDs
func (t *Tokenizer) Tokenize(text string) []int32 {
 words := strings.Fields(strings.ToLower(text))
 tokens := make([]int32, t.maxLength)

 for i, word := range words {
  if i >= t.maxLength {
   break
  }
  if id, ok := t.vocab[word]; ok {
   tokens[i] = int32(id)
  } else {
   tokens[i] = int32(t.vocab["[UNK]"]) // Unknown token
  }
 }

 return tokens
}

func NewTextClassifier(modelPath, vocabPath string, labels []string) (*TextClassifier, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 tokenizer, err := NewTokenizer(vocabPath, 128)
 if err != nil {
  return nil, err
 }

 return &TextClassifier{
  engine:    engine,
  tokenizer: tokenizer,
  labels:    labels,
 }, nil
}

// Classify分类文本
func (c *TextClassifier) Classify(text string) (string, float32, error) {
 // 1. Tokenization
 tokens := c.tokenizer.Tokenize(text)

 // 2. 转换为float32
 input := make([]float32, len(tokens))
 for i, token := range tokens {
  input[i] = float32(token)
 }

 // 3. 推理
 output, err := c.engine.Predict(input, []int64{1, int64(len(tokens))})
 if err != nil {
  return "", 0, err
 }

 // 4. Softmax + Argmax
 maxIdx := 0
 maxScore := output[0]
 for i, score := range output {
  if score > maxScore {
   maxScore = score
   maxIdx = i
  }
 }

 return c.labels[maxIdx], maxScore, nil
}

// 使用示例
func ExampleTextClassification() {
 classifier, _ := NewTextClassifier(
  "sentiment_model.onnx",
  "vocab.txt",
  []string{"negative", "neutral", "positive"},
 )

 text := "This product is amazing!"
 label, score, _ := classifier.Classify(text)

 fmt.Printf("Text: %s\n", text)
 fmt.Printf("Label: %s (%.2f)\n", label, score)
 // Output: Label: positive (0.95)
}
```

### 5.2 命名实体识别 (NER)

```go
package nlp

// NERModel执行命名实体识别
type NERModel struct {
 engine    *ONNXInferenceEngine
 tokenizer *Tokenizer
 labels    []string // ["O", "B-PER", "I-PER", "B-ORG", "I-ORG", "B-LOC", "I-LOC"]
}

type Entity struct {
 Text  string
 Type  string
 Start int
 End   int
}

func (m *NERModel) ExtractEntities(text string) ([]Entity, error) {
 // 1. Tokenize
 tokens := m.tokenizer.Tokenize(text)
 input := make([]float32, len(tokens))
 for i, t := range tokens {
  input[i] = float32(t)
 }

 // 2. 推理
 output, err := m.engine.Predict(input, []int64{1, int64(len(tokens))})
 if err != nil {
  return nil, err
 }

 // 3. 解析实体
 entities := make([]Entity, 0)
 var currentEntity *Entity

 words := strings.Fields(text)
 for i, word := range words {
  labelIdx := argmax(output[i*len(m.labels) : (i+1)*len(m.labels)])
  label := m.labels[labelIdx]

  if strings.HasPrefix(label, "B-") {
   // 开始新实体
   if currentEntity != nil {
    entities = append(entities, *currentEntity)
   }
   currentEntity = &Entity{
    Text:  word,
    Type:  label[2:],
    Start: i,
    End:   i + 1,
   }
  } else if strings.HasPrefix(label, "I-") && currentEntity != nil {
   // 继续当前实体
   currentEntity.Text += " " + word
   currentEntity.End = i + 1
  } else {
   // "O" 标签,结束当前实体
   if currentEntity != nil {
    entities = append(entities, *currentEntity)
    currentEntity = nil
   }
  }
 }

 if currentEntity != nil {
  entities = append(entities, *currentEntity)
 }

 return entities, nil
}

func argmax(values []float32) int {
 maxIdx := 0
 maxVal := values[0]
 for i, v := range values {
  if v > maxVal {
   maxVal = v
   maxIdx = i
  }
 }
 return maxIdx
}

// 使用示例
func ExampleNER() {
 ner, _ := NewNERModel("ner_model.onnx", "vocab.txt")

 text := "Apple CEO Tim Cook announced the new iPhone in Cupertino."
 entities, _ := ner.ExtractEntities(text)

 for _, entity := range entities {
  fmt.Printf("%s (%s)\n", entity.Text, entity.Type)
 }
 // Output:
 // Apple (ORG)
 // Tim Cook (PER)
 // iPhone (PRODUCT)
 // Cupertino (LOC)
}
```

---

## 6. 计算机视觉

### 6.1 图像分类

```go
package vision

import (
 "fmt"
 "image"
 "image/color"
 "os"

 "gocv.io/x/gocv"
 ort "github.com/yalue/onnxruntime_go"
)

// ImageClassifier实现图像分类
type ImageClassifier struct {
 engine *ONNXInferenceEngine
 labels []string
 mean   []float32
 std    []float32
}

func NewImageClassifier(modelPath, labelsPath string) (*ImageClassifier, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 // 加载类别标签
 labels, err := loadLabels(labelsPath)
 if err != nil {
  return nil, err
 }

 // ImageNet normalization
 return &ImageClassifier{
  engine: engine,
  labels: labels,
  mean:   []float32{0.485, 0.456, 0.406},
  std:    []float32{0.229, 0.224, 0.225},
 }, nil
}

// Classify分类图像
func (c *ImageClassifier) Classify(imagePath string) (string, float32, error) {
 // 1. 加载图像
 img := gocv.IMRead(imagePath, gocv.IMReadColor)
 if img.Empty() {
  return "", 0, fmt.Errorf("failed to load image")
 }
 defer img.Close()

 // 2. 预处理
 input := c.preprocess(img)

 // 3. 推理
 output, err := c.engine.Predict(input, []int64{1, 3, 224, 224})
 if err != nil {
  return "", 0, err
 }

 // 4. Softmax + Top-1
 probs := softmax(output)
 maxIdx := argmax(probs)

 return c.labels[maxIdx], probs[maxIdx], nil
}

// preprocess预处理图像
func (c *ImageClassifier) preprocess(img gocv.Mat) []float32 {
 // 1. Resize to 224x224
 resized := gocv.NewMat()
 defer resized.Close()
 gocv.Resize(img, &resized, image.Point{X: 224, Y: 224}, 0, 0, gocv.InterpolationLinear)

 // 2. Convert BGR to RGB
 rgb := gocv.NewMat()
 defer rgb.Close()
 gocv.CvtColor(resized, &rgb, gocv.ColorBGRToRGB)

 // 3. Normalize to [0, 1]
 normalized := gocv.NewMat()
 defer normalized.Close()
 rgb.ConvertTo(&normalized, gocv.MatTypeCV32F)
 normalized.DivideFloat(255.0)

 // 4. Apply mean and std
 data := normalized.ToBytes()
 input := make([]float32, 3*224*224)

 for c := 0; c < 3; c++ {
  for h := 0; h < 224; h++ {
   for w := 0; w < 224; w++ {
    idx := c*224*224 + h*224 + w
    pixelIdx := h*224*3 + w*3 + c
    val := float32(data[pixelIdx]) / 255.0
    input[idx] = (val - c.mean[c]) / c.std[c]
   }
  }
 }

 return input
}

func softmax(logits []float32) []float32 {
 // 计算exp(x - max(x))
 maxLogit := logits[0]
 for _, l := range logits {
  if l > maxLogit {
   maxLogit = l
  }
 }

 expSum := float32(0)
 probs := make([]float32, len(logits))
 for i, l := range logits {
  probs[i] = float32(math.Exp(float64(l - maxLogit)))
  expSum += probs[i]
 }

 // 归一化
 for i := range probs {
  probs[i] /= expSum
 }

 return probs
}

func loadLabels(path string) ([]string, error) {
 data, err := os.ReadFile(path)
 if err != nil {
  return nil, err
 }
 return strings.Split(string(data), "\n"), nil
}
```

### 6.2 目标检测 (YOLO)

```go
package vision

import (
 "fmt"
 "image"

 "gocv.io/x/gocv"
)

// YOLODetector实现目标检测
type YOLODetector struct {
 engine     *ONNXInferenceEngine
 labels     []string
 inputSize  int
 confThresh float32
 iouThresh  float32
}

type Detection struct {
 Label      string
 Confidence float32
 Box        image.Rectangle
}

func NewYOLODetector(modelPath string) (*YOLODetector, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 return &YOLODetector{
  engine:     engine,
  inputSize:  640,
  confThresh: 0.25,
  iouThresh:  0.45,
 }, nil
}

// Detect检测图像中的对象
func (d *YOLODetector) Detect(imagePath string) ([]Detection, error) {
 // 1. 加载图像
 img := gocv.IMRead(imagePath, gocv.IMReadColor)
 if img.Empty() {
  return nil, fmt.Errorf("failed to load image")
 }
 defer img.Close()

 origHeight := img.Rows()
 origWidth := img.Cols()

 // 2. 预处理
 input := d.preprocessYOLO(img)

 // 3. 推理
 output, err := d.engine.Predict(input, []int64{1, 3, int64(d.inputSize), int64(d.inputSize)})
 if err != nil {
  return nil, err
 }

 // 4. 后处理
 detections := d.postprocess(output, origWidth, origHeight)

 // 5. NMS (Non-Maximum Suppression)
 detections = nms(detections, d.iouThresh)

 return detections, nil
}

func (d *YOLODetector) preprocessYOLO(img gocv.Mat) []float32 {
 // Resize with padding
 resized := gocv.NewMat()
 defer resized.Close()
 gocv.Resize(img, &resized, image.Point{X: d.inputSize, Y: d.inputSize}, 0, 0, gocv.InterpolationLinear)

 // Normalize to [0, 1]
 normalized := gocv.NewMat()
 defer normalized.Close()
 resized.ConvertTo(&normalized, gocv.MatTypeCV32F)
 normalized.DivideFloat(255.0)

 // Convert to CHW format
 data := normalized.ToBytes()
 input := make([]float32, 3*d.inputSize*d.inputSize)

 for c := 0; c < 3; c++ {
  for h := 0; h < d.inputSize; h++ {
   for w := 0; w < d.inputSize; w++ {
    idx := c*d.inputSize*d.inputSize + h*d.inputSize + w
    pixelIdx := h*d.inputSize*3 + w*3 + (2 - c) // BGR to RGB
    input[idx] = float32(data[pixelIdx]) / 255.0
   }
  }
 }

 return input
}

func (d *YOLODetector) postprocess(output []float32, origWidth, origHeight int) []Detection {
 detections := make([]Detection, 0)

 // YOLO output format: [batch, num_boxes, 85]
 // 85 = 4 (bbox) + 1 (objectness) + 80 (classes)
 numBoxes := len(output) / 85

 scaleX := float32(origWidth) / float32(d.inputSize)
 scaleY := float32(origHeight) / float32(d.inputSize)

 for i := 0; i < numBoxes; i++ {
  base := i * 85

  // 提取box和confidence
  cx := output[base+0] * scaleX
  cy := output[base+1] * scaleY
  w := output[base+2] * scaleX
  h := output[base+3] * scaleY
  objectness := output[base+4]

  if objectness < d.confThresh {
   continue
  }

  // 找到最大类别概率
  maxClassIdx := 0
  maxClassProb := output[base+5]
  for j := 1; j < 80; j++ {
   classProb := output[base+5+j]
   if classProb > maxClassProb {
    maxClassProb = classProb
    maxClassIdx = j
   }
  }

  confidence := objectness * maxClassProb
  if confidence < d.confThresh {
   continue
  }

  // 转换为xyxy格式
  x1 := int(cx - w/2)
  y1 := int(cy - h/2)
  x2 := int(cx + w/2)
  y2 := int(cy + h/2)

  detections = append(detections, Detection{
   Label:      d.labels[maxClassIdx],
   Confidence: confidence,
   Box:        image.Rect(x1, y1, x2, y2),
  })
 }

 return detections
}

// NMS (Non-Maximum Suppression)
func nms(detections []Detection, iouThresh float32) []Detection {
 // 按confidence降序排序
 sort.Slice(detections, func(i, j int) bool {
  return detections[i].Confidence > detections[j].Confidence
 })

 result := make([]Detection, 0)
 suppressed := make([]bool, len(detections))

 for i := range detections {
  if suppressed[i] {
   continue
  }

  result = append(result, detections[i])

  // 抑制重叠的检测
  for j := i + 1; j < len(detections); j++ {
   if suppressed[j] {
    continue
   }

   iou := calculateIoU(detections[i].Box, detections[j].Box)
   if iou > iouThresh {
    suppressed[j] = true
   }
  }
 }

 return result
}

func calculateIoU(box1, box2 image.Rectangle) float32 {
 // 计算交集
 intersect := box1.Intersect(box2)
 intersectArea := float32(intersect.Dx() * intersect.Dy())

 // 计算并集
 box1Area := float32(box1.Dx() * box1.Dy())
 box2Area := float32(box2.Dx() * box2.Dy())
 unionArea := box1Area + box2Area - intersectArea

 if unionArea == 0 {
  return 0
 }

 return intersectArea / unionArea
}
```

---

## 7. 推荐系统

### 7.1 协同过滤

```go
package recommendation

import (
 "fmt"
 "sort"
)

// CollaborativeFilteringModel实现协同过滤
type CollaborativeFilteringModel struct {
 userEmbeddings map[int][]float32
 itemEmbeddings map[int][]float32
 embeddingDim   int
}

func NewCollaborativeFilteringModel(userEmb, itemEmb map[int][]float32, dim int) *CollaborativeFilteringModel {
 return &CollaborativeFilteringModel{
  userEmbeddings: userEmb,
  itemEmbeddings: itemEmb,
  embeddingDim:   dim,
 }
}

// Recommend推荐TopK items
func (m *CollaborativeFilteringModel) Recommend(userID int, k int, excludeItems []int) ([]int, []float32, error) {
 userEmb, ok := m.userEmbeddings[userID]
 if !ok {
  return nil, nil, fmt.Errorf("user %d not found", userID)
 }

 excludeSet := make(map[int]bool)
 for _, id := range excludeItems {
  excludeSet[id] = true
 }

 // 计算所有item的分数
 type itemScore struct {
  itemID int
  score  float32
 }
 scores := make([]itemScore, 0)

 for itemID, itemEmb := range m.itemEmbeddings {
  if excludeSet[itemID] {
   continue
  }

  score := dotProduct(userEmb, itemEmb)
  scores = append(scores, itemScore{itemID: itemID, score: score})
 }

 // 排序并取TopK
 sort.Slice(scores, func(i, j int) bool {
  return scores[i].score > scores[j].score
 })

 if k > len(scores) {
  k = len(scores)
 }

 itemIDs := make([]int, k)
 itemScores := make([]float32, k)
 for i := 0; i < k; i++ {
  itemIDs[i] = scores[i].itemID
  itemScores[i] = scores[i].score
 }

 return itemIDs, itemScores, nil
}

func dotProduct(a, b []float32) float32 {
 var sum float32
 for i := range a {
  sum += a[i] * b[i]
 }
 return sum
}
```

### 7.2 深度学习推荐模型

```go
package recommendation

// DeepFMModel实现DeepFM推荐模型
type DeepFMModel struct {
 engine *ONNXInferenceEngine
}

func NewDeepFMModel(modelPath string) (*DeepFMModel, error) {
 engine, err := NewONNXInferenceEngine(modelPath)
 if err != nil {
  return nil, err
 }

 return &DeepFMModel{engine: engine}, nil
}

// PredictCTR预测点击率
func (m *DeepFMModel) PredictCTR(features []float32) (float32, error) {
 // features包含: [user_id, item_id, user_features..., item_features..., context_features...]

 output, err := m.engine.Predict(features, []int64{1, int64(len(features))})
 if err != nil {
  return 0, err
 }

 // Sigmoid激活
 ctr := sigmoid(output[0])
 return ctr, nil
}

// RankItems对候选items排序
func (m *DeepFMModel) RankItems(userFeatures []float32, candidateItems [][]float32) ([]int, []float32, error) {
 type itemScore struct {
  idx   int
  score float32
 }

 scores := make([]itemScore, len(candidateItems))

 for i, itemFeatures := range candidateItems {
  // 合并user和item features
  features := append(userFeatures, itemFeatures...)

  ctr, err := m.PredictCTR(features)
  if err != nil {
   return nil, nil, err
  }

  scores[i] = itemScore{idx: i, score: ctr}
 }

 // 按CTR降序排序
 sort.Slice(scores, func(i, j int) bool {
  return scores[i].score > scores[j].score
 })

 indices := make([]int, len(scores))
 ctrs := make([]float32, len(scores))
 for i, s := range scores {
  indices[i] = s.idx
  ctrs[i] = s.score
 }

 return indices, ctrs, nil
}

func sigmoid(x float32) float32 {
 return 1.0 / (1.0 + float32(math.Exp(-float64(x))))
}
```

---

## 8. 模型服务化

### 8.1 RESTful API

```go
package serving

import (
 "encoding/json"
 "fmt"
 "net/http"

 "github.com/gin-gonic/gin"
)

// ModelService提供REST API
type ModelService struct {
 classifier *ImageClassifier
 detector   *YOLODetector
}

func NewModelService(classifierPath, detectorPath string) (*ModelService, error) {
 classifier, err := NewImageClassifier(classifierPath, "labels.txt")
 if err != nil {
  return nil, err
 }

 detector, err := NewYOLODetector(detectorPath)
 if err != nil {
  return nil, err
 }

 return &ModelService{
  classifier: classifier,
  detector:   detector,
 }, nil
}

func (s *ModelService) SetupRoutes(r *gin.Engine) {
 api := r.Group("/api/v1")
 {
  api.POST("/classify", s.handleClassify)
  api.POST("/detect", s.handleDetect)
  api.GET("/health", s.handleHealth)
 }
}

type ClassifyRequest struct {
 ImageURL string `json:"image_url"`
}

type ClassifyResponse struct {
 Label      string  `json:"label"`
 Confidence float32 `json:"confidence"`
}

func (s *ModelService) handleClassify(c *gin.Context) {
 var req ClassifyRequest
 if err := c.ShouldBindJSON(&req); err != nil {
  c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request"})
  return
 }

 // 下载图像
 imagePath, err := downloadImage(req.ImageURL)
 if err != nil {
  c.JSON(http.StatusBadRequest, gin.H{"error": "Failed to download image"})
  return
 }
 defer os.Remove(imagePath)

 // 分类
 label, conf, err := s.classifier.Classify(imagePath)
 if err != nil {
  c.JSON(http.StatusInternalServerError, gin.H{"error": "Classification failed"})
  return
 }

 c.JSON(http.StatusOK, ClassifyResponse{
  Label:      label,
  Confidence: conf,
 })
}

func (s *ModelService) handleHealth(c *gin.Context) {
 c.JSON(http.StatusOK, gin.H{
  "status": "healthy",
 })
}

func main() {
 service, err := NewModelService("resnet50.onnx", "yolov8.onnx")
 if err != nil {
  log.Fatal(err)
 }

 r := gin.Default()
 service.SetupRoutes(r)

 r.Run(":8080")
}
```

### 8.2 模型版本管理

```go
package serving

import (
 "fmt"
 "sync"
)

// ModelRegistry管理多版本模型
type ModelRegistry struct {
 mu     sync.RWMutex
 models map[string]map[string]*ONNXInferenceEngine // modelName -> version -> engine
}

func NewModelRegistry() *ModelRegistry {
 return &ModelRegistry{
  models: make(map[string]map[string]*ONNXInferenceEngine),
 }
}

// RegisterModel注册模型版本
func (r *ModelRegistry) RegisterModel(name, version, path string) error {
 engine, err := NewONNXInferenceEngine(path)
 if err != nil {
  return err
 }

 r.mu.Lock()
 defer r.mu.Unlock()

 if r.models[name] == nil {
  r.models[name] = make(map[string]*ONNXInferenceEngine)
 }

 r.models[name][version] = engine
 return nil
}

// GetModel获取指定版本的模型
func (r *ModelRegistry) GetModel(name, version string) (*ONNXInferenceEngine, error) {
 r.mu.RLock()
 defer r.mu.RUnlock()

 versions, ok := r.models[name]
 if !ok {
  return nil, fmt.Errorf("model %s not found", name)
 }

 engine, ok := versions[version]
 if !ok {
  return nil, fmt.Errorf("model %s version %s not found", name, version)
 }

 return engine, nil
}

// ListModels列出所有模型
func (r *ModelRegistry) ListModels() map[string][]string {
 r.mu.RLock()
 defer r.mu.RUnlock()

 result := make(map[string][]string)
 for name, versions := range r.models {
  versionList := make([]string, 0, len(versions))
  for v := range versions {
   versionList = append(versionList, v)
  }
  result[name] = versionList
 }

 return result
}

// A/B Testing
func (r *ModelRegistry) ABTest(name, versionA, versionB string, trafficSplit float32) (*ONNXInferenceEngine, error) {
 if rand.Float32() < trafficSplit {
  return r.GetModel(name, versionA)
 }
 return r.GetModel(name, versionB)
}
```

---

(继续第9-10章...)

## 9. 性能优化

### 9.1 模型量化

```go
// 模型量化 (Quantization)
/*
精度对比:
  FP32 (浮点32位): 标准精度
  FP16 (浮点16位): 2x faster, 2x smaller
  INT8 (整数8位): 4x faster, 4x smaller

量化方法:
  1. 训练后量化 (Post-Training Quantization)
     • 简单,无需重新训练
     • 精度可能下降0.5-2%

  2. 量化感知训练 (Quantization-Aware Training)
     • 训练时模拟量化
     • 精度下降 <0.5%

性能提升:
  ResNet-50 INT8 vs FP32:
    • 延迟: 50ms → 15ms (3.3x faster) ✅
    • 模型大小: 98MB → 25MB (4x smaller) ✅
    • 精度: 76.1% → 75.8% (-0.3%)
*/

// ONNX Runtime量化示例
/*
Python: 导出量化模型
import onnxruntime as ort
from onnxruntime.quantization import quantize_dynamic

model_fp32 = "model.onnx"
model_int8 = "model_quantized.onnx"

quantize_dynamic(
    model_input=model_fp32,
    model_output=model_int8,
    weight_type=QuantType.QUInt8
)
*/

// Go: 加载量化模型
func NewQuantizedEngine(modelPath string) (*ONNXInferenceEngine, error) {
 ort.SetSharedLibraryPath("onnxruntime.so")
 ort.InitializeEnvironment()

 options := ort.NewSessionOptions()
 options.SetIntraOpNumThreads(4)
 options.SetGraphOptimizationLevel(ort.AllOptimizations)

 // 量化模型推理与FP32模型相同
 session, err := ort.NewSession(modelPath, options)
 if err != nil {
  return nil, err
 }

 return &ONNXInferenceEngine{session: session}, nil
}
```

### 9.2 批处理优化

```go
package optimization

import (
 "context"
 "sync"
 "time"
)

// AdaptiveBatcher动态调整批大小
type AdaptiveBatcher struct {
 engine       *ONNXInferenceEngine
 minBatchSize int
 maxBatchSize int
 maxLatency   time.Duration

 mu        sync.Mutex
 buffer    []InferenceRequest
 results   map[string]chan InferenceResult
 batchSize int
}

func NewAdaptiveBatcher(engine *ONNXInferenceEngine, min, max int, latency time.Duration) *AdaptiveBatcher {
 ab := &AdaptiveBatcher{
  engine:       engine,
  minBatchSize: min,
  maxBatchSize: max,
  maxLatency:   latency,
  buffer:       make([]InferenceRequest, 0, max),
  results:      make(map[string]chan InferenceResult),
  batchSize:    min,
 }

 go ab.batchWorker()
 go ab.adaptBatchSize()

 return ab
}

func (ab *AdaptiveBatcher) Infer(ctx context.Context, req InferenceRequest) ([]float32, error) {
 resultChan := make(chan InferenceResult, 1)

 ab.mu.Lock()
 ab.buffer = append(ab.buffer, req)
 ab.results[req.ID] = resultChan
 shouldProcess := len(ab.buffer) >= ab.batchSize
 ab.mu.Unlock()

 if shouldProcess {
  ab.processBatch()
 }

 select {
 case result := <-resultChan:
  return result.Output, result.Error
 case <-ctx.Done():
  return nil, ctx.Err()
 }
}

func (ab *AdaptiveBatcher) adaptBatchSize() {
 ticker := time.NewTicker(10 * time.Second)
 defer ticker.Stop()

 var prevLatency time.Duration

 for range ticker.C {
  // 测量当前延迟
  start := time.Now()
  ab.processBatch()
  latency := time.Since(start)

  // 自适应调整
  if latency < ab.maxLatency && ab.batchSize < ab.maxBatchSize {
   // 延迟低,增加批大小
   ab.batchSize = min(ab.batchSize+4, ab.maxBatchSize)
  } else if latency > ab.maxLatency && ab.batchSize > ab.minBatchSize {
   // 延迟高,减小批大小
   ab.batchSize = max(ab.batchSize-4, ab.minBatchSize)
  }

  prevLatency = latency
 }
}

func min(a, b int) int {
 if a < b {
  return a
 }
 return b
}

func max(a, b int) int {
 if a > b {
  return a
 }
 return b
}
```

---

## 10. 完整项目示例

### 10.1 AI驱动的内容审核系统

```go
package main

import (
 "context"
 "fmt"
 "log"
 "net/http"

 "github.com/gin-gonic/gin"
)

// ContentModerationSystem实现多模态内容审核
type ContentModerationSystem struct {
 textClassifier  *TextClassifier
 imageClassifier *ImageClassifier
 ner             *NERModel
 yolo            *YOLODetector
}

func NewContentModerationSystem() (*ContentModerationSystem, error) {
 textClassifier, err := NewTextClassifier("toxicity_model.onnx", "vocab.txt", []string{"safe", "toxic"})
 if err != nil {
  return nil, err
 }

 imageClassifier, err := NewImageClassifier("nsfw_model.onnx", "labels.txt")
 if err != nil {
  return nil, err
 }

 ner, err := NewNERModel("ner_model.onnx", "vocab.txt")
 if err != nil {
  return nil, err
 }

 yolo, err := NewYOLODetector("yolov8.onnx")
 if err != nil {
  return nil, err
 }

 return &ContentModerationSystem{
  textClassifier:  textClassifier,
  imageClassifier: imageClassifier,
  ner:             ner,
  yolo:            yolo,
 }, nil
}

type ModerationRequest struct {
 Text     string `json:"text"`
 ImageURL string `json:"image_url"`
}

type ModerationResponse struct {
 Safe         bool     `json:"safe"`
 TextScore    float32  `json:"text_score"`
 ImageScore   float32  `json:"image_score"`
 Violations   []string `json:"violations"`
 SensitiveInfo []string `json:"sensitive_info"`
}

func (cms *ContentModerationSystem) Moderate(ctx context.Context, req ModerationRequest) (*ModerationResponse, error) {
 response := &ModerationResponse{
  Safe:       true,
  Violations: make([]string, 0),
  SensitiveInfo: make([]string, 0),
 }

 // 1. 文本审核
 if req.Text != "" {
  label, score, err := cms.textClassifier.Classify(req.Text)
  if err != nil {
   return nil, err
  }

  response.TextScore = score
  if label == "toxic" && score > 0.7 {
   response.Safe = false
   response.Violations = append(response.Violations, "toxic_language")
  }

  // NER - 检测敏感信息
  entities, _ := cms.ner.ExtractEntities(req.Text)
  for _, entity := range entities {
   if entity.Type == "PER" || entity.Type == "ID" {
    response.SensitiveInfo = append(response.SensitiveInfo, entity.Text)
   }
  }
 }

 // 2. 图像审核
 if req.ImageURL != "" {
  imagePath, err := downloadImage(req.ImageURL)
  if err != nil {
   return nil, err
  }
  defer os.Remove(imagePath)

  // NSFW检测
  label, score, err := cms.imageClassifier.Classify(imagePath)
  if err != nil {
   return nil, err
  }

  response.ImageScore = score
  if label == "nsfw" && score > 0.8 {
   response.Safe = false
   response.Violations = append(response.Violations, "nsfw_content")
  }

  // 目标检测 - 检测违禁物品
  detections, _ := cms.yolo.Detect(imagePath)
  for _, det := range detections {
   if det.Label == "weapon" || det.Label == "drugs" {
    response.Safe = false
    response.Violations = append(response.Violations, fmt.Sprintf("detected_%s", det.Label))
   }
  }
 }

 return response, nil
}

func main() {
 system, err := NewContentModerationSystem()
 if err != nil {
  log.Fatal(err)
 }

 r := gin.Default()

 r.POST("/moderate", func(c *gin.Context) {
  var req ModerationRequest
  if err := c.ShouldBindJSON(&req); err != nil {
   c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request"})
   return
  }

  result, err := system.Moderate(c.Request.Context(), req)
  if err != nil {
   c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
   return
  }

  c.JSON(http.StatusOK, result)
 })

 log.Println("Content Moderation System running on :8080")
 r.Run(":8080")
}
```

### 10.2 生产环境配置

```yaml
# docker-compose.yml
version: '3.8'

services:
  model-server:
    build: .
    ports:
      - "8080:8080"
    environment:
      - MODEL_PATH=/models
      - LOG_LEVEL=info
      - MAX_BATCH_SIZE=32
      - GPU_ENABLED=true
    volumes:
      - ./models:/models
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

```dockerfile
# Dockerfile
FROM golang:1.23-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=1 GOOS=linux go build -o /model-server ./cmd/server

FROM nvidia/cuda:12.0-base

# 安装ONNX Runtime
RUN apt-get update && apt-get install -y \
    libgomp1 \
    wget \
    && wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.0/onnxruntime-linux-x64-gpu-1.16.0.tgz \
    && tar -xzf onnxruntime-linux-x64-gpu-1.16.0.tgz \
    && cp onnxruntime-linux-x64-gpu-1.16.0/lib/* /usr/local/lib/ \
    && ldconfig \
    && rm -rf onnxruntime*

COPY --from=builder /model-server /usr/local/bin/

EXPOSE 8080

CMD ["model-server"]
```

---

## 总结

### Go AI/ML技术栈

```text
1. 模型格式:
   ✅ ONNX (推荐)
   ✅ TensorFlow SavedModel
   ✅ PyTorch TorchScript
   ✅ TensorFlow Lite

2. 推理引擎:
   ✅ ONNX Runtime (最佳性能)
   ✅ TensorFlow C API
   ✅ LibTorch

3. 应用场景:
   ✅ 图像分类与检测
   ✅ 自然语言处理
   ✅ 推荐系统
   ✅ 内容审核
   ✅ 搜索排序
```

### 性能优势

```text
Go vs Python推理性能:
  • 延迟: 5-10x faster
  • 吞吐量: 3-5x higher
  • 内存: 2-3x smaller
  • 并发: 10x better

适合Go的场景:
  ✅ 高并发推理服务
  ✅ 低延迟要求 (<50ms)
  ✅ 生产环境部署
  ✅ 微服务架构
```

### 最佳实践

1. **选择合适的模型格式**: ONNX通用性最佳
2. **批处理优化**: 提升吞吐量3-5倍
3. **模型量化**: INT8可减少75%模型大小
4. **GPU加速**: 大模型推荐使用GPU
5. **监控指标**: 延迟、吞吐量、错误率

**相关文档:**

- [Serverless与FaaS](32-Go-1.25.3Serverless与FaaS完整实战.md)
- [性能优化](../07-性能优化/10-Go-1.25.3性能优化完整实战.md)
- [微服务架构](09-Go-1.25.3微服务架构完整实战.md)
