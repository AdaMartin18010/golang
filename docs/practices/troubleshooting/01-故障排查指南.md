# 故障排查指南

## 📋 目录

- [1. 排查方法论](#1.-排查方法论)
  - [系统化排查流程](#系统化排查流程)
  - [排查工具箱](#排查工具箱)
- [2. 常见故障分类](#2.-常见故障分类)
  - [故障症状与原因对照表](#故障症状与原因对照表)
- [3. 性能问题排查](#3.-性能问题排查)
  - [案例1: CPU占用100%](#案例1-cpu占用100)
  - [案例2: 响应时间P99突然升高](#案例2-响应时间p99突然升高)
- [4. 内存问题排查](#4.-内存问题排查)
  - [案例1: 内存持续增长导致OOM](#案例1-内存持续增长导致oom)
  - [案例2: Goroutine泄漏](#案例2-goroutine泄漏)
- [5. 并发问题排查](#5.-并发问题排查)
  - [案例1: Data Race](#案例1-data-race)
  - [案例2: 死锁](#案例2-死锁)
- [6. 网络问题排查](#6.-网络问题排查)
  - [案例: TIME_WAIT过多导致连接失败](#案例-timewait过多导致连接失败)
- [7. 数据库问题排查](#7.-数据库问题排查)
  - [案例: 数据库连接池耗尽](#案例-数据库连接池耗尽)
- [8. 实战案例](#8.-实战案例)
  - [综合案例: 生产环境完整排查](#综合案例-生产环境完整排查)
    - [第1步: 快速定位 (5分钟)](#第1步-快速定位-5分钟)
    - [第2步: 确认依赖 (3分钟)](#第2步-确认依赖-3分钟)
    - [第3步: 深入分析 (10分钟)](#第3步-深入分析-10分钟)
    - [第4步: 找到根因 (5分钟)](#第4步-找到根因-5分钟)
    - [第5步: 应急修复 (2分钟)](#第5步-应急修复-2分钟)
    - [第6步: 验证效果 (5分钟)](#第6步-验证效果-5分钟)
    - [第7步: 复盘与预防](#第7步-复盘与预防)
- [9. 应急响应](#9.-应急响应)
  - [应急响应流程](#应急响应流程)
  - [常用应急命令](#常用应急命令)
  - [应急工具包](#应急工具包)
- [🔗 相关资源](#相关资源)

## 1. 排查方法论

### 系统化排查流程

```text
1. 问题定义
   ├─ 收集症状
   ├─ 确定影响范围
   └─ 评估严重程度
   
2. 信息收集
   ├─ 日志分析
   ├─ 监控指标
   └─ 环境信息
   
3. 假设验证
   ├─ 提出假设
   ├─ 设计验证
   └─ 执行测试
   
4. 根因分析
   ├─ 找到根本原因
   ├─ 确认因果关系
   └─ 记录过程
   
5. 解决方案
   ├─ 实施修复
   ├─ 验证效果
   └─ 复盘总结
```

---

### 排查工具箱

| 工具 | 用途 | 命令示例 |
|------|------|----------|
| **pprof** | CPU/内存分析 | `go tool pprof cpu.prof` |
| **trace** | 执行追踪 | `go tool trace trace.out` |
| **delve** | 调试器 | `dlv debug` |
| **strace** | 系统调用追踪 | `strace -p PID` |
| **netstat** | 网络连接 | `netstat -antp` |
| **lsof** | 文件句柄 | `lsof -p PID` |
| **top/htop** | 系统资源 | `top -p PID` |
| **dmesg** | 系统日志 | `dmesg \| tail` |

---

## 2. 常见故障分类

### 故障症状与原因对照表

| 症状 | 可能原因 | 排查方向 |
|------|----------|----------|
| **CPU高** | 死循环、热点函数、GC频繁 | pprof CPU分析 |
| **内存持续增长** | 内存泄漏、缓存无界 | pprof heap分析 |
| **响应慢** | 数据库慢查询、锁竞争、网络延迟 | trace分析 |
| **连接数增长** | 连接泄漏、超时配置不当 | netstat检查 |
| **频繁panic** | 空指针、数组越界、类型断言 | 日志+调试 |
| **goroutine泄漏** | 未关闭channel、无退出机制 | goroutine分析 |
| **死锁** | 锁循环依赖、channel阻塞 | -race检测 |

---

## 3. 性能问题排查

### 案例1: CPU占用100%

**问题描述**: 服务CPU突然飙升至100%，响应变慢

**排查步骤**:

```bash
# 1. 确认是哪个进程
top
# 发现进程PID 12345占用CPU 95%

# 2. 采集CPU profile
curl http://localhost:6060/debug/pprof/profile?seconds=30 > cpu.prof

# 3. 分析热点
go tool pprof cpu.prof
> top 10
> list functionName

# 4. 查看火焰图
go tool pprof -http=:8080 cpu.prof
```

**案例分析**:

```go
// 问题代码
func processData(data []byte) {
    var result string
    for i := 0; i < len(data); i++ {
        result += string(data[i])  // 每次都重新分配！
    }
    return result
}

// pprof输出显示:
// Total: 30s
// 28s (93.3%) runtime.concatstrings
//  1s ( 3.3%) processData
//  1s ( 3.3%) other
```

**修复方案**:

```go
// 修复后
func processData(data []byte) string {
    var builder strings.Builder
    builder.Grow(len(data))
    for i := 0; i < len(data); i++ {
        builder.WriteByte(data[i])
    }
    return builder.String()
}

// 效果: CPU 100% → 15%
```

---

### 案例2: 响应时间P99突然升高

**问题描述**: API的P99延迟从50ms突升到2秒

**排查步骤**:

```bash
# 1. 查看监控图表
# 发现在15:30开始延迟增加

# 2. 检查日志
tail -f /var/log/app.log | grep ERROR
# 发现大量"database connection timeout"

# 3. 检查数据库连接
curl http://localhost:6060/debug/pprof/goroutine?debug=1 | grep -A 10 "database"

# 4. 查看数据库连接池状态
curl http://localhost:6060/debug/vars
```

**发现问题**:

```go
// 数据库连接池配置
db.SetMaxOpenConns(10)     // 太小！
db.SetMaxIdleConns(5)
db.SetConnMaxLifetime(time.Hour)

// 监控显示:
// - 并发请求: 100 QPS
// - 每个请求需要连接: 0.1秒
// - 需要连接: 100 * 0.1 = 10个
// - 配置连接: 10个 ← 不够用！
```

**修复方案**:

```go
// 增加连接池大小
db.SetMaxOpenConns(50)     // 根据实际负载调整
db.SetMaxIdleConns(25)
db.SetConnMaxLifetime(30 * time.Minute)
db.SetConnMaxIdleTime(10 * time.Minute)

// 效果: P99延迟 2s → 50ms
```

---

## 4. 内存问题排查

### 案例1: 内存持续增长导致OOM

**问题描述**: 服务运行12小时后因OOM被Kill

**排查步骤**:

```bash
# 1. 采集多个时间点的heap快照
curl http://localhost:6060/debug/pprof/heap > heap1.prof
sleep 3600  # 等待1小时
curl http://localhost:6060/debug/pprof/heap > heap2.prof
sleep 3600
curl http://localhost:6060/debug/pprof/heap > heap3.prof

# 2. 对比快照找出增长点
go tool pprof -base=heap1.prof heap2.prof
> top 20 -cum

# 3. 查看具体分配
go tool pprof heap3.prof
> list suspiciousFunction
```

**发现问题**:

```go
// 问题代码 - 无界缓存
type Cache struct {
    data map[string]*Entry  // 永不删除！
    mu   sync.RWMutex
}

var globalCache = &Cache{
    data: make(map[string]*Entry),
}

func handleRequest(key string, value []byte) {
    globalCache.mu.Lock()
    globalCache.data[key] = &Entry{
        Value:     value,
        Timestamp: time.Now(),
    }
    globalCache.mu.Unlock()
}

// 问题分析:
// - 每小时请求: 10000
// - 每个Entry: ~1KB
// - 12小时累积: 10000 * 12 * 1KB = 120MB
// - 实际测量: 2GB (存在大对象)
```

**修复方案**:

```go
// 方案1: 使用LRU缓存
import "github.com/hashicorp/golang-lru"

type Cache struct {
    data *lru.Cache
    mu   sync.RWMutex
}

func NewCache() *Cache {
    cache, _ := lru.NewWithEvict(10000, func(key, value interface{}) {
        // 清理回调
    })
    return &Cache{data: cache}
}

// 方案2: 定期清理
func (c *Cache) cleanup() {
    ticker := time.NewTicker(10 * time.Minute)
    defer ticker.Stop()
    
    for range ticker.C {
        c.mu.Lock()
        now := time.Now()
        for key, entry := range c.data {
            if now.Sub(entry.Timestamp) > time.Hour {
                delete(c.data, key)
            }
        }
        c.mu.Unlock()
    }
}

// 效果: 内存稳定在200MB
```

---

### 案例2: Goroutine泄漏

**问题描述**: goroutine数量持续增长至100K+

**排查步骤**:

```bash
# 1. 查看goroutine数量
curl http://localhost:6060/debug/pprof/goroutine?debug=1 > goroutines.txt

# 2. 分析堆栈
grep -C 5 "stuck" goroutines.txt

# 3. 找出重复的堆栈
sort goroutines.txt | uniq -c | sort -rn | head -20
```

**发现问题**:

```go
// 问题代码
func handleWebSocket(conn *websocket.Conn) {
    go func() {
        for {
            msg, err := conn.ReadMessage()
            if err != nil {
                log.Println(err)
                continue  // 错误！应该return
            }
            process(msg)
        }
    }()  // goroutine永不退出
}

// goroutine分析显示:
// 99000 goroutines in: websocket.(*Conn).ReadMessage
//     阻塞在已关闭的连接上
```

**修复方案**:

```go
// 修复后
func handleWebSocket(ctx context.Context, conn *websocket.Conn) {
    go func() {
        defer conn.Close()
        
        for {
            select {
            case <-ctx.Done():
                return  // 正常退出
            default:
                msg, err := conn.ReadMessage()
                if err != nil {
                    return  // 连接关闭时退出
                }
                process(msg)
            }
        }
    }()
}

// 使用方式
ctx, cancel := context.WithCancel(context.Background())
defer cancel()
handleWebSocket(ctx, conn)

// 效果: goroutine数量稳定在100以下
```

---

## 5. 并发问题排查

### 案例1: Data Race

**问题描述**: 偶现panic或数据不一致

**排查步骤**:

```bash
# 1. 使用race detector编译运行
go build -race
./myapp

# 或在测试中
go test -race ./...

# 2. 查看race报告
==================
WARNING: DATA RACE
Write at 0x00c000124010 by goroutine 7:
  main.updateCounter()
      /app/main.go:25 +0x50

Previous read at 0x00c000124010 by goroutine 6:
  main.getCounter()
      /app/main.go:30 +0x40
==================
```

**问题代码**:

```go
// ❌ 问题代码
type Stats struct {
    counter int64
}

func (s *Stats) Increment() {
    s.counter++  // 非原子操作！
}

func (s *Stats) Get() int64 {
    return s.counter  // 数据竞争！
}
```

**修复方案**:

```go
// ✅ 方案1: 使用atomic
type Stats struct {
    counter int64
}

func (s *Stats) Increment() {
    atomic.AddInt64(&s.counter, 1)
}

func (s *Stats) Get() int64 {
    return atomic.LoadInt64(&s.counter)
}

// ✅ 方案2: 使用互斥锁
type Stats struct {
    counter int64
    mu      sync.RWMutex
}

func (s *Stats) Increment() {
    s.mu.Lock()
    s.counter++
    s.mu.Unlock()
}

func (s *Stats) Get() int64 {
    s.mu.RLock()
    defer s.mu.RUnlock()
    return s.counter
}
```

---

### 案例2: 死锁

**问题描述**: 服务完全hang住，无响应

**排查步骤**:

```bash
# 1. 获取所有goroutine堆栈
curl http://localhost:6060/debug/pprof/goroutine?debug=2 > stacks.txt

# 2. 查找持有锁的goroutine
grep -B 5 -A 10 "sync.(*Mutex).Lock" stacks.txt
```

**问题代码**:

```go
// ❌ 死锁场景
type Account struct {
    balance int
    mu      sync.Mutex
}

func transfer(from, to *Account, amount int) {
    from.mu.Lock()
    defer from.mu.Unlock()
    
    to.mu.Lock()      // 可能死锁！
    defer to.mu.Unlock()
    
    from.balance -= amount
    to.balance += amount
}

// 死锁发生:
// Goroutine 1: transfer(A, B, 100)  - 持有A锁，等待B锁
// Goroutine 2: transfer(B, A, 50)   - 持有B锁，等待A锁
// → 循环等待 → 死锁！
```

**修复方案**:

```go
// ✅ 修复: 按固定顺序加锁
func transfer(from, to *Account, amount int) {
    // 保证总是按ID顺序加锁
    if from.id < to.id {
        from.mu.Lock()
        defer from.mu.Unlock()
        to.mu.Lock()
        defer to.mu.Unlock()
    } else {
        to.mu.Lock()
        defer to.mu.Unlock()
        from.mu.Lock()
        defer from.mu.Unlock()
    }
    
    from.balance -= amount
    to.balance += amount
}
```

---

## 6. 网络问题排查

### 案例: TIME_WAIT过多导致连接失败

**问题描述**: 高并发下出现"cannot assign requested address"

**排查步骤**:

```bash
# 1. 查看连接状态
netstat -an | grep TIME_WAIT | wc -l
# 输出: 28000 (过多！)

# 2. 查看系统限制
cat /proc/sys/net/ipv4/ip_local_port_range
# 输出: 32768  60999 (可用端口: 28231)

# 3. 查看哪些连接
netstat -antp | grep TIME_WAIT | head -20
```

**问题分析**:

```go
// 问题代码 - 每次创建新连接
func callAPI(url string) ([]byte, error) {
    resp, err := http.Get(url)  // 默认transport不复用
    if err != nil {
        return nil, err
    }
    defer resp.Body.Close()
    
    return io.ReadAll(resp.Body)
}

// 问题:
// - 每次请求新建连接
// - 连接关闭后进入TIME_WAIT (60秒)
// - 高并发导致端口耗尽
```

**修复方案**:

```go
// ✅ 修复: 复用HTTP连接
var httpClient = &http.Client{
    Transport: &http.Transport{
        MaxIdleConns:        100,
        MaxIdleConnsPerHost: 10,
        IdleConnTimeout:     90 * time.Second,
    },
    Timeout: 10 * time.Second,
}

func callAPI(url string) ([]byte, error) {
    resp, err := httpClient.Get(url)
    if err != nil {
        return nil, err
    }
    defer resp.Body.Close()
    
    return io.ReadAll(resp.Body)
}

// 系统调优
// echo "1" > /proc/sys/net/ipv4/tcp_tw_reuse
// echo "10" > /proc/sys/net/ipv4/tcp_fin_timeout

// 效果: TIME_WAIT 28000 → 200
```

---

## 7. 数据库问题排查

### 案例: 数据库连接池耗尽

**问题描述**: 大量请求超时，日志显示"too many connections"

**排查步骤**:

```bash
# 1. 查看当前连接数
mysql> SHOW PROCESSLIST;
# 显示: 150个连接，大部分Sleep状态

# 2. 查看应用连接池状态
curl http://localhost:6060/debug/vars | jq '.db_stats'

# 3. 分析慢查询日志
tail -f /var/log/mysql/slow.log
```

**问题分析**:

```go
// 问题代码 - 事务未提交
func getUserOrders(userID string) ([]Order, error) {
    tx, _ := db.Begin()  // 开启事务
    
    rows, err := tx.Query("SELECT * FROM orders WHERE user_id = ?", userID)
    if err != nil {
        return nil, err  // 忘记回滚！
    }
    defer rows.Close()
    
    var orders []Order
    for rows.Next() {
        var order Order
        rows.Scan(&order.ID, &order.Amount)
        orders = append(orders, order)
    }
    
    // 忘记提交或回滚！
    return orders, nil
}

// 问题:
// - 每次调用都开启事务但不关闭
// - 连接一直被占用
// - 最终耗尽连接池
```

**修复方案**:

```go
// ✅ 修复1: 正确使用事务
func getUserOrders(userID string) ([]Order, error) {
    tx, err := db.Begin()
    if err != nil {
        return nil, err
    }
    defer tx.Rollback()  // 确保回滚
    
    rows, err := tx.Query("SELECT * FROM orders WHERE user_id = ?", userID)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var orders []Order
    for rows.Next() {
        var order Order
        rows.Scan(&order.ID, &order.Amount)
        orders = append(orders, order)
    }
    
    tx.Commit()  // 显式提交
    return orders, nil
}

// ✅ 修复2: 这个场景不需要事务
func getUserOrders(userID string) ([]Order, error) {
    rows, err := db.Query("SELECT * FROM orders WHERE user_id = ?", userID)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var orders []Order
    for rows.Next() {
        var order Order
        rows.Scan(&order.ID, &order.Amount)
        orders = append(orders, order)
    }
    
    return orders, nil
}

// 配置连接池
db.SetMaxOpenConns(50)
db.SetMaxIdleConns(25)
db.SetConnMaxLifetime(5 * time.Minute)

// 效果: 连接泄漏问题解决
```

---

## 8. 实战案例

### 综合案例: 生产环境完整排查

**背景**:

- 电商API服务
- 突然收到告警: P99延迟 > 5秒
- 错误率从0.1% → 5%
- 时间: 凌晨3:00

**排查过程**:

#### 第1步: 快速定位 (5分钟)

```bash
# 1. 查看监控大盘
# 发现: CPU正常30%, 内存正常, 但QPS下降50%

# 2. 查看错误日志
tail -n 1000 /var/log/app/error.log | grep -i error
# 发现大量: "context deadline exceeded"

# 3. 初步判断: 下游服务超时
```

#### 第2步: 确认依赖 (3分钟)

```bash
# 检查所有下游服务
for service in user product inventory; do
    echo "Checking $service..."
    curl -w "time: %{time_total}s\n" http://$service/health
done

# 输出:
# user: time: 0.05s ✓
# product: time: 0.08s ✓
# inventory: time: 8.52s ✗  ← 问题在这里！
```

#### 第3步: 深入分析 (10分钟)

```bash
# 登录inventory服务器
ssh inventory-01

# 查看系统资源
top
# 发现: CPU 100%, 一个进程占用

# 查看进程
ps aux | grep inventory
# PID: 12345

# 采集CPU profile
curl http://localhost:6060/debug/pprof/profile?seconds=30 > cpu.prof

# 分析
go tool pprof cpu.prof
> top 10
# 输出: checkStock 函数占用95%
```

#### 第4步: 找到根因 (5分钟)

```go
// 查看代码
func checkStock(productID string) (int, error) {
    // 问题代码: 每次都扫描全表！
    rows, _ := db.Query("SELECT * FROM inventory")  // 没有WHERE条件
    defer rows.Close()
    
    for rows.Next() {
        var id string
        var stock int
        rows.Scan(&id, &stock)
        
        if id == productID {
            return stock, nil
        }
    }
    
    return 0, errors.New("not found")
}

// 发现问题:
// - 库存表有100万条记录
// - 每次查询扫描全表
// - 凌晨3点有定时任务批量查询1000个商品
// - 1000 * 全表扫描 = 数据库崩溃
```

#### 第5步: 应急修复 (2分钟)

```sql
-- 立即添加索引
ALTER TABLE inventory ADD INDEX idx_product_id (product_id);
```

```bash
# 重启服务
systemctl restart inventory-service

# 验证
curl http://inventory-01/health
# time: 0.05s ✓ 恢复正常
```

#### 第6步: 验证效果 (5分钟)

```bash
# 监控大盘显示:
# - P99延迟: 5s → 50ms ✓
# - 错误率: 5% → 0.1% ✓
# - QPS: 恢复到正常值 ✓
```

#### 第7步: 复盘与预防

**根本原因**:

- 代码bug: 缺少WHERE条件
- 未添加数据库索引
- 缺少SQL审查机制

**预防措施**:

1. 代码审查: 强制review数据库查询
2. 性能测试: 添加大数据量测试
3. 监控告警: 慢查询监控
4. 熔断降级: 添加超时和熔断

---

## 9. 应急响应

### 应急响应流程

```text
1. 告警触发 (Alert)
   ↓
2. 快速评估 (Assess)
   - 影响范围
   - 严重程度
   ↓
3. 止血措施 (Mitigate)
   - 回滚
   - 限流
   - 降级
   ↓
4. 根因分析 (Analyze)
   ↓
5. 永久修复 (Fix)
   ↓
6. 复盘总结 (Postmortem)
```

---

### 常用应急命令

```bash
# 快速重启服务
systemctl restart myapp

# 回滚到上一版本
kubectl rollout undo deployment/myapp

# 手动扩容
kubectl scale deployment myapp --replicas=10

# 临时限流
# 在nginx中
location /api/ {
    limit_req zone=mylimit burst=10;
}

# 查看实时日志
kubectl logs -f deployment/myapp --tail=100

# 进入容器排查
kubectl exec -it myapp-pod-xxx -- /bin/sh
```

---

### 应急工具包

```go
// 应急开关 - 功能降级
var (
    enableCache   = true
    enableLogging = true
    enableAnalytics = false  // 紧急情况下关闭非关键功能
)

func handleRequest(w http.ResponseWriter, r *http.Request) {
    // 检查降级开关
    if !enableLogging {
        // 跳过日志记录
    }
    
    if !enableAnalytics {
        // 跳过分析统计
    }
    
    // 核心业务逻辑
    processCore(w, r)
}

// 动态配置 - 无需重启
func updateConfig() {
    ticker := time.NewTicker(10 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        config := fetchConfigFromEtcd()
        enableCache = config.EnableCache
        enableLogging = config.EnableLogging
        enableAnalytics = config.EnableAnalytics
    }
}
```

---

## 🔗 相关资源

- [性能分析工具](../../advanced/performance/01-性能分析工具.md)
- [性能调优实战](../../advanced/performance/06-性能调优实战.md)
- [监控系统](../observability/01-监控系统.md)
- [日志管理](../observability/02-日志管理.md)

---

**最后更新**: 2025-10-29  
**Go版本**: 1.25.3  
**文档类型**: 实战指南 ✨
