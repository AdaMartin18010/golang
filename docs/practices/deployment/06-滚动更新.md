# 滚动更新

**版本**: v1.0
**更新日期**: 2025-11-11
**适用于**: Go 1.25.3

---

## 📋 目录

- [滚动更新](#滚动更新)
  - [1. 📖 概念介绍](#1-概念介绍)
  - [2. 🎯 更新策略](#2-更新策略)
- [deployment.yaml](#deploymentyaml)
- [蓝色环境（当前）](#蓝色环境当前)
- [绿色环境（新版本）](#绿色环境新版本)
- [Service切换](#service切换)
- [主版本 - 90%流量](#主版本-90流量)
- [金丝雀版本 - 10%流量](#金丝雀版本-10流量)
  - [3. 🔄 更新操作](#3-更新操作)
- [方式1：kubectl set image](#方式1kubectl-set-image)
- [方式2：kubectl apply](#方式2kubectl-apply)
- [查看更新状态](#查看更新状态)
- [查看更新历史](#查看更新历史)
- [暂停更新（用于金丝雀）](#暂停更新用于金丝雀)
- [观察指标](#观察指标)
- [恢复更新](#恢复更新)
- [回滚到上一版本](#回滚到上一版本)
- [回滚到特定版本](#回滚到特定版本)
- [查看回滚状态](#查看回滚状态)
  - [4. 💡 最佳实践](#4-最佳实践)
  - [5. 📊 监控更新](#5-监控更新)
- [实时监控](#实时监控)
- [查看事件](#查看事件)
- [查看日志](#查看日志)
  - [6. ⚠️ 常见问题](#6-️-常见问题)

---

## 1. 📖 概念介绍

滚动更新是零停机部署的关键技术，通过逐步替换旧版本实例来更新应用，确保服务持续可用。根据生产环境的实际经验，合理的滚动更新策略可以将部署停机时间从数分钟降低到零，将部署风险降低 80-90%，将回滚时间从数十分钟缩短到数分钟。

**滚动更新性能对比**:

| 操作类型 | 停机部署 | 滚动更新 | 提升比例 |
|---------|---------|---------|---------|
| **部署停机时间** | 5-15 分钟 | 0 秒 | -100% |
| **部署风险** | 高（100%流量切换） | 低（逐步切换） | -80-90% |
| **回滚时间** | 30-60 分钟 | 2-5 分钟 | -90%+ |
| **资源利用率** | 50% | 90%+ | +80% |
| **用户体验影响** | 高 | 低 | -90%+ |

**滚动更新核心价值**:

1. **零停机部署**: 逐步替换实例，确保服务持续可用（停机时间 0 秒）
2. **降低风险**: 逐步验证新版本，降低部署风险（降低风险 80-90%）
3. **快速回滚**: 快速回滚到上一版本，减少故障影响（回滚时间 -90%+）
4. **资源高效**: 无需额外资源，资源利用率高（提升利用率 80%）

---

## 2. 🎯 更新策略

### 2.1 K8s滚动更新

**完整的生产环境滚动更新配置**:

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: production
  labels:
    app: myapp
    version: v2.0.0
spec:
  replicas: 4
  revisionHistoryLimit: 10  # 保留10个历史版本，便于回滚

  # 滚动更新策略
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1           # 最多额外1个Pod（25%）
      maxUnavailable: 0     # 零停机（不允许不可用）

  selector:
    matchLabels:
      app: myapp

  template:
    metadata:
      labels:
        app: myapp
        version: v2.0.0
    spec:
      # 服务账户
      serviceAccountName: myapp

      # 安全上下文
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000

      containers:
      - name: myapp
        image: registry.example.com/myapp:v2.0.0
        imagePullPolicy: Always

        # 资源限制
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

        # 端口
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP

        # 环境变量
        env:
        - name: ENV
          value: "production"
        - name: LOG_LEVEL
          value: "info"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: myapp-secrets
              key: database-url

        # 健康检查
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 20
          timeoutSeconds: 5
          failureThreshold: 3
          successThreshold: 1

        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
          successThreshold: 1

        # 启动探针（用于慢启动应用）
        startupProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 30  # 最多等待5分钟

        # 生命周期钩子
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]  # 等待15秒，确保流量完全迁移

        # 优雅关闭
        terminationGracePeriodSeconds: 30
```

**滚动更新过程可视化**:

```text
初始状态（4个副本）:
  [v1] [v1] [v1] [v1]

步骤1: 创建新Pod（maxSurge=1）
  [v2] [v1] [v1] [v1] [v1]  # 5个Pod（1个新版本）

步骤2: 等待v2就绪，终止1个v1
  [v2] [v2] [v1] [v1] [v1]  # v2就绪后创建下一个

步骤3: 继续替换
  [v2] [v2] [v2] [v1] [v1]

步骤4: 完成更新
  [v2] [v2] [v2] [v2]  # 回到4个副本
```

**滚动更新性能对比**:

| 配置项 | 保守策略 | 平衡策略 | 激进策略 | 推荐 |
|--------|---------|---------|---------|------|
| **maxSurge** | 0 | 1 (25%) | 2 (50%) | 1 (25%) |
| **maxUnavailable** | 0 | 0 | 1 (25%) | 0 |
| **minReadySeconds** | 30s | 10s | 0s | 10s |
| **更新速度** | 慢 | 中 | 快 | 中 |
| **风险** | 低 | 中 | 高 | 中 |
| **资源占用** | 100% | 125% | 150% | 125% |

---

### 2.2 蓝绿部署

**完整的生产环境蓝绿部署配置**:

```yaml
# k8s/blue-green-deployment.yaml
# 蓝色环境（当前生产版本）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-blue
  namespace: production
  labels:
    app: myapp
    version: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: myapp
        image: registry.example.com/myapp:v1.0.0
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
# 绿色环境（新版本）
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-green
  namespace: production
  labels:
    app: myapp
    version: green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: myapp
        image: registry.example.com/myapp:v2.0.0
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
# Service（通过切换selector完成部署）
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: production
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
  selector:
    app: myapp
    version: blue  # 切换到 green 即完成部署
---
# 蓝绿部署切换脚本
# scripts/blue-green-switch.sh
#!/bin/bash

CURRENT_VERSION=$(kubectl get svc myapp -n production -o jsonpath='{.spec.selector.version}')

if [ "$CURRENT_VERSION" == "blue" ]; then
    NEW_VERSION="green"
    OLD_VERSION="blue"
else
    NEW_VERSION="blue"
    OLD_VERSION="green"
fi

echo "Switching from $OLD_VERSION to $NEW_VERSION"

# 1. 确保新版本就绪
echo "Waiting for $NEW_VERSION to be ready..."
kubectl wait --for=condition=available --timeout=5m deployment/myapp-$NEW_VERSION -n production

# 2. 切换Service
kubectl patch svc myapp -n production -p "{\"spec\":{\"selector\":{\"version\":\"$NEW_VERSION\"}}}"

# 3. 等待流量迁移
sleep 30

# 4. 验证新版本
echo "Verifying $NEW_VERSION..."
HEALTH_URL="https://myapp.example.com/health"
for i in {1..30}; do
    if curl -f $HEALTH_URL > /dev/null 2>&1; then
        echo "✅ $NEW_VERSION is healthy"
        break
    fi
    echo "Waiting for $NEW_VERSION to be healthy... ($i/30)"
    sleep 2
done

# 5. 保留旧版本一段时间（便于快速回滚）
echo "Keeping $OLD_VERSION for 1 hour for quick rollback..."
sleep 3600

# 6. 清理旧版本（可选）
# kubectl delete deployment myapp-$OLD_VERSION -n production
```

**蓝绿部署性能对比**:

| 操作类型 | 滚动更新 | 蓝绿部署 | 说明 |
|---------|---------|---------|------|
| **部署速度** | 5-10 分钟 | 1-2 分钟 | 蓝绿部署更快 |
| **回滚速度** | 2-5 分钟 | < 1 分钟 | 蓝绿部署回滚最快 |
| **资源占用** | 100% | 200% | 蓝绿部署需要双倍资源 |
| **风险** | 中 | 低 | 蓝绿部署风险最低 |
| **适用场景** | 常规更新 | 重大更新 | 根据场景选择 |

---

### 2.3 金丝雀发布

**完整的生产环境金丝雀发布配置（使用 Istio）**:

```yaml
# k8s/canary-deployment.yaml
# 主版本（稳定版本）- 90%流量
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-stable
  namespace: production
spec:
  replicas: 9
  selector:
    matchLabels:
      app: myapp
      version: stable
  template:
    metadata:
      labels:
        app: myapp
        version: stable
    spec:
      containers:
      - name: myapp
        image: registry.example.com/myapp:v1.0.0
        ports:
        - containerPort: 8080
---
# 金丝雀版本（新版本）- 10%流量
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-canary
  namespace: production
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
      version: canary
  template:
    metadata:
      labels:
        app: myapp
        version: canary
    spec:
      containers:
      - name: myapp
        image: registry.example.com/myapp:v2.0.0
        ports:
        - containerPort: 8080
---
# Service（选择所有版本）
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: production
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080
---
# VirtualService（流量分配）
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp
  namespace: production
spec:
  hosts:
  - myapp.example.com
  http:
  - match:
    - headers:
        user-agent:
          regex: ".*Mobile.*"  # 移动端用户使用新版本
    route:
    - destination:
        host: myapp
        subset: canary
      weight: 100
  - route:
    - destination:
        host: myapp
        subset: stable
      weight: 90
    - destination:
        host: myapp
        subset: canary
      weight: 10
---
# DestinationRule（定义子集）
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp
  namespace: production
spec:
  host: myapp
  subsets:
  - name: stable
    labels:
      version: stable
  - name: canary
    labels:
      version: canary
```

**金丝雀发布流程**:

```text
阶段1: 初始部署（10%流量）
  Stable: 90% 流量
  Canary: 10% 流量

阶段2: 监控和验证（30分钟）
  - 监控错误率
  - 监控延迟
  - 监控资源使用

阶段3: 逐步增加流量（如果验证通过）
  10% → 25% → 50% → 100%

阶段4: 完成发布
  - 将 Canary 提升为 Stable
  - 清理旧版本
```

**金丝雀发布性能对比**:

| 操作类型 | 直接发布 | 金丝雀发布 | 提升比例 |
|---------|---------|-----------|---------|
| **部署风险** | 高（100%流量） | 低（10%流量） | -90% |
| **问题发现时间** | 数分钟 | 数秒 | -95%+ |
| **影响范围** | 100%用户 | 10%用户 | -90% |
| **回滚时间** | 5-10分钟 | < 1分钟 | -90%+ |
| **资源占用** | 100% | 110% | +10% |

---

## 3. 🔄 更新操作

### 3.1 更新镜像

```bash
# 方式1：kubectl set image
kubectl set image deployment/myapp myapp=myapp:v2

# 方式2：kubectl apply
kubectl apply -f deployment.yaml

# 查看更新状态
kubectl rollout status deployment/myapp

# 查看更新历史
kubectl rollout history deployment/myapp
```

---

### 3.2 暂停和恢复

```bash
# 暂停更新（用于金丝雀）
kubectl rollout pause deployment/myapp

# 观察指标
# ...

# 恢复更新
kubectl rollout resume deployment/myapp
```

---

### 3.3 回滚

```bash
# 回滚到上一版本
kubectl rollout undo deployment/myapp

# 回滚到特定版本
kubectl rollout undo deployment/myapp --to-revision=2

# 查看回滚状态
kubectl rollout status deployment/myapp
```

---

## 4. 💡 最佳实践

### 4.1 健康检查最佳实践

**完整的健康检查配置**:

```yaml
# 就绪探针（Readiness Probe）
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
    httpHeaders:
    - name: Custom-Header
      value: "HealthCheck"
  initialDelaySeconds: 5      # 启动后5秒开始检查
  periodSeconds: 5           # 每5秒检查一次
  timeoutSeconds: 3          # 3秒超时
  successThreshold: 1         # 成功1次即认为就绪
  failureThreshold: 3        # 失败3次即认为未就绪

# 存活探针（Liveness Probe）
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 15    # 启动后15秒开始检查
  periodSeconds: 20          # 每20秒检查一次
  timeoutSeconds: 5          # 5秒超时
  successThreshold: 1
  failureThreshold: 3        # 失败3次即重启Pod

# 启动探针（Startup Probe）- 用于慢启动应用
startupProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 30        # 最多等待5分钟（30 * 10秒）
  successThreshold: 1
```

**健康检查端点实现**:

```go
// internal/interfaces/http/health.go
package http

import (
    "context"
    "database/sql"
    "net/http"
    "time"
)

type HealthChecker struct {
    db *sql.DB
}

func (h *HealthChecker) HealthHandler(w http.ResponseWriter, r *http.Request) {
    // 检查数据库连接
    ctx, cancel := context.WithTimeout(r.Context(), 5*time.Second)
    defer cancel()

    if err := h.db.PingContext(ctx); err != nil {
        http.Error(w, "Database unhealthy", http.StatusServiceUnavailable)
        return
    }

    w.WriteHeader(http.StatusOK)
    w.Write([]byte("OK"))
}

func (h *HealthChecker) ReadyHandler(w http.ResponseWriter, r *http.Request) {
    // 检查应用是否就绪（数据库、缓存、消息队列等）
    ctx, cancel := context.WithTimeout(r.Context(), 5*time.Second)
    defer cancel()

    checks := map[string]error{
        "database": h.db.PingContext(ctx),
        // "cache": h.cache.Ping(ctx),
        // "mq": h.mq.Ping(ctx),
    }

    for name, err := range checks {
        if err != nil {
            http.Error(w, name+" not ready", http.StatusServiceUnavailable)
            return
        }
    }

    w.WriteHeader(http.StatusOK)
    w.Write([]byte("READY"))
}
```

### 4.2 优雅关闭最佳实践

**完整的优雅关闭实现**:

```go
// internal/infrastructure/server/graceful.go
package server

import (
    "context"
    "net/http"
    "os"
    "os/signal"
    "syscall"
    "time"

    "log/slog"
)

// GracefulShutdown 优雅关闭服务器
func GracefulShutdown(srv *http.Server, shutdownTimeout time.Duration) {
    // 创建信号通道
    quit := make(chan os.Signal, 1)
    signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)

    // 等待中断信号
    sig := <-quit
    slog.Info("Shutting down server", "signal", sig)

    // 创建超时上下文
    ctx, cancel := context.WithTimeout(context.Background(), shutdownTimeout)
    defer cancel()

    // 优雅关闭服务器
    if err := srv.Shutdown(ctx); err != nil {
        slog.Error("Server forced to shutdown", "error", err)
        os.Exit(1)
    }

    slog.Info("Server exited gracefully")
}

// 使用示例
func main() {
    srv := &http.Server{
        Addr:         ":8080",
        ReadTimeout:  15 * time.Second,
        WriteTimeout: 15 * time.Second,
        IdleTimeout:  60 * time.Second,
    }

    go func() {
        if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
            slog.Error("Server failed to start", "error", err)
            os.Exit(1)
        }
    }()

    // 优雅关闭（30秒超时）
    GracefulShutdown(srv, 30*time.Second)
}
```

### 4.3 更新策略最佳实践

**更新策略配置对比**:

| 策略 | maxSurge | maxUnavailable | minReadySeconds | 适用场景 |
|------|----------|----------------|-----------------|---------|
| **保守** | 0 | 0 | 30s | 关键服务，零风险 |
| **平衡** | 1 (25%) | 0 | 10s | 常规服务（推荐） |
| **快速** | 2 (50%) | 1 (25%) | 0s | 非关键服务，快速更新 |

**推荐的更新策略配置**:

```yaml
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1           # 25%（推荐）
      maxUnavailable: 0      # 零停机（推荐）
  minReadySeconds: 10       # 等待10秒确保稳定（推荐）
  progressDeadlineSeconds: 600  # 10分钟超时
```

### 4.4 更新监控最佳实践

**更新过程监控脚本**:

```bash
#!/bin/bash
# scripts/monitor-rollout.sh

DEPLOYMENT=$1
NAMESPACE=${2:-production}

echo "Monitoring rollout of $DEPLOYMENT in namespace $NAMESPACE"

# 1. 查看更新状态
kubectl rollout status deployment/$DEPLOYMENT -n $NAMESPACE --timeout=10m

# 2. 实时监控Pod状态
watch -n 2 "kubectl get pods -l app=$DEPLOYMENT -n $NAMESPACE"

# 3. 查看事件
kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp' | grep $DEPLOYMENT

# 4. 检查指标（Prometheus）
# - 错误率
# - 延迟
# - 请求率
```

**更新策略性能对比**:

| 策略 | 更新时间 | 风险 | 资源占用 | 推荐度 |
|------|---------|------|---------|--------|
| **保守策略** | 10-15分钟 | 低 | 100% | ⭐⭐⭐ |
| **平衡策略** | 5-10分钟 | 中 | 125% | ⭐⭐⭐⭐⭐ |
| **快速策略** | 2-5分钟 | 高 | 150% | ⭐⭐ |

---

## 5. 📊 监控更新

```bash
# 实时监控
watch kubectl get pods

# 查看事件
kubectl get events --sort-by='.lastTimestamp'

# 查看日志
kubectl logs -f deployment/myapp
```

---

## 6. ⚠️ 常见问题

### 6.1 更新卡住

**问题**: 更新过程中卡住，无法继续

**原因**:

- 新版本健康检查失败
- 资源不足
- 镜像拉取失败

**解决方案**:

```bash
# 1. 查看Pod状态
kubectl get pods -l app=myapp

# 2. 查看Pod事件
kubectl describe pod <pod-name>

# 3. 查看Pod日志
kubectl logs <pod-name>

# 4. 如果更新卡住，可以暂停并恢复
kubectl rollout pause deployment/myapp
# 修复问题后
kubectl rollout resume deployment/myapp

# 5. 如果无法恢复，回滚
kubectl rollout undo deployment/myapp
```

### 6.2 更新后性能下降

**问题**: 更新后应用性能下降

**解决方案**:

1. **监控关键指标**: 错误率、延迟、资源使用
2. **逐步增加流量**: 使用金丝雀发布
3. **快速回滚**: 如果性能下降，立即回滚

### 6.3 更新后数据不一致

**问题**: 更新后出现数据不一致

**解决方案**:

1. **数据库迁移**: 确保数据库迁移向后兼容
2. **版本兼容**: 确保新旧版本可以共存
3. **数据验证**: 更新后验证数据一致性

### 6.4 更新超时

**问题**: 更新超过超时时间

**解决方案**:

```yaml
# 增加超时时间
spec:
  progressDeadlineSeconds: 1200  # 20分钟
```

### 6.5 更新后服务不可用

**问题**: 更新后服务完全不可用

**解决方案**:

1. **立即回滚**: `kubectl rollout undo deployment/myapp`
2. **检查健康检查**: 确保健康检查配置正确
3. **检查资源**: 确保有足够的资源
4. **检查配置**: 确保配置正确
